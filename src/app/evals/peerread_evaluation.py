"""
PeerRead evaluation utilities for comparing agent reviews against ground truth.

This module provides functionality to evaluate agent-generated scientific paper
reviews against the peer reviews in the PeerRead dataset. It includes similarity
metrics and structured comparison results.
"""

import re

from app.data_models.peerread_evaluation_models import PeerReadEvalResult
from app.data_models.peerread_models import PeerReadReview
from app.data_utils.datasets_peerread import load_peerread_config

# FIXME use metric from huggingface, sklearn ...


def calculate_cosine_similarity(text1: str, text2: str) -> float:
    """Calculate cosine similarity between two text strings.

    Args:
        text1: First text string.
        text2: Second text string.

    Returns:
        Cosine similarity score (0-1).
    """
    # Simple implementation using word overlap
    # In production, use proper embeddings or TF-IDF
    words1 = set(re.findall(r"\w+", text1.lower()))
    words2 = set(re.findall(r"\w+", text2.lower()))

    if not words1 or not words2:
        return 0.0

    intersection = len(words1 & words2)
    union = len(words1 | words2)

    if union == 0:
        return 0.0

    return intersection / union


def calculate_jaccard_similarity(text1: str, text2: str) -> float:
    """Calculate Jaccard similarity between two text strings.

    Args:
        text1: First text string.
        text2: Second text string.

    Returns:
        Jaccard similarity score (0-1).
    """
    words1 = set(re.findall(r"\w+", text1.lower()))
    words2 = set(re.findall(r"\w+", text2.lower()))

    if not words1 and not words2:
        return 1.0

    intersection = len(words1 & words2)
    union = len(words1 | words2)

    return intersection / union if union > 0 else 0.0


def evaluate_review_similarity(agent_review: str, ground_truth: str) -> float:
    """Evaluate similarity between agent review and ground truth.

    Args:
        agent_review: Review text generated by agent.
        ground_truth: Ground truth review text.

    Returns:
        Weighted similarity score (0-1).
    """
    # Simple implementation - in production, use semantic embeddings
    cosine_sim = calculate_cosine_similarity(agent_review, ground_truth)
    jaccard_sim = calculate_jaccard_similarity(agent_review, ground_truth)

    # Weighted combination (weights from config)
    config = load_peerread_config()
    cosine_weight = config.similarity_metrics["cosine_weight"]
    jaccard_weight = config.similarity_metrics["jaccard_weight"]

    # For now, use only cosine and jaccard (semantic would require embeddings)
    total_weight = cosine_weight + jaccard_weight

    return (cosine_sim * cosine_weight + jaccard_sim * jaccard_weight) / total_weight


def create_evaluation_result(
    paper_id: str,
    agent_review: str,
    ground_truth_reviews: list[PeerReadReview],
) -> PeerReadEvalResult:
    """Create evaluation result comparing agent review to ground truth.

    Args:
        paper_id: Paper identifier.
        agent_review: Review generated by agent.
        ground_truth_reviews: Original peer reviews.

    Returns:
        PeerReadEvalResult with similarity metrics.
    """
    # Calculate similarity against all ground truth reviews
    similarities: list[float] = []
    for gt_review in ground_truth_reviews:
        sim = evaluate_review_similarity(agent_review, gt_review.comments)
        similarities.append(sim)

    overall_similarity = max(similarities) if similarities else 0.0

    # Simple recommendation matching (could be more sophisticated)
    agent_sentiment = "positive" if "good" in agent_review.lower() else "negative"
    gt_recommendations = [float(r.recommendation) for r in ground_truth_reviews]

    if len(gt_recommendations) == 0:
        # No ground truth to compare - default to False
        recommendation_match = False
    else:
        avg_gt_recommendation = sum(gt_recommendations) / len(gt_recommendations)
        recommendation_match = (
            agent_sentiment == "positive" and avg_gt_recommendation >= 3.0
        ) or (agent_sentiment == "negative" and avg_gt_recommendation < 3.0)

    return PeerReadEvalResult(
        paper_id=paper_id,
        agent_review=agent_review,
        ground_truth_reviews=ground_truth_reviews,
        similarity_scores={
            "cosine": max(
                [
                    calculate_cosine_similarity(agent_review, r.comments)
                    for r in ground_truth_reviews
                ],
                default=0.0,
            ),
            "jaccard": max(
                [
                    calculate_jaccard_similarity(agent_review, r.comments)
                    for r in ground_truth_reviews
                ],
                default=0.0,
            ),
        },
        overall_similarity=overall_similarity,
        recommendation_match=recommendation_match,
    )
