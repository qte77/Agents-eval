{
  "project": "Product Requirements Document: Agents-eval Sprint 5",
  "description": "",
  "source": "PRD.md",
  "generated": "2026-02-15 19:40:57",
  "stories": [
    {
      "id": "STORY-001",
      "title": "Wire Tier 2 judge provider fallback and auto-inherit agent provider",
      "description": "The `LLMJudgeEngine.select_available_provider()` method exists but is never called. When `tier2_provider=openai` and no `OPENAI_API_KEY` is set, all three Tier 2 metrics fail with 401 and fall back to neutral 0.5 scores. Wire the existing fallback chain so the judge validates API key availability before attempting LLM calls, and add a `tier2_provider=auto` mode that inherits the agent system's active provider.",
      "acceptance": [
        "`LLMJudgeEngine` calls `select_available_provider()` before creating judge agents",
        "When primary provider API key is missing, fallback provider is used automatically",
        "When both providers are unavailable, Tier 2 is skipped with a single warning (no 401 errors, no neutral 0.5 fallback scores)",
        "When Tier 2 is skipped, its 3 metrics (technical_accuracy, constructiveness, planning_rationality) are excluded from composite scoring and their weights redistributed to Tier 1 and Tier 3 metrics",
        "Compound redistribution: When both Tier 2 skipped AND single-agent mode (STORY-003), composite scorer must handle both conditions (skip 3 Tier 2 metrics + skip `coordination_quality`, redistribute remaining weights)",
        "New `tier2_provider=auto` mode inherits the agent system's active `chat_provider`",
        "`EvaluationPipeline` accepts optional `chat_provider` parameter to pass through to judge",
        "`_run_evaluation_if_enabled()` in `app.py` passes `chat_provider` to the pipeline",
        "Existing `JUDGE_TIER2_PROVIDER` env var override continues to work",
        "Tests: Hypothesis property tests for provider selection invariants (fallback only when primary unavailable)",
        "Tests: inline-snapshot for log messages during fallback",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/judge/llm_evaluation_managers.py",
        "src/app/judge/evaluation_pipeline.py",
        "src/app/judge/settings.py",
        "src/app/app.py",
        "src/app/llms/models.py",
        "tests/judge/test_llm_evaluation_managers.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T21:55:00Z",
      "content_hash": "4f0a2f5a8fc1ff0fbf37877b3a36f8c2fd88cd2064fe9b22a9af3a0abeb378e7",
      "depends_on": []
    },
    {
      "id": "STORY-002",
      "title": "Configurable agent token limits via CLI, GUI, and env var",
      "description": "The Cerebras provider has `usage_limits: 60000` in `config_chat.json`, but the `gpt-oss-120b` model consumed 75,954 tokens (74,714 input + 1,240 output) during a GUI run, causing `UsageLimitExceeded`. The high input token count was amplified by PeerRead tool returning 23 papers plus structured output validation retries. Add CLI and GUI overrides for `total_tokens_limit` so users can adjust without editing `config_chat.json`.",
      "acceptance": [
        "CLI: `--token-limit N` flag overrides `usage_limits` from `config_chat.json`",
        "GUI: Token limit input field in settings sidebar (pre-populated from `config_chat.json`)",
        "When flag/field is not set, existing `config_chat.json` value is used (no regression)",
        "`AGENT_TOKEN_LIMIT` environment variable override (lowest priority after CLI/GUI)",
        "Validation: minimum 1000, maximum 1000000",
        "Tests: Hypothesis property tests for limit bounds and override priority",
        "Tests: inline-snapshot for CLI help text",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/main.py",
        "src/app/app.py",
        "src/app/agents/agent_system.py",
        "src/gui/pages/settings.py",
        "src/gui/pages/run_app.py",
        "tests/app/test_cli_token_limit.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T22:13:22Z",
      "content_hash": "2efb79696a2af308b336daf4235ac9299933e30b0669f0eb7532cb3e70f98ab1",
      "depends_on": []
    },
    {
      "id": "STORY-003",
      "title": "Single-agent composite score weight redistribution",
      "description": "The composite scorer uses equal weights (0.167 each) across 6 metrics. For single-agent runs (no multi-agent delegation), `coordination_quality` is structurally 0.0 (mapped from `coordination_centrality` in Tier 3 graph analysis), causing a guaranteed 0.167 deduction. The scorer should detect single-agent runs and redistribute `coordination_quality` weight to the remaining 5 metrics.",
      "acceptance": [
        "Detect single-agent runs from `GraphTraceData` (0 or 1 unique agent IDs, empty `coordination_events`)",
        "When single-agent detected, redistribute `coordination_quality` weight (0.167) equally across remaining 5 metrics",
        "Multi-agent runs continue using all 6 metrics with equal weights (no regression)",
        "`CompositeResult` includes `single_agent_mode: bool` flag for transparency",
        "Compound redistribution: When both Tier 2 skipped (STORY-001) AND single-agent mode, composite scorer must handle both conditions (see STORY-001 for interaction)",
        "Log message when weight redistribution occurs",
        "Tests: Hypothesis property tests for weight sum invariant (always sums to ~1.0)",
        "Tests: inline-snapshot for metric weights in single-agent vs multi-agent mode",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/judge/composite_scorer.py",
        "src/app/data_models/evaluation_models.py",
        "tests/judge/test_composite_scorer.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T22:31:31Z",
      "content_hash": "f7a71b2b2103ff66c52892679b21967134e016217679251b5c54aefbe066d62c",
      "depends_on": []
    },
    {
      "id": "STORY-004",
      "title": "PeerRead dataset validation resilience for optional fields",
      "description": "Six papers (304-308, 330) fail validation with `KeyError: 'IMPACT'` at `datasets_peerread.py:724` because they lack the `IMPACT` field. These papers are silently skipped, reducing dataset coverage. The `IMPACT` field should be treated as optional with a sensible default instead of causing validation failure.",
      "acceptance": [
        "Papers with missing `IMPACT` field are validated successfully with `IMPACT` defaulting to `None` or `\"UNKNOWN\"`",
        "Papers with missing other optional fields (`histories`, `comments`) also handled gracefully",
        "Existing papers with valid `IMPACT` field are unaffected (no regression)",
        "Log debug message when optional field is missing (not warning)",
        "Tests: Hypothesis property tests for paper validation with arbitrary missing optional fields",
        "Tests: inline-snapshot for validated paper with missing IMPACT",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/data_utils/datasets_peerread.py",
        "src/app/data_models/peerread_models.py",
        "tests/data_utils/test_datasets_peerread.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T23:10:18Z",
      "content_hash": "00ec0cd1f19af11fb87e1970fd18b954019b7d43341dc5f7818ec08456a98748",
      "depends_on": []
    },
    {
      "id": "STORY-005",
      "title": "Update documentation and diagrams for Sprint 5",
      "description": "Update project documentation and architecture diagrams to reflect Sprint 5 changes: judge provider fallback, configurable token limits, single-agent score redistribution, and PeerRead validation resilience. Add Sprint 5 to the roadmap and update architecture decision records where applicable.",
      "acceptance": [
        "`README.md`: Version badge updated, Sprint 5 referenced in status section",
        "`docs/roadmap.md`: Sprint 5 row added to roadmap table with status \"Active\" and link to `PRD-Sprint5-Ralph.md`",
        "`docs/architecture.md`: Composite Scoring section updated to document single-agent weight redistribution behavior",
        "`docs/architecture.md`: Tier 2 LLM-as-Judge section updated to document provider fallback chain and `auto` mode",
        "`docs/architecture.md`: Implementation Status section updated with Sprint 5 entry",
        "`docs/arch_vis/`: Update relevant PlantUML diagrams if evaluation pipeline flow changed (e.g., provider selection step in Tier 2)",
        "CHANGELOG.md updated",
        "Stale \"Opik integration\" docstrings in graph_analysis.py (lines 423, 506) updated to reference Phoenix",
        "No broken internal links introduced"
      ],
      "files": [
        "README.md",
        "docs/roadmap.md",
        "docs/architecture.md",
        "docs/arch_vis/mas-enhanced-workflow.plantuml",
        "docs/arch_vis/metrics-eval-sweep.plantuml",
        "CHANGELOG.md"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "bcc1ba6aa3f9784ae5d0a78273c7e92ef3a7dbe87f39281cdce7b581e8570353",
      "depends_on": [
        "STORY-001",
        "STORY-002",
        "STORY-003",
        "STORY-004",
        "STORY-006",
        "STORY-007",
        "STORY-008",
        "STORY-009",
        "STORY-010",
        "STORY-011",
        "STORY-012",
        "STORY-013",
        "STORY-014",
        "STORY-015",
        "STORY-016",
        "STORY-017"
      ]
    },
    {
      "id": "STORY-006",
      "title": "Background query execution with tab navigation resilience",
      "description": "When a user navigates away from the App tab during query execution, the run aborts because Streamlit re-runs the script on page change. The execution result is also lost because it is not persisted to session state. The app should run queries in the background and persist results so users can navigate freely and return to see completed output.",
      "acceptance": [
        "Query execution continues when user navigates to another tab (Settings, Evaluation Results, etc.)",
        "User can return to App tab and see the result after execution completes",
        "A progress indicator (spinner or status) shows while execution is in progress",
        "If execution is in progress when returning to App tab, spinner is displayed",
        "Execution result (`CompositeResult`, agent output) stored in `st.session_state`",
        "Error state stored in session state and displayed when user returns",
        "Tests: pytest unit tests for session state transitions (idle → running → completed/error)",
        "Tests: inline-snapshot for session state keys after execution",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/gui/pages/run_app.py",
        "src/run_gui.py",
        "tests/test_gui/test_run_app.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T23:21:51Z",
      "content_hash": "ddd26eaca19461991b2c7acd9d98cb28c0a0ac065a31f144405c7fbeddfa2652",
      "depends_on": []
    },
    {
      "id": "STORY-007",
      "title": "Debug log panel in App tab",
      "description": "Add an expandable panel in the App tab that displays real-time pipeline log output (evaluation metrics, tier results, errors) that currently only appears in the terminal. Users should see the same diagnostic information visible in the CLI without needing terminal access.",
      "acceptance": [
        "Expandable/collapsible \"Debug Log\" section at the bottom of the App tab",
        "Captures loguru output from `app.*` modules during execution",
        "Displays log entries with timestamp, level, and message (formatted, not raw)",
        "Log panel updates after execution completes (not required to be real-time streaming)",
        "Collapsed by default to keep UI clean",
        "Log entries color-coded by level: INFO (default), WARNING (yellow), ERROR (red)",
        "Tests: pytest unit tests for log capture sink (filters app.* modules, clears buffer)",
        "Tests: inline-snapshot for log panel HTML structure",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/gui/pages/run_app.py",
        "src/gui/utils/log_capture.py",
        "tests/test_gui/test_log_capture.py"
      ],
      "passes": true,
      "completed_at": "2026-02-16T00:10:00Z",
      "content_hash": "f768e730d0fbfa7e6f1815fda4d45dcce8c27eb41d0f80d5260cc1746332cf99",
      "depends_on": [
        "STORY-006"
      ]
    },
    {
      "id": "STORY-008",
      "title": "Wire Evaluation Results and Agent Graph tabs to real data",
      "description": "The \"Evaluation Results\" and \"Agent Graph\" tabs have full rendering implementations but are called with `None` data from `run_gui.py` (lines 100, 103). After a query execution in the App tab, both pages should display actual results from the completed run instead of showing placeholder messages.",
      "acceptance": [
        "After App tab execution completes, navigating to \"Evaluation Results\" displays the actual `CompositeResult`",
        "After App tab execution completes, navigating to \"Agent Graph\" displays the actual interaction graph from `GraphTraceData`",
        "Both pages show informational message when no execution has been run yet (existing behavior preserved)",
        "Evaluation Results page displays: composite score, tier scores, metric comparison chart, recommendation",
        "Agent Graph page displays: interactive Pyvis network with agent and tool nodes",
        "Data persists across tab navigation within the same session",
        "Tests: Hypothesis property tests for session state data integrity across page switches",
        "Tests: inline-snapshot for evaluation page render with real CompositeResult",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/run_gui.py",
        "src/gui/pages/run_app.py",
        "src/gui/pages/evaluation.py",
        "src/gui/pages/agent_graph.py",
        "tests/test_gui/test_evaluation_page.py",
        "tests/test_gui/test_agent_graph_page.py"
      ],
      "passes": true,
      "completed_at": "2026-02-16T00:59:05Z",
      "content_hash": "01ad08923e54522e03288a6e51bbb5bccbd7ff30760069c667119d93808fed05",
      "depends_on": [
        "STORY-006"
      ]
    },
    {
      "id": "STORY-009",
      "title": "Editable settings page with session-scoped persistence",
      "description": "The Settings page (`src/gui/pages/settings.py`) currently displays `JudgeSettings` and `AppEnv` values as read-only text. Only the \"Agent Configuration\" section (provider selector, agent toggles) in the App tab sidebar is interactive. All displayed settings should be editable via the GUI and applied to the current session.",
      "acceptance": [
        "`JudgeSettings` fields editable: `tiers_enabled`, `tier2_provider`, `tier2_model`, `tier2_fallback_provider`, `tier2_fallback_model`, timeout values",
        "`JudgeSettings` composite thresholds editable: `composite_accept_threshold`, `composite_weak_accept_threshold`, `composite_weak_reject_threshold`",
        "Observability settings editable: `logfire_enabled`, `phoenix_endpoint`, `trace_collection`",
        "Changed settings applied to the current session (stored in `st.session_state`)",
        "\"Reset to Defaults\" button restores original `JudgeSettings()` defaults",
        "Settings changes take effect on next App tab execution (no restart required)",
        "Input validation matches pydantic field constraints (e.g., `gt=0, le=300` for timeouts)",
        "Tests: Hypothesis property tests for settings value bounds",
        "Tests: inline-snapshot for settings page widget structure",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/gui/pages/settings.py",
        "src/gui/pages/run_app.py",
        "tests/test_gui/test_settings_page.py"
      ],
      "passes": true,
      "completed_at": "2026-02-16T01:45:30Z",
      "content_hash": "5bcf21564208aa25c41758e743a471700aecabc6686c6d1f96dd5015a7f2dda3",
      "depends_on": [
        "STORY-001"
      ]
    },
    {
      "id": "STORY-010",
      "title": "Code quality and OWASP MAESTRO security review",
      "description": "Comprehensive code quality and security audit of the entire codebase using the `reviewing-code` and `securing-mas` Claude Code skills, supported by Context7 MCP for up-to-date library documentation and Exa MCP for security advisory lookups. The review applies the OWASP MAESTRO 7-layer security framework documented in `docs/best-practices/mas-security.md` and produces actionable findings with fix recommendations.",
      "acceptance": [
        "Code quality review completed using `reviewing-code` skill across all `src/app/` modules",
        "Security review completed using `securing-mas` skill applying OWASP MAESTRO 7-layer framework",
        "MAESTRO Layer 1 (Model): Prompt injection risks assessed in agent system prompts and tool outputs",
        "MAESTRO Layer 2 (Agent Logic): Input validation and type safety verified across agent factories, evaluation managers",
        "MAESTRO Layer 3 (Integration): External service failure handling reviewed (LLM providers, PeerRead API, OTLP export)",
        "MAESTRO Layer 4 (Monitoring): Log injection risks and sensitive data in traces checked",
        "MAESTRO Layer 5 (Execution): Resource exhaustion risks reviewed (token limits, timeouts, thread pools)",
        "MAESTRO Layer 6 (Environment): Secret management verified (API keys in `.env`, no hardcoded credentials)",
        "MAESTRO Layer 7 (Orchestration): Agent delegation and tool registration security reviewed",
        "Context7 MCP used to verify current best practices for PydanticAI, Logfire, and Streamlit security patterns",
        "Exa MCP used to check for known CVEs in project dependencies",
        "Review findings documented in `docs/reviews/sprint5-code-review.md`",
        "Critical and high findings fixed in code; medium/low findings documented as future work",
        "`make validate` passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "docs/reviews/sprint5-code-review.md",
        "src/app/"
      ],
      "passes": true,
      "completed_at": "2026-02-16T02:34:54Z",
      "content_hash": "f2ae4b352d4ac9a7f11a733fd74058d5121581b2e47a5cfcfae4f76e796a3cfe",
      "depends_on": []
    },
    {
      "id": "STORY-011",
      "title": "Test suite audit and behavioral refactoring",
      "description": "Systematic audit of all 56 test files against the testing strategy (`docs/best-practices/testing-strategy.md`). Tests that only verify implementation details (field existence, type checks, default values, import availability) are deleted or replaced with behavioral tests. Tests that verify actual behavior (business logic, error handling, integration contracts) are kept and improved. The goal is a leaner, higher-signal test suite where every test catches real bugs.",
      "acceptance": [
        "Every test file in `tests/` audited against testing strategy criteria",
        "Tests that only verify implementation details identified and removed (see anti-patterns below)",
        "Tests that verify actual behavior kept and improved where needed",
        "No reduction in behavioral coverage -- only implementation-detail tests removed",
        "Remaining tests use appropriate tooling: pytest for logic, Hypothesis for properties, inline-snapshot for structure",
        "Audit findings documented in `docs/reviews/sprint5-test-audit.md` with per-file decisions (keep/delete/refactor)",
        "`make validate` passes after refactoring",
        "`make test_all` passes with no regressions in behavioral coverage",
        "CHANGELOG.md updated"
      ],
      "files": [
        "tests/",
        "docs/reviews/sprint5-test-audit.md"
      ],
      "passes": true,
      "completed_at": "2026-02-16T02:47:42Z",
      "content_hash": "fb0511e91f4a3198dac0f5bca89a1c0ed851a563068bd6dc03aae91bcd2b5d58",
      "depends_on": []
    },
    {
      "id": "STORY-012",
      "title": "Fix OTLP endpoint double-path bug in Logfire instrumentation",
      "description": "The Logfire instrumentation sets OTEL_EXPORTER_OTLP_ENDPOINT to http://localhost:6006/v1/traces (logfire_instrumentation.py:59). Per the OTEL spec, the SDK auto-appends signal-specific paths to this base endpoint, producing http://localhost:6006/v1/traces/v1/traces for spans and http://localhost:6006/v1/traces/v1/metrics for metrics -- both return HTTP 405 from Phoenix. All trace export silently fails despite the agent instrumentation working correctly.",
      "acceptance": [
        "Traces from agent runs appear in the Phoenix UI at http://localhost:6006",
        "No HTTP 405 errors in logs for /v1/traces/v1/traces or /v1/traces/v1/metrics paths",
        "OTEL_EXPORTER_OTLP_ENDPOINT set to base URL only (http://localhost:6006), not the signal-specific path",
        "Existing PHOENIX_ENDPOINT env var and phoenix_endpoint config field continue to work",
        "Tests: pytest unit test for endpoint construction logic (base URL without signal path)",
        "Tests: inline-snapshot for the constructed OTLP endpoint value",
        "make validate passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/agents/logfire_instrumentation.py",
        "tests/agents/test_logfire_instrumentation.py"
      ],
      "passes": true,
      "completed_at": "2026-02-16T02:54:38Z",
      "content_hash": "6492f8f8b68987ce191856eab1ca9fd84f2d868f19122aa901ad6041b3e9530b",
      "depends_on": []
    },
    {
      "id": "STORY-013",
      "title": "Fix Tier 3 tool accuracy overwrite and dead communication_overhead metric",
      "description": "Two issues in graph_analysis.py affect Tier 3 scoring accuracy. First, add_node at line 171 overwrites success_rate each time a tool is called, so only the last call outcome survives -- if a tool succeeds 9 times and fails once (last), success_rate=0.0. The same overwrite applies to add_edge at line 173 for agent-tool edge weights. Second, communication_overhead is computed and stored in Tier3Result but never included in overall_score (lines 392-397), making it a dead metric that inflates the model without contributing to scoring.",
      "acceptance": [
        "Tool success_rate accumulates across all calls (e.g., 9/10 successes = 0.9), not just the last call",
        "Agent-tool edge weight accumulates or averages across repeated calls, not overwritten",
        "communication_overhead either contributes to overall_score or is removed from Tier3Result",
        "If communication_overhead is included in scoring, weights are rebalanced to sum to 1.0",
        "Existing multi-tool and single-tool scenarios produce correct tool_selection_accuracy",
        "Tests: Hypothesis property tests for tool accuracy with repeated calls (success_rate in [0.0, 1.0])",
        "Tests: inline-snapshot for Tier3Result with known tool call sequences",
        "make validate passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/judge/graph_analysis.py",
        "src/app/data_models/evaluation_models.py",
        "tests/judge/test_graph_analysis.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "5880787ee6b8a3d0881057588c43a671abb40b3f888f8d4f729c0dc9c768e9c9",
      "depends_on": []
    },
    {
      "id": "STORY-014",
      "title": "Guard wandb import and disable crash telemetry default",
      "description": "login.py:9 has an unconditional from wandb import login as wandb_login at module level. If the optional wandb package is not installed, the entire login.py module fails to import, breaking the application. Additionally, wandb sends crash telemetry to Sentry by default with no opt-out. The weave import at line 44 is already guarded inside the function body -- the wandb import should follow the same pattern.",
      "acceptance": [
        "Application starts successfully when wandb is not installed (no ImportError)",
        "When wandb is installed and WANDB_API_KEY is set, login and weave init work as before",
        "When wandb is not installed, login() skips wandb/weave initialization with a debug log",
        "WANDB_ERROR_REPORTING defaults to false (respects user override if already set)",
        "Dead agentops commented code removed from login.py: commented import at line 7 (`# from agentops import init as agentops_init`) and commented code block at lines 30-37",
        "Tests: pytest unit test for login with wandb unavailable (mock ImportError)",
        "Tests: inline-snapshot for log output when wandb is missing",
        "make validate passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/utils/login.py",
        "tests/utils/test_login.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "ba6191c525e4f9b610111eac7e7d993fd9ea102f121f2de776b6be3a80f010ef",
      "depends_on": []
    },
    {
      "id": "STORY-015",
      "title": "Debug logging for empty API keys in provider resolution",
      "description": "When get_api_key() returns False for a provider whose key exists in .env but resolves to empty string at runtime, there is no diagnostic log. This makes transient .env loading issues (CWD mismatch, env var unset between runs) hard to diagnose. Add a debug log when a key is expected (provider registered with env_key) but the value is empty.",
      "acceptance": [
        "get_api_key() logs a debug message when a registered provider key resolves to empty string",
        "Debug message includes the env_key name (e.g., GITHUB_API_KEY) for diagnosis",
        "No log emitted for providers without API keys (e.g., Ollama)",
        "No log emitted when key is correctly loaded",
        "Tests: pytest unit test for empty-key debug log scenario",
        "make validate passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/llms/providers.py",
        "tests/llms/test_providers.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "dadf752222b3bee5cd22b7bfbfa989a7f12e2ee4c1c7c9c14fc002bd00813505",
      "depends_on": []
    },
    {
      "id": "STORY-016",
      "title": "Move PeerRead tools from manager to researcher agent",
      "description": "The manager agent receives both delegation tools (researcher(), analyst(), synthesiser()) and PeerRead tools (get_peerread_paper, generate_paper_review_content_from_template, save_structured_review) via add_peerread_tools_to_manager() at agent_system.py:411. Sub-agents get minimal tools: researcher has only duckduckgo_search_tool(), analyst and synthesiser have none. Models take the path of least resistance -- the manager uses PeerRead tools directly instead of delegating, resulting in zero multi-agent coordination. Moving PeerRead tools to the researcher enforces separation of concerns: manager coordinates, researcher executes.",
      "acceptance": [
        "PeerRead tools (get_peerread_paper, generate_paper_review_content_from_template, save_structured_review) registered on the researcher agent, not the manager",
        "Manager agent retains only delegation tools (researcher(), analyst(), synthesiser())",
        "Researcher agent has PeerRead tools plus duckduckgo_search_tool()",
        "Manager delegates to researcher for PeerRead operations (verified via GraphTraceData showing delegation events)",
        "Tier 3 graph analysis produces non-zero coordination_centrality and communication_overhead in multi-agent runs",
        "Single-agent fallback still works if researcher is disabled via agent toggles",
        "Existing CLI and GUI behavior produces correct review output (no regression in review quality)",
        "Tests: pytest unit test for tool registration (researcher has PeerRead tools, manager does not)",
        "Tests: Hypothesis property tests for delegation invariant (manager never calls PeerRead tools directly)",
        "make validate passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/agents/agent_system.py",
        "src/app/tools/peerread_tools.py",
        "tests/agents/test_agent_system.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "165119693847801b0e2b211e91688d2474af7fed11ad4963b6c928b9fe0e5a91",
      "depends_on": []
    },
    {
      "id": "STORY-017",
      "title": "Delete duplicate AppEnv class and dead code in load_settings.py",
      "description": "src/app/utils/load_settings.py contains a duplicate AppEnv class (lines 22-49) that diverges from the canonical AppEnv in src/app/data_models/app_models.py (lines 219-249). The duplicate is missing ANTHROPIC_API_KEY, CEREBRAS_API_KEY, OPENAI_API_KEY, and uses LOGFIRE_TOKEN instead of LOGFIRE_API_KEY. It also eagerly instantiates chat_config = AppEnv() at module level (line 52). Only one consumer exists: datasets_peerread.py:23. The duplicate class and module-level instance should be deleted.",
      "acceptance": [
        "Duplicate AppEnv class removed from load_settings.py",
        "Module-level chat_config = AppEnv() instance removed from load_settings.py",
        "datasets_peerread.py import updated to use canonical AppEnv from app.data_models.app_models",
        "load_config() function retained in load_settings.py ONLY if still used for JSON config loading; if load_config() is unused (grep/search confirms no consumers), delete entire load_settings.py module",
        "No import errors or runtime failures after removal",
        "Tests: pytest unit test verifying single AppEnv source of truth",
        "make validate passes",
        "CHANGELOG.md updated"
      ],
      "files": [
        "src/app/utils/load_settings.py",
        "src/app/data_utils/datasets_peerread.py",
        "tests/data_utils/test_datasets_peerread.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "27a9257f9b314b1a5bd06ad54b7b37f0d45d94c30483c0c005d8166c5993303a",
      "depends_on": []
    }
  ]
}
