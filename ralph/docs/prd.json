{
  "project": "Product Requirements Document - Agents-eval Sprint 10",
  "description": "\"Sprint 10: 6 features — E2E CLI/GUI parity for CC engine (solo + teams), graph visualization for all modes, expanded providers, judge UX, PydanticAI migration, test quality.\"",
  "source": "PRD.md",
  "generated": "2026-02-22 13:57:41",
  "stories": [
    {
      "id": "STORY-010",
      "title": "Connect all execution modes to the same three-tier evaluation pipeline",
      "description": "All execution modes (MAS, CC solo, CC teams) must produce comparable evaluation results through the same `evaluate_comprehensive()` call. The evaluation pipeline interface is already engine-agnostic — all three tiers operate on plain strings and dicts, not MAS types:",
      "acceptance": [
        "AC1: `evaluate_comprehensive()` is the sole evaluation entry point for MAS, CC solo, and CC teams — no mode-specific evaluation logic exists outside it",
        "AC2: CC solo and CC teams produce non-empty `review` text passed to the pipeline (extracted from `CCResult.output_data[\"result\"]`)",
        "AC3: All modes load `reference_reviews` from PeerRead when `paper_id` is set — Tier 1 scores against actual ground truth, not empty strings",
        "AC4: CC solo produces a `GraphTraceData` (minimal or from `CCTraceAdapter`); composite scorer detects `single_agent_mode=True` and redistributes `coordination_quality` weight",
        "AC5: CC teams produces a `GraphTraceData` with `agent_interactions` mapped from `team_artifacts` Task events",
        "AC6: `run_cc_teams` uses process group kill (`os.killpg`) after timeout — not just `proc.kill()` — to clean up teammate child processes",
        "AC7: `CompositeResult.engine_type` is set to `\"mas\"`, `\"cc_solo\"`, or `\"cc_teams\"` for all results",
        "AC8: CLI `--engine=cc` does NOT run the MAS pipeline — `_run_agent_execution()` is not called",
        "AC9: GUI \"Claude Code\" radio invokes CC engine, not MAS; a \"CC Teams\" checkbox appears when CC is selected",
        "AC10: For the same `paper_id`, MAS and CC Tier 1 scores use identical `reference_reviews` (same ground truth)",
        "AC11: All existing MAS tests continue to pass; new tests cover the CC path (solo and teams)",
        "AC12: `make validate` passes with no regressions",
        "**Capture CC review text**: Add `\"result\"` to `_RESULT_KEYS` in `cc_engine.py:85` so `cc_result.output_data[\"result\"]` contains the review text. Add `extract_cc_review_text(cc_result) -> str` helper",
        "**Build `GraphTraceData` from CC artifacts**: Add `cc_result_to_graph_trace(cc_result) -> GraphTraceData` that maps `team_artifacts` Task/TeamCreate events to `agent_interactions`, `tool_calls`, and `coordination_events`. CC solo: minimal `GraphTraceData(execution_id=cc_result.execution_id)` with empty lists — `CompositeScorer._detect_single_agent_mode()` already redistributes `coordination_quality` weight. CC teams: `Task.owner` -> delegation interactions, completed tasks -> `tool_calls`, `TeamCreate` -> `coordination_events`",
        "**Load reference reviews for all modes**: In `evaluation_runner.py`, before `evaluate_comprehensive()`, load from PeerRead: `paper.reviews[*].comments` when `paper_id` is set. This fixes the existing bug for ALL modes (MAS included)",
        "**Add `engine_type` to `CompositeResult`**: `engine_type: str = Field(default=\"mas\")` — enables downstream consumers to know the source engine. Backward-compatible default",
        "**Wire `main()` to branch on engine**: Add `cc_result: CCResult | None = None` param. When `engine == \"cc\"`: skip `_run_agent_execution()` entirely, extract review text via `extract_cc_review_text()`, build `GraphTraceData` via `cc_result_to_graph_trace()`, load paper content + reference reviews from PeerRead, call `evaluate_comprehensive()` with same 4 parameters as MAS, build `nx.DiGraph` via `build_interaction_graph()`",
        "**Fix CLI wiring**: Pass `engine` and `cc_result` explicitly to `main()`: `run(main(**args, engine=engine, cc_result=cc_result))`. Remove pattern where CC runs first then MAS runs anyway",
        "**Fix GUI wiring**: In `_execute_query_background()`, add CC branch that calls `run_cc_solo()` / `run_cc_teams()` before calling `main()` with `cc_result`. Add CC teams checkbox visible when engine is CC",
        "**Fix `run_cc_teams` timeout**: Use `start_new_session=True` + `os.killpg(os.getpgid(proc.pid), signal.SIGTERM)` then `proc.kill()` to clean up teammate child processes",
        "Mock `subprocess.run` and `subprocess.Popen` in tests — never call real `claude` CLI"
      ],
      "files": [
        "src/app/engines/cc_engine.py",
        "src/app/data_models/evaluation_models.py",
        "src/app/judge/evaluation_runner.py",
        "src/app/app.py",
        "src/run_cli.py",
        "src/gui/pages/run_app.py",
        "tests/engines/test_cc_engine.py",
        "tests/cli/test_cc_engine_wiring.py",
        "tests/judge/test_evaluation_runner.py"
      ],
      "status": "passed",
      "wave": 1,
      "completed_at": "2026-02-22T14:01:25Z",
      "content_hash": "677e6f39cc431e68a755182cde7c16a454617b377e50ce136fc322e097263d9e",
      "depends_on": []
    },
    {
      "id": "STORY-011",
      "title": "Graph visualization polish for all execution modes",
      "description": "Feature 1 builds `GraphTraceData` and `nx.DiGraph` for CC runs. This feature handles the visualization layer: the Agent Graph page must distinguish between no-execution-yet, empty graph (CC solo), and populated graph (MAS or CC teams). CC Tier 3 graph metrics need \"informational\" labeling since they aren't comparable to MAS scores. `CCResult.team_artifacts` already retains parsed events from the JSONL stream (per `cc_engine.py:111-112`).",
      "acceptance": [
        "AC1: CC solo produces an `nx.DiGraph` (may be minimal — single node) displayed on Agent Graph page",
        "AC2: CC teams produces an `nx.DiGraph` showing team member nodes and delegation edges",
        "AC3: Empty graphs (0 nodes, 0 edges) display a descriptive warning (e.g., \"CC solo mode — no agent interactions to display\") instead of generic \"No agent interaction data available\"",
        "AC4: MAS graph visualization continues to work unchanged",
        "AC5: Tier 3 graph metrics from CC runs are labeled \"informational — not comparable to MAS scores\" in evaluation display",
        "AC6: `make validate` passes with no regressions",
        "In `agent_graph.py`: distinguish between `graph is None` (no execution yet), empty graph (execution produced no interactions — show mode-specific message using `CompositeResult.engine_type`), and populated graph",
        "For Tier 3 metrics on CC runs: when `engine_type` starts with `\"cc\"`, prefix metric labels with \"Informational\" in evaluation display",
        "Graph building itself is handled by Feature 1 (`cc_result_to_graph_trace()` + `build_interaction_graph()`)"
      ],
      "files": [
        "src/gui/pages/agent_graph.py",
        "src/gui/pages/evaluation_results.py",
        "tests/test_gui/test_agent_graph.py"
      ],
      "status": "passed",
      "wave": 2,
      "completed_at": "2026-02-22T14:01:59Z",
      "content_hash": "3c3a6648868d6789cae0f3ffeffbd05ba95668d421d3c46d980346440342c974",
      "depends_on": [
        "STORY-010"
      ]
    },
    {
      "id": "STORY-012",
      "title": "Expand inference provider registry and update stale models",
      "description": "The current `PROVIDER_REGISTRY` has 12 providers but is missing many popular OpenAI-compatible inference providers. Key omissions: Groq, Fireworks AI, DeepSeek, Mistral, SambaNova, Nebius, Cohere. The `anthropic` provider entry falls through to the generic `OpenAIChatModel` handler in `create_llm_model()` instead of using PydanticAI's native Anthropic support. Several existing `config_chat.json` entries have stale/deprecated model IDs -- two are live bugs: `huggingface` uses `facebook/bart-large-mnli` (a classification model, not chat -- will fail immediately) and `together` uses `Llama-3.3-70B-Instruct-Turbo-Free` (removed Jul 2025 -- will fail silently). Multiple `max_content_length` values are wrong (e.g., `cerebras` says 8192 but `gpt-oss-120b` has 128K context; `grok` says 15000 but should be 131K). Values must reflect the maximum token usage allowed on each provider's free tier before requests get blocked. See [Inference-Providers.md](../analysis/Inference-Providers.md) for the full provider analysis.",
      "acceptance": [
        "AC1: `PROVIDER_REGISTRY` includes the following new providers: `groq`, `fireworks`, `deepseek`, `mistral`, `sambanova`, `nebius`, `cohere`",
        "AC2: Each new provider has correct `env_key`, `base_url`, and `model_name_prefix` in `PROVIDER_REGISTRY`",
        "AC3: Each new provider has a matching entry in `config_chat.json` with best free-tier model and correct `max_content_length`",
        "AC4: Live bug fixed: `huggingface` model updated from `facebook/bart-large-mnli` (classification, not chat) to `meta-llama/Meta-Llama-3.3-70B-Instruct`",
        "AC5: Live bug fixed: `together` model updated from removed `Llama-3.3-70B-Instruct-Turbo-Free` to `meta-llama/Llama-3.3-70B-Instruct-Turbo`",
        "AC6: Existing stale `config_chat.json` entries updated to current models: `gemini-2.0-flash`, `gpt-4.1-mini` (openai + github), `grok-3-mini`, `claude-sonnet-4-20250514`, `qwen/qwen3-next-80b-a3b-instruct:free` (openrouter), `llama3.3:latest` (ollama)",
        "AC7: `max_content_length` in `config_chat.json` reflects the maximum token usage allowed on each provider's free tier before requests get rate-limited or blocked (per [Inference-Providers.md](../analysis/Inference-Providers.md) \"Key Limit\" column)",
        "AC8: `create_llm_model()` handles `anthropic` provider using PydanticAI's native `AnthropicModel` instead of the generic OpenAI-compatible fallback",
        "AC9: `create_llm_model()` handles `groq` with `OpenAIModelProfile(openai_supports_strict_tool_definition=False)` (same as existing `cerebras` handling)",
        "AC10: GUI Settings page provider dropdown automatically includes all new providers (already dynamic from `PROVIDER_REGISTRY.keys()`)",
        "AC11: CLI `--chat-provider` accepts all new provider names and validates against `PROVIDER_REGISTRY` at argument parsing time",
        "AC12: `make validate` passes with no regressions"
      ],
      "files": [
        "src/app/data_models/app_models.py",
        "src/app/llms/models.py",
        "src/app/config/config_chat.json",
        "src/run_cli.py",
        "tests/llms/test_models.py"
      ],
      "status": "passed",
      "wave": 1,
      "completed_at": "2026-02-22T15:05:42Z",
      "content_hash": "39ea0dc3a3a21688669abae679b0aaec8e4b4d912332def2afabf84b8bb4e101",
      "depends_on": []
    },
    {
      "id": "STORY-013",
      "title": "Judge auto mode -- conditional settings display",
      "description": "When `tier2_provider` is set to `\"auto\"` in the GUI Settings page, the downstream Tier 2 LLM Judge controls (model, fallback provider, fallback model, fallback strategy, timeout) are still displayed. Since \"auto\" delegates provider selection to the runtime, these manual overrides are confusing and logically redundant. They should be hidden when \"auto\" is selected.",
      "acceptance": [
        "AC1: When `tier2_provider` is `\"auto\"`, the following controls are hidden: primary model selectbox, fallback provider, fallback model, fallback strategy",
        "AC2: When `tier2_provider` is changed from `\"auto\"` to a specific provider, the hidden controls reappear immediately",
        "AC3: Timeout and cost budget controls remain visible regardless of provider selection (they apply to all modes)",
        "AC4: Session state values for hidden controls retain their defaults (not cleared when hidden)",
        "AC5: `make validate` passes with no regressions",
        "In `_render_tier2_llm_judge()` in `settings.py`, wrap the model/fallback controls in `if selected_provider != \"auto\":` conditional",
        "Keep `tier2_timeout_seconds` and `tier2_cost_budget_usd` outside the conditional -- they apply regardless",
        "Ensure `_build_judge_settings_from_session()` in `run_app.py` still constructs a valid `JudgeSettings` when auto is selected (fields use defaults from the model)"
      ],
      "files": [
        "src/gui/pages/settings.py",
        "tests/test_gui/test_settings_judge_auto.py"
      ],
      "status": "passed",
      "wave": 1,
      "completed_at": "2026-02-22T15:05:47Z",
      "content_hash": "9ffd217007bc28c171c7a9327ebea7bf8d8e2723aed355fd1bb51ab118bcb6f4",
      "depends_on": []
    },
    {
      "id": "STORY-014",
      "title": "PydanticAI API migration",
      "description": "`agent_system.py:543-551` uses the deprecated `manager.run()` PydanticAI API with 3 FIXME markers and broad `type: ignore` directives (`reportDeprecated`, `reportUnknownArgumentType`, `reportCallOverload`, `call-overload`). The `result.usage()` call also requires `type: ignore`. Additionally, `RunContext` may be deprecated in the installed PydanticAI version (Review F6), and `_model_name` private attribute access at `agent_system.py:537` should use the public `model_name` API (Review F23). Migrate all three patterns in one pass.",
      "acceptance": [
        "AC1: `manager.run()` replaced with current PydanticAI API (non-deprecated call)",
        "AC2: All `type: ignore` comments on lines 548 and 551 removed -- pyright passes cleanly",
        "AC3: All 3 FIXME comments (lines 543-544, 550) removed",
        "AC4: Agent execution produces identical results (same `execution_id`, same `result.output`)",
        "AC5: `RunContext` verified against installed PydanticAI version; updated to current name (e.g., `AgentRunContext`) if deprecated (Review F6)",
        "AC6: `_model_name` private attribute access replaced with public `model_name` API (Review F23)",
        "AC7: `make validate` passes with no new type errors or test failures",
        "Research current PydanticAI `Agent.run()` signature and migrate `mgr_cfg` dict unpacking accordingly",
        "Verify `result.usage()` return type is properly typed after migration",
        "Verify `RunContext` deprecation status: `python -c \"from pydantic_ai import RunContext; print(RunContext)\"`. If deprecated, update all tool function signatures in `agent_system.py` and `peerread_tools.py`",
        "Replace `getattr(manager, \"model\")._model_name` with `getattr(manager, \"model\").model_name` (public attribute) with fallback to `\"unknown\"`",
        "Preserve `trace_collector` start/end calls and error handling structure",
        "Mock PydanticAI agent in tests -- never call real LLM providers"
      ],
      "files": [
        "src/app/agents/agent_system.py",
        "src/app/tools/peerread_tools.py",
        "tests/agents/test_agent_system.py"
      ],
      "status": "failed",
      "wave": 2,
      "completed_at": null,
      "content_hash": "23a6f0f85dc97651a94291727c2098dd0ca4eea51d3862a4644b8f4de3d9929e",
      "depends_on": [
        "STORY-010",
        "STORY-012"
      ]
    },
    {
      "id": "STORY-015",
      "title": "Replace inspect.getsource tests with behavioral tests",
      "description": "Six test files use `inspect.getsource(module)` then assert string presence (e.g., `'engine != \"cc\"' in source`). This pattern breaks on code reformatting, passes if the string appears anywhere in source, and couples tests to implementation rather than behavior. Identified as a top-3 anti-pattern by prevalence in the tests parallel review (H5, H6, M14, M15 -- ~20 occurrences across 6 files).",
      "acceptance": [
        "AC1: `tests/utils/test_weave_optional.py` -- `inspect.getsource` replaced with behavioral test: import module with weave absent, verify `op()` is a callable no-op decorator (tests-review H5)",
        "AC2: `tests/gui/test_story012_a11y_fixes.py` -- all 11 `inspect.getsource` occurrences replaced with Streamlit mock-based assertions (tests-review H6)",
        "AC3: `tests/gui/test_story013_ux_fixes.py` -- source inspection replaced with behavioral widget assertions (tests-review H6)",
        "AC4: `tests/gui/test_story010_gui_report.py` -- 2 source inspections replaced with output assertions (tests-review H6)",
        "AC5: `tests/cli/test_cc_engine_wiring.py` -- 4 source inspections removed; behavioral tests already exist alongside (tests-review H6, M15)",
        "AC6: `tests/gui/test_prompts_integration.py` -- source file read + string assertion replaced with render function mock test (tests-review M14)",
        "AC7: Zero occurrences of `inspect.getsource` remain in `tests/` directory",
        "AC8: `make validate` passes with no regressions",
        "Replace source-level string assertions with behavioral tests: call the function with relevant inputs and assert outputs",
        "For UI tests, verify widgets called via Streamlit mocks instead of inspecting source",
        "For CLI tests, remove redundant source inspections where behavioral `parse_args` tests already cover the logic",
        "Run `grep -r \"inspect.getsource\" tests/` to verify zero remaining occurrences"
      ],
      "files": [
        "tests/utils/test_weave_optional.py",
        "tests/gui/test_story012_a11y_fixes.py",
        "tests/gui/test_story013_ux_fixes.py",
        "tests/gui/test_story010_gui_report.py",
        "tests/cli/test_cc_engine_wiring.py",
        "tests/gui/test_prompts_integration.py"
      ],
      "status": "failed",
      "wave": 1,
      "completed_at": null,
      "content_hash": "996492ac084d304e0fd23d39570fa716262ff2e2af8f556c8d79ad615a8a7a29",
      "depends_on": []
    }
  ]
}
