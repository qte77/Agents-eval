{
  "project": "Agents-eval",
  "description": "Settings migration, eval wiring, trace capture, graph-vs-text comparison",
  "source": "docs/PRD-Sprint2.md",
  "generated": "2026-02-11T00:00:00Z",
  "stories": [
    {
      "id": "STORY-001",
      "title": "Migrate EvaluationConfig to JudgeSettings pydantic-settings",
      "description": "Replace JSON-based EvaluationConfig with JudgeSettings(BaseSettings) using JUDGE_ env prefix. Defaults in code, overridable via .env. Reuse CommonSettings pattern.",
      "acceptance": [
        "JudgeSettings(BaseSettings) with JUDGE_ env prefix replaces EvaluationConfig",
        "Typed defaults in code: tier weights, timeouts, model selection, enabled tiers",
        "EvaluationPipeline uses JudgeSettings instead of loading config_eval.json",
        "Existing evaluation tests pass with settings-based config",
        "Timeout fields use bounded validators (gt=0, le=300)",
        "Time tracking pattern standardized across all tiers",
        "Existing test fixtures updated: pipeline uses JudgeSettings, JSON fixtures removed",
        "make validate passes"
      ],
      "files": [
        "src/app/evals/settings.py",
        "src/app/evals/evaluation_config.py",
        "src/app/evals/evaluation_pipeline.py",
        "src/app/evals/composite_scorer.py"
      ],
      "passes": true,
      "completed_at": "2026-02-12Z09:30:00",
      "content_hash": "",
      "depends_on": []
    },
    {
      "id": "STORY-002",
      "title": "Wire evaluate_comprehensive after run_manager",
      "description": "Connect run_manager() output to EvaluationPipeline.evaluate_comprehensive() in app.py. Add --skip-eval CLI flag. Pipeline uses JudgeSettings from STORY-001.",
      "acceptance": [
        "After run_manager() completes, EvaluationPipeline runs automatically",
        "--skip-eval CLI flag disables evaluation",
        "Graceful skip when no ground-truth reviews available",
        "make validate passes"
      ],
      "files": [
        "src/app/app.py",
        "src/run_cli.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "",
      "depends_on": [
        "STORY-001"
      ]
    },
    {
      "id": "STORY-003",
      "title": "Capture GraphTraceData during MAS execution",
      "description": "Wire TraceCollector into agent orchestration so GraphTraceData is populated from real agent runs.",
      "acceptance": [
        "Agent-to-agent delegations logged via trace_collector.log_agent_interaction()",
        "Tool calls logged via trace_collector.log_tool_call()",
        "Timing data captured for each delegation step",
        "GraphTraceData passed to evaluate_comprehensive() with real data",
        "GraphTraceData constructed via model_validate() instead of manual .get() extraction",
        "make validate passes"
      ],
      "files": [
        "src/app/agents/agent_system.py",
        "src/app/agents/orchestration.py",
        "src/app/app.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "",
      "depends_on": [
        "STORY-002"
      ]
    },
    {
      "id": "STORY-004",
      "title": "Add graph vs text metric comparison logging",
      "description": "Log comparative summary showing Tier 1 (text) vs Tier 3 (graph) scores after evaluation completes.",
      "acceptance": [
        "Log shows Tier 1 overall score vs Tier 3 overall score",
        "Individual graph metrics displayed (path_convergence, tool_selection_accuracy, etc.)",
        "Individual text metrics displayed (cosine_score, jaccard_score, semantic_score)",
        "Composite score shows per-tier contribution",
        "make validate passes"
      ],
      "files": [
        "src/app/app.py",
        "src/app/evals/evaluation_pipeline.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "",
      "depends_on": [
        "STORY-003"
      ]
    }
  ]
}
