{
  "project": "Product Requirements Document: Agents-eval Sprint 4",
  "description": "",
  "source": "PRD.md",
  "generated": "2026-02-15 15:58:26",
  "stories": [
    {
      "id": "STORY-001",
      "title": "Graceful Logfire trace export failures",
      "description": "Suppress noisy exception stack traces when Logfire/OTLP trace export fails due to connection errors (e.g., Opik service not running on localhost:6006). Currently, both span and metrics export print full ConnectionRefusedError stack traces to stderr multiple times during execution and at shutdown, cluttering logs during normal operation when tracing is unavailable. Affects both CLI (`make run_cli`) and GUI (`make run_gui`) equally.",
      "acceptance": [
        "Logfire initialization catches connection errors and logs single warning message",
        "Failed span exports do not print stack traces to stderr during agent runs",
        "Failed metrics exports do not print stack traces to stderr at shutdown",
        "When OTLP endpoint is unreachable, log one warning at initialization (not per-export)",
        "App continues normal operation when Logfire endpoint unavailable (both CLI and GUI)",
        "When Opik service is running, traces and metrics export successfully (no regression)",
        "Suppression works for both `/v1/traces/v1/traces` (spans) and `/v1/traces/v1/metrics` (metrics) endpoints",
        "Tests: Hypothesis property tests for retry/backoff behavior bounds",
        "Tests: inline-snapshot for warning message format",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "Add connection check in `LogfireInstrumentationManager._initialize_logfire()` (`src/app/agents/logfire_instrumentation.py:50-71`)",
        "Catch `requests.exceptions.ConnectionError` during initialization",
        "Set `self.config.enabled = False` when OTLP endpoint unreachable",
        "Log single warning: \"Logfire tracing unavailable: {endpoint} unreachable (spans and metrics export disabled)\"",
        "Configure OTLP span exporter with retry backoff to minimize per-span error noise",
        "Configure OTLP metrics exporter with retry backoff to minimize per-metric error noise",
        "Ensure existing `try/except` at line 69-71 handles initialization failures",
        "Suppress OpenTelemetry SDK export errors when endpoint connection fails (both span and metrics exporters)"
      ],
      "files": [
        "src/app/agents/logfire_instrumentation.py",
        "tests/agents/test_logfire_instrumentation.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T16:07:51Z",
      "content_hash": "dc0e848af6068881d5c58a006c6e9ddc7ad9cbc9e6e70ad12b4f7c803e7c6861",
      "depends_on": []
    },
    {
      "id": "STORY-002",
      "title": "Thread-safe graph analysis timeout handling",
      "description": "Replace Python `signal`-based timeouts in Tier 3 graph analysis with thread-safe alternatives. Currently, `_with_timeout()` fails with \"signal only works in main thread\" when called from Streamlit (non-main thread), causing `path_convergence` metric to return 0.0 fallback.",
      "acceptance": [
        "Graph analysis timeout handling works in both main and non-main threads",
        "`path_convergence` calculation succeeds in Streamlit GUI (no signal error)",
        "CLI evaluation continues to work with timeouts (no regression)",
        "Timeout mechanism uses `concurrent.futures.ThreadPoolExecutor` with timeout parameter",
        "Graceful fallback when timeout occurs (return 0.3, log warning)",
        "Tests: Hypothesis property tests for timeout bounds (0.0 <= fallback <= 0.5)",
        "Tests: inline-snapshot for timeout error result structure",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "Replace `signal`-based `_with_timeout()` in `src/app/judge/graph_analysis.py:348`",
        "Implement thread-safe timeout using `concurrent.futures.ThreadPoolExecutor`:"
      ],
      "files": [
        "src/app/judge/graph_analysis.py",
        "tests/evals/test_graph_analysis.py"
      ],
      "passes": true,
      "completed_at": "2026-02-15T16:16:44Z",
      "content_hash": "c8ae0e89d7eb293c4c0883871cf3a9baafc0fe88d6b40a546eece5fe02991449",
      "depends_on": []
    },
    {
      "id": "STORY-003",
      "title": "Tier 2 judge provider fallback validation",
      "description": "End-to-end validation that judge provider fallback works correctly. This is a testing and documentation task to confirm existing implementation handles missing API keys gracefully.",
      "acceptance": [
        "Integration test: Run evaluation with `tier2_provider=openai` and no `OPENAI_API_KEY` set",
        "Verify fallback to `tier2_fallback_provider` occurs (check logs)",
        "Verify Tier 2 metrics use neutral fallback scores (0.5) when all providers unavailable",
        "Verify composite score redistributes weights when Tier 2 is skipped",
        "Verify `Tier2Result` includes fallback metadata flag",
        "Update `docs/best-practices/troubleshooting.md` with Tier 2 auth failure guidance",
        "Tests: inline-snapshot for Tier2Result with fallback metadata",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "Create integration test in `tests/evals/test_llm_evaluation_managers_integration.py`",
        "Test scenarios:"
      ],
      "files": [
        "tests/evals/test_llm_evaluation_managers_integration.py",
        "docs/best-practices/troubleshooting.md"
      ],
      "passes": true,
      "completed_at": "2026-02-15T16:29:56Z",
      "content_hash": "3a316c61479b0aa99b1b0a042584bd937c373d451c023966e8fc67c4605ad1d8",
      "depends_on": []
    },
    {
      "id": "STORY-004",
      "title": "Complete test suite alignment with hypothesis and inline-snapshot",
      "description": "Refactor remaining test suite to use hypothesis (property-based testing) and inline-snapshot (regression testing), completing the test infrastructure alignment. No production code changes. Covers integration tests, benchmarks, GUI tests, and data utilities not yet converted. Explicitly excludes BDD/Gherkin (pytest-bdd).",
      "acceptance": [
        "Property-based tests using `@given` for data validation (PeerRead dataset schemas, model invariants)",
        "Property-based tests for integration test invariants (API responses, file I/O operations)",
        "Property-based tests for GUI state management (session state updates, widget interactions)",
        "Snapshot tests using `snapshot()` for integration test outputs (trace data, evaluation results)",
        "Snapshot tests for GUI page rendering outputs (Streamlit component structures)",
        "Snapshot tests for benchmark result structures",
        "Remove low-value tests (trivial assertions, field existence checks per testing-strategy.md)",
        "All existing test coverage maintained or improved",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "Add `from hypothesis import given, strategies as st` imports to relevant test files",
        "Add `from inline_snapshot import snapshot` imports to relevant test files",
        "Convert data validation tests to property tests with invariants (schemas always valid)",
        "Convert integration test outputs to snapshot tests",
        "Document usage patterns in test files for future reference",
        "NO pytest-bdd, NO Gherkin, NO BDD methodology (use TDD with hypothesis for properties)",
        "Apply hypothesis for property-based testing to:"
      ],
      "files": [
        "tests/app/test_evaluation_wiring.py",
        "tests/benchmarks/test_performance_baselines.py",
        "tests/data_utils/test_datasets_peerread.py",
        "tests/evals/test_opik_metrics.py",
        "tests/integration/test_enhanced_peerread_integration.py",
        "tests/integration/test_opik_integration.py",
        "tests/integration/test_peerread_integration.py",
        "tests/integration/test_peerread_real_dataset_validation.py",
        "tests/metrics/test_metrics_output_similarity.py",
        "tests/test_gui/test_agent_graph_page.py",
        "tests/test_gui/test_evaluation_page.py",
        "tests/test_gui/test_sidebar_phoenix.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "cce2614587548f2f0f0f9a4078b76a43519dfde14c7b9f4f47e7bef465aad92d",
      "depends_on": []
    },
    {
      "id": "STORY-005",
      "title": "CC trace adapter for solo and teams artifacts",
      "description": "Parse Claude Code artifacts into `GraphTraceData` format in two modes so CC runs can be evaluated through the same three-tier pipeline used for PydanticAI MAS runs. Both modes assume CC has full internal tool, plugin, and MCP access (the same capabilities as the PydanticAI agents).",
      "acceptance": [
        "Output `GraphTraceData` instance passes existing Tier 3 graph analysis without modification in both modes",
        "Auto-detect mode from directory structure (presence of `config.json` with `members` array indicates teams; otherwise solo)",
        "Graceful error handling when CC artifact directories are missing or malformed",
        "Tests: Hypothesis property tests for data mapping invariants (all fields populated, timestamps ordered) in both modes",
        "Tests: inline-snapshot for `GraphTraceData` output structure from sample CC artifacts (one solo, one teams)",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "Create `CCTraceAdapter` class that accepts a CC artifacts directory path and auto-detects mode",
        "**Teams mode** data mapping from CC artifacts to `GraphTraceData`:",
        "Adapter reads CC team config from `config.json` and extracts `execution_id` from team name",
        "Adapter parses `inboxes/*.json` messages into `agent_interactions` list",
        "Adapter parses `tasks/*.json` completions into `tool_calls` list (task completions as proxy)",
        "Adapter derives `timing_data` from first/last timestamps across all artifacts",
        "Adapter extracts `coordination_events` from task assignments and blocked-by relationships",
        "Adapter reads CC session export directory and extracts `execution_id` from session metadata",
        "Adapter parses tool-call entries from session logs into `tool_calls` list",
        "Adapter derives `timing_data` from session start/end timestamps",
        "`agent_interactions` is empty or contains only user-agent exchanges",
        "`coordination_events` is empty (single agent, no delegation)"
      ],
      "files": [
        "src/app/judge/cc_trace_adapter.py",
        "tests/judge/test_cc_trace_adapter.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "72938759ccccd098c51426e98f45e706b74df0bf1dd541e1d441428dacfa2750",
      "depends_on": []
    },
    {
      "id": "STORY-006",
      "title": "Baseline comparison engine for CompositeResult diffing",
      "description": "New `BaselineComparison` Pydantic model and comparison logic to diff `CompositeResult` instances across three systems: PydanticAI MAS, CC solo (no orchestration), and CC teams (with orchestration). The pairwise `compare()` function diffs any two `CompositeResult` instances; a `compare_all()` convenience function produces all three pairwise comparisons at once. Reuses existing `CompositeResult` model and `CompositeScorer.extract_metric_values()`.",
      "acceptance": [
        "`BaselineComparison` Pydantic model with fields: `label_a`, `label_b`, `result_a`, `result_b`, `metric_deltas`, `tier_deltas`, `summary`",
        "`compare(result_a, result_b, label_a, label_b)` accepts two `CompositeResult` instances and returns `BaselineComparison`",
        "`compare_all(pydantic_result, cc_solo_result, cc_teams_result)` returns list of 3 `BaselineComparison` (PydanticAI vs CC-solo, PydanticAI vs CC-teams, CC-solo vs CC-teams)",
        "`compare_all()` accepts `None` for any result and skips comparisons involving that result",
        "`metric_deltas` contains per-metric delta for all 6 composite metrics",
        "`tier_deltas` contains tier-level score differences (Tier 1, Tier 2, Tier 3)",
        "`summary` is a human-readable comparison string (e.g., \"PydanticAI scored +0.12 higher on technical_accuracy vs CC-solo\")",
        "Handles missing tiers gracefully (one system has Tier 2, other does not)",
        "Tests: Hypothesis property tests for delta symmetry (swap inputs -> negated deltas)",
        "Tests: inline-snapshot for `BaselineComparison` model dump structure",
        "Tests: inline-snapshot for `compare_all()` output with one None result",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "Create `BaselineComparison` Pydantic model:"
      ],
      "files": [
        "src/app/judge/baseline_comparison.py",
        "src/app/data_models/evaluation_models.py",
        "tests/judge/test_baseline_comparison.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "c5caefb17a9ae4468b879988abededa5615eafbbb2b66c5db31d7492f1106970",
      "depends_on": []
    },
    {
      "id": "STORY-007",
      "title": "CLI and GUI baseline integration",
      "description": "Wire the CC trace adapter and baseline comparison engine into the existing CLI and GUI so users can run side-by-side evaluations. Supports two CC baseline modes: solo (single CC instance, no orchestration) and teams (CC Agent Teams with delegation). Both modes assume CC had full internal tool, plugin, and MCP access during the run being evaluated.",
      "acceptance": [
        "CLI: `--cc-solo-dir PATH` flag accepts path to CC solo session export directory",
        "CLI: `--cc-teams-dir PATH` flag accepts path to CC Agent Teams artifacts directory",
        "CLI: Both flags can be provided together for three-way comparison (PydanticAI vs CC-solo vs CC-teams)",
        "CLI: Adapter auto-detects mode per directory; flags override auto-detection",
        "CLI: Baseline comparison(s) printed to console after standard evaluation output",
        "GUI: Baseline comparison view on evaluation results page (side-by-side metrics display)",
        "GUI: Separate directory inputs for CC solo and CC teams artifacts",
        "GUI: Three-way comparison table when both CC baselines are provided",
        "Both CLI and GUI skip baseline comparison when no CC artifacts provided (no regression)",
        "Tests: inline-snapshot for CLI output with single baseline and three-way comparison",
        "Tests: Hypothesis property tests for GUI state management with baseline data",
        "`make validate` passes",
        "CHANGELOG.md updated",
        "CLI: Add `--cc-solo-dir` and `--cc-teams-dir` arguments to CLI entry point",
        "CLI: For each provided directory, call `CCTraceAdapter(path).parse()` to get CC `GraphTraceData`, then run through `evaluate_comprehensive()` pipeline",
        "CLI: Call `compare_all()` with available results (pass `None` for missing baselines) and print each `BaselineComparison.summary`",
        "GUI: Add baseline section to evaluation results page using existing Streamlit patterns",
        "GUI: Display `metric_deltas` as side-by-side bar chart and `summary` as text for each pairwise comparison",
        "All traces go through the same evaluation pipeline (`evaluate_comprehensive()`)",
        "Reuse existing GUI evaluation page patterns (`src/gui/pages/evaluation.py`)"
      ],
      "files": [
        "src/app/app.py",
        "src/gui/pages/evaluation.py",
        "tests/app/test_cli_baseline.py",
        "tests/test_gui/test_evaluation_baseline.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "3be5f246ad56b49128c6a64c83d7179befecaf7ca29f0275b20a9c0777f1e289",
      "depends_on": [
        "STORY-002",
        "STORY-005",
        "STORY-006"
      ]
    }
  ]
}
