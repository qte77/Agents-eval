├── .devcontainer
    ├── setup_dev
    │   └── devcontainer.json
    ├── setup_dev_claude
    │   └── devcontainer.json
    └── setup_dev_ollama
    │   └── devcontainer.json
├── .env.example
├── .github
    ├── dependabot.yaml
    ├── scripts
    │   ├── create_pr.sh
    │   └── delete_branch_pr_tag.sh
    └── workflows
    │   ├── bump-my-version.yaml
    │   ├── codeql.yaml
    │   ├── generate-deploy-mkdocs-ghpages.yaml
    │   ├── links-fail-fast.yaml
    │   ├── pytest.yaml
    │   ├── ruff.yaml
    │   ├── summarize-jobs-reusable.yaml
    │   └── write-llms-txt.yaml
├── .gitignore
├── .gitmessage
├── .streamlit
    └── config.toml
├── .vscode
    ├── extensions.json
    └── settings.json
├── CHANGELOG.md
├── Dockerfile
├── LICENSE.md
├── Makefile
├── README.md
├── assets
    └── images
    │   ├── c4-multi-agent-system.png
    │   ├── customer-journey-activity-dark.png
    │   ├── customer-journey-activity-light.png
    │   └── metrics-eval-sweep.png
├── docs
    ├── PRD.md
    ├── SprintPlan.md
    ├── UserStory.md
    ├── architecture
    │   ├── c4-multi-agent-system.plantuml
    │   ├── customer-journey-activity-dark
    │   ├── customer-journey-activity-light.plantuml
    │   └── metrics-eval-sweep.plantuml
    └── llms.txt
├── mkdocs.yaml
├── pyproject.toml
├── src
    ├── app
    │   ├── __init__.py
    │   ├── agents
    │   │   ├── __init__.py
    │   │   ├── agent_system.py
    │   │   └── llm_model_funs.py
    │   ├── config
    │   │   ├── __init__.py
    │   │   ├── config_app.py
    │   │   ├── config_chat.json
    │   │   ├── config_eval.json
    │   │   └── data_models.py
    │   ├── evals
    │   │   ├── __init__.py
    │   │   └── metrics.py
    │   ├── main.py
    │   ├── py.typed
    │   └── utils
    │   │   ├── __init__.py
    │   │   ├── error_messages.py
    │   │   ├── load_configs.py
    │   │   ├── load_settings.py
    │   │   ├── log.py
    │   │   ├── login.py
    │   │   └── utils.py
    ├── examples
    │   ├── config.json
    │   ├── run_simple_agent_no_tools.py
    │   ├── run_simple_agent_system.py
    │   ├── run_simple_agent_tools.py
    │   └── utils
    │   │   ├── agent_simple_no_tools.py
    │   │   ├── agent_simple_system.py
    │   │   ├── agent_simple_tools.py
    │   │   ├── data_models.py
    │   │   ├── tools.py
    │   │   └── utils.py
    ├── gui
    │   ├── components
    │   │   ├── footer.py
    │   │   ├── header.py
    │   │   ├── output.py
    │   │   ├── prompts.py
    │   │   └── sidebar.py
    │   ├── config
    │   │   ├── config.py
    │   │   ├── styling.py
    │   │   └── text.py
    │   └── pages
    │   │   ├── home.py
    │   │   ├── prompts.py
    │   │   ├── run_app.py
    │   │   └── settings.py
    └── run_gui.py
├── tests
    ├── test_agent_system.py
    ├── test_env.py
    ├── test_metrics_output_similarity.py
    ├── test_metrics_time_taken.py
    └── test_provider_config.py
└── uv.lock


/.devcontainer/setup_dev/devcontainer.json:
--------------------------------------------------------------------------------
1 | {
2 |   "name": "make setup_dev",
3 |   "image": "mcr.microsoft.com/vscode/devcontainers/python:3.13",
4 |   "postCreateCommand": "make setup_dev"
5 | }


--------------------------------------------------------------------------------
/.devcontainer/setup_dev_claude/devcontainer.json:
--------------------------------------------------------------------------------
1 | {
2 |   "name": "make setup_dev_claude",
3 |   "image": "mcr.microsoft.com/vscode/devcontainers/python:3.13",
4 |   "features": {
5 |     "ghcr.io/devcontainers/features/node:1": {}
6 |   },
7 |   "postCreateCommand": "make setup_dev_claude"
8 | }


--------------------------------------------------------------------------------
/.devcontainer/setup_dev_ollama/devcontainer.json:
--------------------------------------------------------------------------------
1 | {
2 |     "name": "make setup_dev_ollama",
3 |     "image": "mcr.microsoft.com/vscode/devcontainers/python:3.13",
4 |     "postCreateCommand": "make setup_dev_ollama"
5 | }


--------------------------------------------------------------------------------
/.env.example:
--------------------------------------------------------------------------------
 1 | # inference EP
 2 | ANTHROPIC_API_KEY="sk-abc-xyz"
 3 | GEMINI_API_KEY="xyz"
 4 | GITHUB_API_KEY="ghp_xyz"
 5 | GROK_API_KEY="xai-xyz"
 6 | HUGGINGFACE_API_KEY="hf_xyz"
 7 | OPENROUTER_API_KEY="sk-or-v1-xyz"
 8 | PERPLEXITY_API_KEY=""
 9 | RESTACK_API_KEY="xyz"
10 | TOGETHER_API_KEY="xyz"
11 | 
12 | # tools
13 | TAVILY_API_KEY=""
14 | 
15 | # log/mon/trace
16 | AGENTOPS_API_KEY="x-y-z-x-y"
17 | LOGFIRE_API_KEY="pylf_v1_xx_y"  # LOGFIRE_TOKEN
18 | WANDB_API_KEY="xyz"
19 | 
20 | # eval
21 | 


--------------------------------------------------------------------------------
/.github/dependabot.yaml:
--------------------------------------------------------------------------------
 1 | ---
 2 | # https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates
 3 | version: 2
 4 | updates:
 5 |   - package-ecosystem: "pip"
 6 |     directory: "/"
 7 |     schedule:
 8 |       interval: "weekly"
 9 | ...
10 | 


--------------------------------------------------------------------------------
/.github/scripts/create_pr.sh:
--------------------------------------------------------------------------------
 1 | #!/bin/bash
 2 | # 1 base ref, 2 target ref, 3 title suffix
 3 | # 4 current version, 5 bumped
 4 | 
 5 | pr_title="PR $2 $3"
 6 | pr_body="PR automatically created from \`$1\` to bump from \`$4\` to \`$5\` on \`$2\`. Tag \`v$5\` will be created and has to be deleted manually if PR gets closed without merge."
 7 | 
 8 | gh pr create \
 9 |   --base $1 \
10 |   --head $2 \
11 |   --title "${pr_title}" \
12 |   --body "${pr_body}"
13 |   # --label "bump"
14 | 


--------------------------------------------------------------------------------
/.github/scripts/delete_branch_pr_tag.sh:
--------------------------------------------------------------------------------
 1 | #!/bin/bash
 2 | # 1 repo, 2 target ref, 3 current version
 3 | 
 4 | tag_to_delete="v$3"
 5 | branch_del_api_call="repos/$1/git/refs/heads/$2"
 6 | del_msg="'$2' force deletion attempted."
 7 | close_msg="Closing PR '$2' to rollback after failure"
 8 | 
 9 | echo "Tag $tag_to_delete for $del_msg"
10 | git tag -d "$tag_to_delete"
11 | echo "PR for $del_msg"
12 | gh pr close "$2" --comment "$close_msg"
13 | echo "Branch $del_msg"
14 | gh api "$branch_del_api_call" -X DELETE && \
15 |   echo "Branch without error return deleted."


--------------------------------------------------------------------------------
/.github/workflows/bump-my-version.yaml:
--------------------------------------------------------------------------------
  1 | ---
  2 | name: bump-my-version
  3 | 
  4 | on:
  5 |   # pull_request:
  6 |   #  types: [closed]
  7 |   #  branches: [main]
  8 |   workflow_dispatch:
  9 |     inputs:
 10 |       bump_type:
 11 |         description: '[major|minor|patch]'
 12 |         required: true
 13 |         default: 'patch'
 14 |         type: choice
 15 |         options:
 16 |         - 'major'
 17 |         - 'minor'
 18 |         - 'patch'
 19 | 
 20 | env:
 21 |   BRANCH_NEW: "bump-${{ github.run_number }}-${{ github.ref_name }}"
 22 |   SKIP_PR_HINT: "[skip ci bump]"
 23 |   SCRIPT_PATH: ".github/scripts"
 24 | 
 25 | jobs:
 26 |   bump_my_version:
 27 |     # TODO bug? currently resulting in: Unrecognized named-value: 'env'.
 28 |     # https://stackoverflow.com/questions/61238849/github-actions-if-contains-function-not-working-with-env-variable/61240761
 29 |     # if: !contains(
 30 |     #      github.event.pull_request.title,
 31 |     #      ${{ env.SKIP_PR_HINT }}
 32 |     #    )
 33 |     # TODO check for PR closed by bot to avoid PR creation loop
 34 |     # github.actor != 'github-actions'
 35 |     if: >
 36 |         github.event_name == 'workflow_dispatch' ||
 37 |         ( github.event.pull_request.merged == true &&
 38 |         github.event.pull_request.closed_by != 'github-actions' )
 39 |     runs-on: ubuntu-latest
 40 |     outputs:
 41 |       branch_new: ${{ steps.create_branch.outputs.branch_new }}
 42 |       summary_data: ${{ steps.set_summary.outputs.summary_data }}
 43 |     permissions:
 44 |       actions: read
 45 |       checks: write
 46 |       contents: write
 47 |       pull-requests: write
 48 |     steps:
 49 | 
 50 |       - name: Checkout repo
 51 |         uses: actions/checkout@v4
 52 |         with:
 53 |           fetch-depth: 1
 54 | 
 55 |       - name: Set git cfg and create branch
 56 |         id: create_branch
 57 |         run: |
 58 |           git config user.email "bumped@qte77.gha"
 59 |           git config user.name "bump-my-version"
 60 |           git checkout -b "${{ env.BRANCH_NEW }}"
 61 |           echo "branch_new=${{ env.BRANCH_NEW }}" >> $GITHUB_OUTPUT
 62 | 
 63 |       - name: Bump version
 64 |         id: bump
 65 |         uses: callowayproject/bump-my-version@0.29.0
 66 |         env:
 67 |           BUMPVERSION_TAG: "true"
 68 |         with:
 69 |           args: ${{ inputs.bump_type }}
 70 |           branch: ${{ env.BRANCH_NEW }}
 71 | 
 72 |       - name: "Create PR '${{ env.BRANCH_NEW }}'"
 73 |         if: steps.bump.outputs.bumped == 'true'
 74 |         env:
 75 |           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
 76 |         run: |
 77 |           src="${{ env.SCRIPT_PATH }}/create_pr.sh"
 78 |           chmod +x "$src"
 79 |           $src "${{ github.ref_name }}" "${{ env.BRANCH_NEW }}" "${{ env.SKIP_PR_HINT }}" "${{ steps.bump.outputs.previous-version }}" "${{ steps.bump.outputs.current-version }}"
 80 | 
 81 |       - name: Delete branch, PR and tag in case of failure or cancel
 82 |         if: failure() || cancelled()
 83 |         env:
 84 |           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
 85 |         run: |
 86 |           src="${{ env.SCRIPT_PATH }}/delete_branch_pr_tag.sh"
 87 |           chmod +x "$src"
 88 |           $src "${{ github.repository }}" "${{ env.BRANCH_NEW }}" "${{ steps.bump.outputs.current-version }}"
 89 | 
 90 |       - name: Set summary data
 91 |         id: set_summary
 92 |         if: ${{ always() }}
 93 |         run: echo "summary_data=${GITHUB_STEP_SUMMARY}" >> $GITHUB_OUTPUT
 94 |   
 95 |   generate_summary:
 96 |     name: Generate Summary Report 
 97 |     if: ${{ always() }}
 98 |     needs: bump_my_version
 99 |     uses: ./.github/workflows/summarize-jobs-reusable.yaml
100 |     with:
101 |       branch_to_summarize: ${{ needs.bump_my_version.outputs.branch_new }}
102 |       summary_data: ${{ needs.bump_my_version.outputs.summary_data }}
103 | ...
104 | 


--------------------------------------------------------------------------------
/.github/workflows/codeql.yaml:
--------------------------------------------------------------------------------
 1 | ---
 2 | # https://github.blog/changelog/2023-01-18-code-scanning-codeql-action-v1-is-now-deprecated/
 3 | name: "CodeQL"
 4 | 
 5 | on:
 6 |   push:
 7 |   pull_request:
 8 |     types: [closed]
 9 |     branches: [ main ]
10 |   schedule:
11 |     - cron: '27 11 * * 0'
12 |   workflow_dispatch:
13 | 
14 | jobs:
15 |   analyze:
16 |     name: Analyze
17 |     runs-on: ubuntu-latest
18 |     permissions:
19 |       actions: read
20 |       contents: read
21 |       security-events: write
22 | 
23 |     steps:
24 |     - name: Checkout repository
25 |       uses: actions/checkout@v4
26 | 
27 |     - name: Initialize CodeQL
28 |       uses: github/codeql-action/init@v3
29 |       with:
30 |         languages: python
31 | 
32 |     - name: Autobuild
33 |       uses: github/codeql-action/autobuild@v3
34 |     # if autobuild fails
35 |     #- run: |
36 |     #   make bootstrap
37 |     #   make release
38 | 
39 |     - name: Perform CodeQL Analysis
40 |       uses: github/codeql-action/analyze@v3
41 |     #- name: sarif
42 |     #  uses: github/codeql-action/upload-sarif@v2
43 | ...
44 | 


--------------------------------------------------------------------------------
/.github/workflows/generate-deploy-mkdocs-ghpages.yaml:
--------------------------------------------------------------------------------
  1 | ---
  2 | name: Deploy Docs
  3 | 
  4 | on:
  5 |   pull_request:
  6 |     types: [closed]
  7 |     branches: [main]
  8 |   workflow_dispatch:
  9 | 
 10 | env:
 11 |   DOCSTRINGS_FILE: "docstrings.md"
 12 |   DOC_DIR: "docs"
 13 |   SRC_DIR: "src"
 14 |   SITE_DIR: "site"
 15 |   IMG_DIR: "assets/images"
 16 | 
 17 | jobs:
 18 |   build-and-deploy:
 19 |     runs-on: ubuntu-latest
 20 |     permissions:
 21 |       contents: read
 22 |       pages: write
 23 |       id-token: write
 24 |     environment:
 25 |       name: github-pages
 26 |     steps:
 27 | 
 28 |     - name: Checkout the repository
 29 |       uses: actions/checkout@v4.0.0
 30 |       with:
 31 |         ref:
 32 |           ${{
 33 |             github.event.pull_request.merged == true &&
 34 |             'main' ||
 35 |             github.ref_name
 36 |           }}
 37 |         fetch-depth: 0
 38 | 
 39 |     - uses: actions/configure-pages@v5.0.0
 40 | 
 41 |     # caching instead of actions/cache@v4.0.0
 42 |     # https://docs.astral.sh/uv/guides/integration/github/#caching
 43 |     - name: Install uv with cache dependency glob
 44 |       uses: astral-sh/setup-uv@v5.0.0
 45 |       with:
 46 |         enable-cache: true
 47 |         cache-dependency-glob: "uv.lock"
 48 | 
 49 |     # setup python from pyproject.toml using uv
 50 |     # instead of using actions/setup-python@v5.0.0
 51 |     # https://docs.astral.sh/uv/guides/integration/github/#setting-up-python
 52 |     - name: "Set up Python"
 53 |       run: uv python install
 54 | 
 55 |     - name: Install only doc deps
 56 |       run: uv sync --only-group docs # --frozen
 57 | 
 58 |     - name: Get repo info and stream into mkdocs.yaml
 59 |       id: repo_info
 60 |       run: |
 61 |         REPO_INFO=$(curl -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
 62 |           -H "Accept: application/vnd.github.v3+json" \
 63 |           https://api.github.com/repos/${{ github.repository }})
 64 |         REPO_URL="${{ github.server_url }}/${{ github.repository }}"
 65 |         REPO_URL=$(echo ${REPO_URL} | sed 's|/|\\/|g')
 66 |         SITE_NAME=$(sed '1!d' README.md | sed '0,/# /{s/# //}')
 67 |         SITE_DESC=$(echo $REPO_INFO | jq -r .description)
 68 |         sed -i "s/<gha_sed_repo_url_here>/${REPO_URL}/g" mkdocs.yaml
 69 |         sed -i "s/<gha_sed_site_name_here>/${SITE_NAME}/g" mkdocs.yaml
 70 |         sed -i "s/<gha_sed_site_description_here>/${SITE_DESC}/g" mkdocs.yaml
 71 | 
 72 |     - name: Copy text files to be included
 73 |       run: |
 74 |         CFG_PATH="src/app/config"
 75 |         mkdir -p "${DOC_DIR}/${CFG_PATH}"
 76 |         cp README.md "${DOC_DIR}/index.md"
 77 |         cp {CHANGELOG,LICENSE}.md "${DOC_DIR}"
 78 |         # Auxiliary files
 79 |         cp .env.example "${DOC_DIR}"
 80 |         cp "${CFG_PATH}/config_chat.json" "${DOC_DIR}/${CFG_PATH}"
 81 | 
 82 |     - name: Generate code docstrings concat file
 83 |       run: |
 84 |         PREFIX="::: "
 85 |         find "${SRC_DIR}" -type f -name "*.py" \
 86 |           -type f -not -name "__*__*" -printf "%P\n" | \
 87 |           sed 's/\//./g' | sed 's/\.py$//' | \
 88 |           sed "s/^/${PREFIX}/" | sort > \
 89 |           "${DOC_DIR}/${DOCSTRINGS_FILE}"
 90 | 
 91 |     - name: Build documentation
 92 |       run: uv run --locked --only-group docs mkdocs build
 93 | 
 94 |     - name: Copy image files to be included
 95 |       run: |
 96 |         # copy images, mkdocs does not by default
 97 |         # mkdocs also overwrites pre-made directories
 98 |         dir="${{ env.SITE_DIR }}/${{ env.IMG_DIR }}"
 99 |         if [ -d "${{ env.IMG_DIR }}" ]; then
100 |           mkdir -p "${dir}"
101 |           cp "${{ env.IMG_DIR }}"/* "${dir}"
102 |         fi
103 | 
104 | #    - name: Push to gh-pages
105 | #      run: uv run mkdocs gh-deploy --force
106 | 
107 |     - name: Upload artifact
108 |       uses: actions/upload-pages-artifact@v3.0.0
109 |       with:
110 |         path: "${{ env.SITE_DIR }}"
111 | 
112 |     - name: Deploy to GitHub Pages
113 |       id: deployment
114 |       uses: actions/deploy-pages@v4.0.0
115 | ...
116 | 


--------------------------------------------------------------------------------
/.github/workflows/links-fail-fast.yaml:
--------------------------------------------------------------------------------
 1 | ---
 2 | # https://github.com/lycheeverse/lychee-action
 3 | # https://github.com/marketplace/actions/lychee-broken-link-checker
 4 | name: "Link Checker"
 5 | 
 6 | on:
 7 |   workflow_dispatch:
 8 |   push:
 9 |     branches-ignore: [main]
10 |   pull_request:
11 |     types: [closed]
12 |     branches: [main]
13 |   schedule:
14 |     - cron: "00 00 * * 0"
15 | 
16 | jobs:
17 |   linkChecker:
18 |     runs-on: ubuntu-latest
19 |     permissions:
20 |       issues: write
21 | 
22 |     steps:
23 |       - uses: actions/checkout@v4
24 | 
25 |       - name: Link Checker
26 |         id: lychee
27 |         uses: lycheeverse/lychee-action@v2
28 | 
29 |       - name: Create Issue From File
30 |         if: steps.lychee.outputs.exit_code != 0
31 |         uses: peter-evans/create-issue-from-file@v5
32 |         with:
33 |           title: lychee Link Checker Report
34 |           content-filepath: ./lychee/out.md
35 |           labels: report, automated issue
36 | ...
37 | 


--------------------------------------------------------------------------------
/.github/workflows/pytest.yaml:
--------------------------------------------------------------------------------
 1 | name: pytest
 2 | 
 3 | on:
 4 |   workflow_dispatch:
 5 | 
 6 | jobs:
 7 |   test:
 8 |     runs-on: ubuntu-latest
 9 |     steps:
10 |       - name: Checkout repository
11 |         uses: actions/checkout@v4
12 | 
13 |       - name: Set up Python
14 |         uses: actions/setup-python@v4
15 |         with:
16 |           python-version: '3.12'
17 | 
18 |       - name: Install dependencies
19 |         run: |
20 |           python -m pip install --upgrade pip
21 |           pip install pytest
22 | 
23 |       - name: Run tests
24 |         run: pytest
25 | 


--------------------------------------------------------------------------------
/.github/workflows/ruff.yaml:
--------------------------------------------------------------------------------
 1 | ---
 2 | # https://github.com/astral-sh/ruff-action
 3 | # https://github.com/astral-sh/ruff
 4 | name: ruff
 5 | on: 
 6 |   push:
 7 |   pull_request:
 8 |     types: [closed]
 9 |     branches: [main]
10 |   schedule:
11 |     - cron: "0 0 * * 0"
12 |   workflow_dispatch:
13 | jobs:
14 |   ruff:
15 |     runs-on: ubuntu-latest
16 |     steps:
17 |       - uses: actions/checkout@v4
18 |       - uses: astral-sh/ruff-action@v3
19 | ...
20 | 


--------------------------------------------------------------------------------
/.github/workflows/summarize-jobs-reusable.yaml:
--------------------------------------------------------------------------------
  1 | ---
  2 | # https://ecanarys.com/supercharging-github-actions-with-job-summaries-and-pull-request-comments/
  3 | # FIXME currently bug in gha summaries ? $GITHUB_STEP_SUMMARY files are empty
  4 | # https://github.com/orgs/community/discussions/110283
  5 | # https://github.com/orgs/community/discussions/67991
  6 | # Possible workaround
  7 | # echo ${{ fromJSON(step).name }}" >> $GITHUB_STEP_SUMMARY
  8 | # echo ${{ fromJSON(step).outcome }}" >> $GITHUB_STEP_SUMMARY
  9 | # echo ${{ fromJSON(step).conclusion }}"
 10 | 
 11 | name: Summarize workflow jobs
 12 | 
 13 | on:
 14 |   workflow_call:
 15 |     outputs:
 16 |       summary:
 17 |         description: "Outputs summaries of jobs in a workflow"
 18 |         value: ${{ jobs.generate_summary.outputs.summary }}
 19 |     inputs:
 20 |       branch_to_summarize:
 21 |         required: false
 22 |         default: 'main'
 23 |         type: string
 24 |       summary_data:
 25 |         required: false
 26 |         type: string
 27 | 
 28 | jobs:
 29 |   generate_summary:
 30 |     name: Generate Summary
 31 |     runs-on: ubuntu-latest
 32 |     permissions:
 33 |       contents: read
 34 |       actions: read
 35 |       checks: read
 36 |       pull-requests: none
 37 |     outputs:
 38 |       summary: ${{ steps.add_changed_files.outputs.summary }}
 39 |     steps:
 40 | 
 41 |       - name: Add general information
 42 |         id: general_info
 43 |         run: |
 44 |           echo "# Job Summaries" >> $GITHUB_STEP_SUMMARY
 45 |           echo "Job: `${{ github.job }}`" >> $GITHUB_STEP_SUMMARY
 46 |           echo "Date: $(date +'%Y-%m-%d %H:%M:%S')" >> $GITHUB_STEP_SUMMARY
 47 | 
 48 |       - name: Add step states
 49 |         id: step_states
 50 |         run: |
 51 |           echo "### Steps:" >> $GITHUB_STEP_SUMMARY
 52 |           # loop summary_data if valid json
 53 |           if jq -e . >/dev/null 2>&1 <<< "${{ inputs.summary_data }}"; then
 54 |             jq -r '
 55 |               .steps[]
 56 |               | select(.conclusion != null)
 57 |               | "- **\(.name)**: \(
 58 |                 if .conclusion == "success" then ":white_check_mark:"
 59 |                 elif .conclusion == "failure" then ":x:"
 60 |                 else ":warning:" end
 61 |               )"
 62 |             ' <<< "${{ inputs.summary_data }}" >> $GITHUB_STEP_SUMMARY
 63 |           else
 64 |             echo "Invalid JSON in summary data." >> $GITHUB_STEP_SUMMARY
 65 |           fi
 66 | 
 67 |       - name: Checkout repo
 68 |         uses: actions/checkout@v4
 69 |         with:
 70 |           ref: "${{ inputs.branch_to_summarize }}"
 71 |           fetch-depth: 0
 72 | 
 73 |       - name: Add changed files since last push
 74 |         id: add_changed_files
 75 |         run: |
 76 |           # Get the tags
 77 |           # Use disabled lines to get last two commits
 78 |           # current=$(git show -s --format=%ci HEAD)
 79 |           # previous=$(git show -s --format=%ci HEAD~1)
 80 |           # git diff --name-only HEAD^ HEAD >> $GITHUB_STEP_SUMMARY
 81 |           version_tag_regex="^v[0-9]+\.[0-9]+\.[0-9]+$" # v0.0.0 
 82 |           tags=$(git tag --sort=-version:refname | \
 83 |             grep -E "${version_tag_regex}" || echo "")
 84 | 
 85 |           # Get latest and previous tags
 86 |           latest_tag=$(echo "${tags}" | head -n 1)
 87 |           previous_tag=$(echo "${tags}" | head -n 2 | tail -n 1)
 88 | 
 89 |           echo "tags: latest '${latest_tag}', previous '${previous_tag}'"
 90 | 
 91 |           # Write to summary
 92 |           error_msg="No files to output. Tag not found:"
 93 |           echo ${{ steps.step_states.outputs.summary }} >> $GITHUB_STEP_SUMMARY
 94 |           echo "## Changed files on '${{ inputs.branch_to_summarize }}'" >> $GITHUB_STEP_SUMMARY
 95 | 
 96 |           if [ -z "${latest_tag}" ]; then
 97 |             echo "${error_msg} latest" >> $GITHUB_STEP_SUMMARY
 98 |           elif [ -z "${previous_tag}" ]; then
 99 |             echo "${error_msg} previous" >> $GITHUB_STEP_SUMMARY
100 |           elif [ "${latest_tag}" == "${previous_tag}" ]; then
101 |             echo "Latest and previous tags are the same: '${latest_tag}'" >> $GITHUB_STEP_SUMMARY
102 |           else
103 |             # Get commit dates and hashes
104 |             latest_date=$(git log -1 --format=%ci $latest_tag)
105 |             previous_date=$(git log -1 --format=%ci $previous_tag)
106 |             current_hash=$(git rev-parse --short $latest_tag)
107 |             previous_hash=$(git rev-parse --short $previous_tag)
108 | 
109 |             # Append summary to the job summary
110 |             echo "Latest Tag Commit: '${latest_tag}' (${current_hash}) ${latest_date}" >> $GITHUB_STEP_SUMMARY
111 |             echo "Previous Tag Commit: '${previous_tag}' (${previous_hash}) ${previous_date}" >> $GITHUB_STEP_SUMMARY
112 |             echo "Files changed:" >> $GITHUB_STEP_SUMMARY
113 |             echo '```' >> $GITHUB_STEP_SUMMARY
114 |             git diff --name-only $previous_tag..$latest_tag >> $GITHUB_STEP_SUMMARY
115 |             echo '```' >> $GITHUB_STEP_SUMMARY
116 |           fi
117 | 
118 |       - name: Output error message in case of failure or cancel
119 |         if: failure() || cancelled()
120 |         run: |
121 |           if [ "${{ job.status }}" == "cancelled" ]; then
122 |             out_msg="## Workflow was cancelled"
123 |           else
124 |             out_msg="## Error in previous step"
125 |           fi
126 |           echo $out_msg >> $GITHUB_STEP_SUMMARY
127 | ...


--------------------------------------------------------------------------------
/.github/workflows/write-llms-txt.yaml:
--------------------------------------------------------------------------------
 1 | # TODO use local installation of repo to text
 2 | # https://github.com/itsitgroup/repo2txt
 3 | name: Write flattened repo to llms.txt
 4 | 
 5 | on:
 6 |   push:
 7 |     branches: [main]
 8 |   workflow_dispatch:
 9 |     inputs:
10 |       LLMS_TXT_PATH:
11 |         description: 'Path to the directory to save llsm.txt'
12 |         required: true
13 |         default: 'docs'
14 |         type: string
15 |       LLMS_TXT_NAME:
16 |         description: 'Path to the directory to save llsm.txt'
17 |         required: true
18 |         default: 'llms.txt'
19 |         type: string
20 |       CONVERTER_URL:
21 |         description: '[uithub|gittodoc]'  # |repo2txt
22 |         required: true
23 |         default: 'uithub.com'
24 |         type: choice
25 |         options:
26 |         - 'uithub.com'
27 |         - 'gittodoc.com'
28 |         # - 'repo2txt.com'
29 | 
30 | jobs:
31 |   generate-file:
32 |     runs-on: ubuntu-latest
33 | 
34 |     steps:
35 |       - name: Checkout repo
36 |         uses: actions/checkout@v4
37 | 
38 |       - name: Construct and create llms.txt path
39 |         id: construct_and_create_llms_txt_path
40 |         run: |
41 |           LLMS_TXT_PATH="${{ inputs.LLMS_TXT_PATH }}"
42 |           LLMS_TXT_PATH="${LLMS_TXT_PATH:-docs}"
43 |           LLMS_TXT_NAME="${{ inputs.LLMS_TXT_NAME }}"
44 |           LLMS_TXT_NAME="${LLMS_TXT_NAME:-llms.txt}"
45 |           echo "LLMS_TXT_FULL=${LLMS_TXT_PATH}/${LLMS_TXT_NAME}" >> $GITHUB_OUTPUT
46 |           mkdir -p "${LLMS_TXT_PATH}"
47 | 
48 |       - name: Fetch TXT from URL
49 |         run: |
50 |           LLMS_TXT_FULL=${{ steps.construct_and_create_llms_txt_path.outputs.LLMS_TXT_FULL }}
51 |           URL="https://${{ inputs.CONVERTER_URL }}/${{ github.repository }}"
52 |           echo "Fetching content from: ${URL}"
53 |           echo "Saving content to: ${LLMS_TXT_FULL}"
54 |           curl -s "${URL}" > "${LLMS_TXT_FULL}"
55 | 
56 |       - name: Commit and push file
57 |         run: |
58 |           LLMS_TXT_FULL=${{ steps.construct_and_create_llms_txt_path.outputs.LLMS_TXT_FULL }}
59 |           git config user.name "github-actions"
60 |           git config user.email "github-actions@github.com"
61 |           git add "${LLMS_TXT_FULL}"
62 |           git commit -m "feat(docs): Add/Update ${LLMS_TXT_FULL} adhering to [llmstxt.org](https://llmstxt.org/)."
63 |           git push
64 | 


--------------------------------------------------------------------------------
/.gitignore:
--------------------------------------------------------------------------------
 1 | # Python bytecode
 2 | __pycache__/
 3 | *.py[cod]
 4 | 
 5 | # environment
 6 | .venv/
 7 | *.env
 8 | unset_env.sh
 9 | 
10 | # Distribution / packaging
11 | build/
12 | dist/
13 | *.egg-info/
14 | 
15 | # Testing
16 | .pytest_cache/
17 | .coverage
18 | 
19 | # Logs
20 | *.log
21 | /logs
22 | 
23 | # Traces
24 | scalene-profiles
25 | profile.html
26 | profile.json
27 | 
28 | # OS generated files
29 | .DS_Store
30 | Thumbs.db
31 | 
32 | # IDE specific files (adjust as needed)
33 | # .vscode/
34 | # .idea/
35 | 
36 | # mkdocs
37 | reference/
38 | site/
39 | 
40 | # linting
41 | .ruff_cache
42 | 
43 | # type checking
44 | .mypy_cache/
45 | 
46 | # project specific
47 | wandb/
48 | 


--------------------------------------------------------------------------------
/.gitmessage:
--------------------------------------------------------------------------------
 1 | #<--- 72 characters --------------------------------------------------->
 2 | #
 3 | # Conventional Commits, semantic commit messages for humans and machines
 4 | # https://www.conventionalcommits.org/en/v1.0.0/
 5 | # Lint your conventional commits
 6 | # https://github.com/conventional-changelog/commitlint/tree/master/%40 \
 7 | #	commitlint/config-conventional
 8 | # Common types can be (based on Angular convention)
 9 | # build, chore, ci, docs, feat, fix, perf, refactor, revert, style, test
10 | # https://github.com/conventional-changelog/commitlint/tree/master/%40
11 | # Footer
12 | # https://git-scm.com/docs/git-interpret-trailers
13 | #
14 | #<--- pattern --------------------------------------------------------->
15 | #
16 | # <feat|fix|build|chore|ci|docs|style|refactor|perf|test>[(Scope)][!]: \
17 | #	<description>
18 | # short description: <type>[(<scope>)]: <subject>
19 | #
20 | # ! after scope in header indicates breaking change
21 | #
22 | # [optional body]
23 | #
24 | # - with bullets points
25 | #
26 | # [optional footer(s)]
27 | #
28 | # [BREAKING CHANGE:, Refs:, Resolves:, Addresses:, Reviewed by:]
29 | #
30 | #<--- usage ----------------------------------------------------------->
31 | #
32 | # Set locally (in the repository)
33 | # `git config commit.template .gitmessage`
34 | #
35 | # Set globally
36 | # `git config --global commit.template .gitmessage`
37 | #
38 | #<--- 72 characters --------------------------------------------------->


--------------------------------------------------------------------------------
/.streamlit/config.toml:
--------------------------------------------------------------------------------
 1 | [theme]
 2 | primaryColor="#f92aad"
 3 | backgroundColor="#0b0c10"
 4 | secondaryBackgroundColor="#1f2833"
 5 | textColor="#66fcf1"
 6 | font="monospace"
 7 | 
 8 | [server]
 9 | # enableCORS = false
10 | enableXsrfProtection = true
11 | 
12 | [browser]
13 | gatherUsageStats = false
14 | 
15 | [client]
16 | # toolbarMode = "minimal"
17 | showErrorDetails = true
18 | 


--------------------------------------------------------------------------------
/.vscode/extensions.json:
--------------------------------------------------------------------------------
 1 | {
 2 |     "recommendations": [
 3 |         "charliermarsh.ruff",
 4 |         "davidanson.vscode-markdownlint",
 5 |         "donjayamanne.githistory",
 6 |         "editorconfig.editorconfig",
 7 |         "gruntfuggly.todo-tree",
 8 |         "mhutchie.git-graph",
 9 |         "PKief.material-icon-theme",
10 |         "redhat.vscode-yaml",
11 |         "tamasfe.even-better-toml",
12 |         "yzhang.markdown-all-in-one",
13 | 
14 |         "github.copilot",
15 |         "github.copilot-chat",
16 |         "github.vscode-github-actions",
17 |         "ms-azuretools.vscode-docker",
18 |         "ms-python.debugpy",
19 |         "ms-python.python",
20 |         "ms-python.vscode-pylance",
21 |         "ms-vscode.makefile-tools",
22 |     ]
23 | }


--------------------------------------------------------------------------------
/.vscode/settings.json:
--------------------------------------------------------------------------------
 1 | {
 2 |     "editor.lineNumbers": "on",
 3 |     "editor.wordWrap": "on",
 4 |     "explorer.confirmDelete": true,
 5 |     "files.autoSave": "onFocusChange",
 6 |     "git.autofetch": true,
 7 |     "git.enableSmartCommit": true,
 8 |     "makefile.configureOnOpen": false,
 9 |     "markdownlint.config": {
10 |         "MD024": false,
11 |         "MD033": false
12 |     },
13 |     "python.analysis.extraPaths": ["./venv/lib/python3.13/site-packages"],
14 |     "python.defaultInterpreterPath": "./.venv/bin/python",
15 |     "python.analysis.typeCheckingMode": "strict",
16 |     "python.analysis.diagnosticSeverityOverrides": {
17 |         "reportMissingTypeStubs": "none",
18 |         "reportUnknownMemberType": "none",
19 |         "reportUnknownVariableType": "none"
20 |     },
21 |     "redhat.telemetry.enabled": false
22 | }


--------------------------------------------------------------------------------
/CHANGELOG.md:
--------------------------------------------------------------------------------
  1 | # Changelog
  2 | 
  3 | All notable changes to this project will be documented in this file.
  4 | 
  5 | The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
  6 | and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).
  7 | 
  8 | ## Guiding Principles
  9 | 
 10 | - Changelogs are for humans, not machines.
 11 | - There should be an entry for every single version.
 12 | - The same types of changes should be grouped.
 13 | - Versions and sections should be linkable.
 14 | - The latest version comes first.
 15 | - The release date of each version is displayed.
 16 | - Mention whether you follow Semantic Versioning.
 17 | 
 18 | ## Types of changes
 19 | 
 20 | - `Added` for new features.
 21 | - `Changed` for changes in existing functionality.
 22 | - `Deprecated` for soon-to-be removed features.
 23 | - `Removed` for now removed features.
 24 | - `Fixed` for any bug fixes.
 25 | - `Security` in case of vulnerabilities.
 26 | 
 27 | ## [Unreleased]
 28 | 
 29 | ### Changed
 30 | 
 31 | - Moved streamlit_gui and examples to /src
 32 | - Moved app to /src/app
 33 | 
 34 | ## [1.0.0] - 2025-03-18
 35 | 
 36 | ### 2025-03-18
 37 | 
 38 | - refactor(agent,streamlit): Convert main and run_manager functions again to async for streamli output
 39 | - fix(prompts): Update system prompts for manager,researcher and synthesiser roles to remove complexity
 40 | - chore(workflows): Update action versions in GitHub workflows for consistency
 41 | - chore(workflows): Update action versions for deploy docs to pgh-pages
 42 | - docs(deps): Add documentation dependencies for MkDocs and related plugins to pyproject.toml
 43 | 
 44 | ### 2025-03-17
 45 | 
 46 | - feat(main,agent): refactor entry point to support async execution and enhance login handling
 47 | - feat(cli,login,log): refactor entry point to integrate Typer, enhance logging, added login every run
 48 | - feat(streamlit): replace load_config with load_app_config, enhance sidebar rendering, and improve output rendering with type support
 49 | - feat(streamlit): enhance render_output function with detailed docstring and improve query handling in run_app
 50 | - feat(streamlit): enhance render_output function with additional info parameter and improve output handling in run_app
 51 | - feat(streamlit,app): add Typer dependency, update main entry point for async execution, add streamlit provider input
 52 | - feat(agent): update configuration and improve agent system setup with enhanced error handling and new environment variables
 53 | - feat(config,login,catch): add inference settings with usage limits and result retries, enhance login function to initialize environment and handle exceptions, comment out raise in error handling context to prevent unintended crashes
 54 | - feat(login,catch): integrate logfire configuration in login function and improve error handling context
 55 | 
 56 | ### 2025-03-16
 57 | 
 58 | - feta(devconatiner): Refactor devcontainer setup: remove old configurations and add new setup targets for development and Ollama
 59 | - feat(devcontainer): Changed from vscode to astral-sh devcontainer
 60 | - feat(devcontainer): Changed to vscode container, added postcreatecommand make setup_env
 61 | - feat(devcontainer): restructure environment setup with new devcontainer configurations
 62 | - feat(devcontainer): update environment names for clarity in devcontainer configurations
 63 | - refactor(agent): Added AgentConfig class for better agent configuration management, Refactored main function for streamlined agent initialization.
 64 | - feat(config,agents): Update model providers and enhance configuration management, examples: Added new model providers: Gemini and OpenRouter, src: Enabled streaming responses in the agent system
 65 | - chore: Remove unused prompt files, update configuration, and enhance logging setup
 66 | - refactor(exception,logfire): Enhance error handling and update model configurations in agent system
 67 | 
 68 | ### 2025-03-14
 69 | 
 70 | - feat(scalene): Add profiling support and update dependencies
 71 | - refactor(Makefile): Improve target descriptions and organization
 72 | 
 73 | ### 2025-03-13
 74 | 
 75 | - refactor(API,except): .env.example, add OpenRouter configuration, enhance error handling in run_simple_agent_system.py, and update ModelConfig to allow optional API key.
 76 | - feat(streamlit): add Streamlit app structure with header, footer, sidebar, and main content components
 77 | - feat(streamlit): enhance Streamlit app with detailed docstrings, improved header/footer, and refined main content layout
 78 | - feat(makefile,streamlit): update Makefile commands for CLI and GUI execution, and modify README for usage instructions, add streamlit config.toml
 79 | - feat(streamlit): restructure Streamlit app by removing unused components, adding new header, footer, sidebar, and output components, and updating configuration settings
 80 | - chore: replace app entrypoint with main, remove unused tools and tests, and update makefile for linting and type checking
 81 | - chore: Enhance makefile with coverage and help commands, update mkdocs.yaml and pyproject.toml for improved project structure and documentation
 82 | - test: Update makefile for coverage reporting, modify pyproject.toml to include pytest-cov, and adjust dependency settings
 83 | - test: Add coverage support with pytest-cov and update makefile for coverage reporting
 84 | - test: makefile for coverage reporting, update dependencies in pyproject.toml for improved testing and coverage support
 85 | - chore: Remove redundant help command from makefile
 86 | - refactor(agent,async): Refactor agent tests to use async fixtures and update verification methods for async results
 87 | - fix(Dockerfile): Remove unnecessary user creation and pip install commands from Dockerfile
 88 | - feat(agent): Update dependencies and add new example structures; remove obsolete files
 89 | - chore(structure): simplified agents.py
 90 | - fix(pyproject): Replace pydantic-ai with pydantic-ai-slim and update dependencies
 91 | - feat(examples): add new examples and data models; update configuration structure
 92 | - feat(agent): update dependencies, enhance examples, and introduce new data models for research and analysis agents
 93 | - feat(examples): enhance prompts structure and refactor research agent integration
 94 | - feat(examples): improve documentation and enhance error handling in agent examples
 95 | - feat(agent): Added data models and configuration for research and analysis agents, Added System C4 plantuml
 96 | - feat(weave,dependencies): update dependencies and integrate Weave for enhanced functionality in the agent system
 97 | - feat(agent): initialize agentops with API key and default tags for enhanced agent functionality
 98 | - feat(agent): integrate logfire for logging and configure initial logging settings
 99 | - feat(agent): adjust usage limits for ollama provider to enhance performance
100 | - feat(agent): refine system prompts and enhance data model structure for improved agent interactions
101 | - feat(agent): update system prompts for improved clarity and accuracy; add example environment configuration
102 | - feat(agent): enhance agent system with synthesiser functionality and update prompts for improved coordination
103 | - feat(agent): add Grok and Gemini API configurations; initialize logging and agent operations
104 | - feat(agent): improve documentation and refactor model configuration handling for agent system
105 | - feat(agent): update environment configuration, enhance logging, and refine agent management functionality
106 | - feat(agent): refactor login handling, update model retrieval, and enhance agent configuration
107 | 
108 | ## [0.0.2] - 2025-01-20
109 | 
110 | ### Added
111 | 
112 | - PRD.md
113 | - C4 architecture diagrams: system context, code
114 | - tests: basic agent evals, config.json
115 | 
116 | ### Changed
117 | 
118 | - make recipes
119 | 
120 | ## [0.0.1] - 2025-01-20
121 | 
122 | ### Added
123 | 
124 | - Makefile: setup, test, ruff
125 | - devcontainer: python only, w/o Jetbrains clutter from default devcontainer
126 | - ollama: server and model download successful
127 | - agent: tools use full run red
128 | - pytest: e2e runm final result red
129 | - Readme: basic project info
130 | - pyproject.toml
131 | 


--------------------------------------------------------------------------------
/Dockerfile:
--------------------------------------------------------------------------------
 1 | ARG APP_ROOT="/src"
 2 | ARG PYTHON_VERSION="3.12"
 3 | ARG USER="appuser"
 4 | 
 5 | 
 6 | # Stage 1: Builder Image
 7 | FROM python:${PYTHON_VERSION}-slim AS builder
 8 | LABEL author="qte77"
 9 | LABEL builder=true
10 | ENV PYTHONDONTWRITEBYTECODE=1 \
11 |     PYTHONUNBUFFERED=1
12 | COPY pyproject.toml uv.lock /
13 | RUN set -xe \
14 |     && pip install --no-cache-dir uv \
15 |     && uv sync --frozen
16 | 
17 | 
18 | # Stage 2: Runtime Image
19 | FROM python:${PYTHON_VERSION}-slim AS runtime
20 | LABEL author="qte77"
21 | LABEL runtime=true
22 | 
23 | ARG APP_ROOT
24 | ARG USER
25 | ENV PYTHONDONTWRITEBYTECODE=1 \
26 |     PYTHONUNBUFFERED=1 \
27 |     PYTHONPATH=${APP_ROOT} \
28 |     PATH="${APP_ROOT}:${PATH}"
29 | #    WANDB_KEY=${WANDB_KEY} \
30 | #    WANDB_DISABLE_CODE=true
31 | 
32 | USER ${USER}
33 | WORKDIR ${APP_ROOT}
34 | COPY --from=builder /.venv .venv
35 | COPY --chown=${USER}:${USER} ${APP_ROOT} .
36 | 
37 | CMD [ \
38 |     "uv", "run", \
39 |     "--locked", "--no-sync", \
40 |     "python", "-m", "." \
41 | ]
42 | 


--------------------------------------------------------------------------------
/LICENSE.md:
--------------------------------------------------------------------------------
 1 | # BSD 3-Clause License
 2 | 
 3 | Copyright (c) 2025 qte77
 4 | 
 5 | Redistribution and use in source and binary forms, with or without
 6 | modification, are permitted provided that the following conditions are met:
 7 | 
 8 | 1. Redistributions of source code must retain the above copyright notice, this
 9 |    list of conditions and the following disclaimer.
10 | 
11 | 2. Redistributions in binary form must reproduce the above copyright notice,
12 |    this list of conditions and the following disclaimer in the documentation
13 |    and/or other materials provided with the distribution.
14 | 
15 | 3. Neither the name of the copyright holder nor the names of its
16 |    contributors may be used to endorse or promote products derived from
17 |    this software without specific prior written permission.
18 | 
19 | THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
20 | AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
21 | IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
22 | DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
23 | FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
24 | DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
25 | SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
26 | CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
27 | OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
28 | OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
29 | 


--------------------------------------------------------------------------------
/Makefile:
--------------------------------------------------------------------------------
  1 | # This Makefile automates the build, test, and clean processes for the project.
  2 | # It provides a convenient way to run common tasks using the 'make' command.
  3 | # It is designed to work with the 'uv' tool for managing Python environments and dependencies.
  4 | # Run `make help` to see all available recipes.
  5 | 
  6 | 
  7 | .SILENT:
  8 | .ONESHELL:
  9 | .PHONY: all setup_prod setup_dev setup_prod_ollama setup_dev_ollama setup_ollama start_ollama stop_ollama clean_ollama ruff run_cli run_gui run_profile test_all coverage_all type_check output_unset_app_env_sh help
 10 | # .DEFAULT: setup_dev_ollama
 11 | .DEFAULT_GOAL := setup_dev_ollama
 12 | 
 13 | SRC_PATH := src
 14 | APP_PATH := $(SRC_PATH)/app
 15 | GUI_PATH_ST := $(SRC_PATH)/run_gui.py
 16 | CHAT_CFG_FILE := $(APP_PATH)/config_chat.json
 17 | OLLAMA_SETUP_URL := https://ollama.com/install.sh
 18 | OLLAMA_MODEL_NAME := $$(jq -r '.providers.ollama.model_name' $(CHAT_CFG_FILE))
 19 | 
 20 | setup_prod:  ## Install uv and deps, Download and start Ollama 
 21 | 	echo "Setting up prod environment ..."
 22 | 	pip install uv -q
 23 | 	uv sync --frozen
 24 | 
 25 | setup_dev:  ## Install uv and deps, Download and start Ollama 
 26 | 	echo "Setting up dev environment ..."
 27 | 	pip install uv -q
 28 | 	uv sync --all-groups
 29 | 
 30 | setup_prod_ollama:
 31 | 	$(MAKE) -s setup_prod
 32 | 	$(MAKE) -s setup_ollama
 33 | 	$(MAKE) -s start_ollama
 34 | 
 35 | setup_dev_ollama:
 36 | 	$(MAKE) -s setup_dev
 37 | 	$(MAKE) -s setup_ollama
 38 | 	$(MAKE) -s start_ollama
 39 | 
 40 | setup_dev_claude:
 41 | 	$(MAKE) -s setup_dev
 42 | 	$(MAKE) -s setup_claude_code
 43 | 
 44 | setup_claude_code:  ## Setup claude code CLI, node.js and npm have to be present
 45 | 	echo "Setting up claude code ..."
 46 | 	npm install -g @anthropic-ai/claude-code
 47 | 	claude config set --global preferredNotifChannel terminal_bell
 48 | 	echo "npm version: $$(npm --version)"
 49 | 	claude --version
 50 | 
 51 | # Ollama BINDIR in /usr/local/bin /usr/bin /bin 
 52 | setup_ollama:  ## Download Ollama, script does start local Ollama server
 53 | 	echo "Downloading Ollama binary... Using '$(OLLAMA_SETUP_URL)'."
 54 | 	# script does start server but not consistently
 55 | 	curl -fsSL $(OLLAMA_SETUP_URL) | sh
 56 | 	echo "Pulling model '$(OLLAMA_MODEL_NAME)' ..."
 57 | 	ollama pull $(OLLAMA_MODEL_NAME)
 58 | 
 59 | start_ollama:  ## Start local Ollama server, default 127.0.0.1:11434
 60 | 	ollama serve
 61 | 
 62 | stop_ollama:  ## Stop local Ollama server
 63 | 	echo "Stopping Ollama server..."
 64 | 	pkill ollama
 65 | 
 66 | clean_ollama:  ## Remove local Ollama from system
 67 | 	echo "Searching for Ollama binary..."
 68 | 	for BINDIR in /usr/local/bin /usr/bin /bin; do
 69 | 		if echo $$PATH | grep -q $$BINDIR; then
 70 | 			echo "Ollama binary found in '$$BINDIR'"
 71 | 			BIN="$$BINDIR/ollama"
 72 | 			break
 73 | 		fi
 74 | 	done
 75 | 	echo "Cleaning up..."
 76 | 	rm -f $(BIN)
 77 | 
 78 | ruff:  ## Lint: Format and check with ruff
 79 | 	uv run ruff format
 80 | 	uv run ruff check --fix
 81 | 
 82 | run_cli:  ## Run app on CLI only
 83 | 	path=$$(echo "$(APP_PATH)" | tr '/' '.')
 84 | 	uv run python -m $${path}.main $(ARGS)
 85 | 
 86 | run_gui:  ## Run app with Streamlit GUI
 87 | 	uv run streamlit run $(GUI_PATH_ST)
 88 | 
 89 | run_profile:  ## Profile app with scalene
 90 | 	uv run scalene --outfile \
 91 | 		"$(APP_PATH)/scalene-profiles/profile-$(date +%Y%m%d-%H%M%S)" \
 92 | 		"$(APP_PATH)/main.py"
 93 | 
 94 | test_all:  ## Run all tests
 95 | 	uv run pytest
 96 | 	
 97 | coverage_all:  ## Get test coverage
 98 | 	uv run coverage run -m pytest || true
 99 | 	uv run coverage report -m
100 | 
101 | type_check:  ## Check for static typing errors
102 | 	uv run mypy $(APP_PATH)
103 | 
104 | output_unset_app_env_sh:  ## Unset app environment variables
105 | 	uf="./unset_env.sh"
106 | 	echo "Outputing '$${uf}' ..."
107 | 	printenv | awk -F= '/_API_KEY=/ {print "unset " $$1}' > $$uf
108 | 
109 | help:
110 | 	# TODO add stackoverflow source
111 | 	echo "Usage: make [recipe]"
112 | 	echo "Recipes:"
113 | 	awk '/^[a-zA-Z0-9_-]+:.*?##/ {
114 | 		helpMessage = match($$0, /## (.*)/)
115 | 		if (helpMessage) {
116 | 			recipe = $$1
117 | 			sub(/:/, "", recipe)
118 | 			printf "  \033[36m%-20s\033[0m %s\n", recipe, substr($$0, RSTART + 3, RLENGTH)
119 | 		}
120 | 	}' $(MAKEFILE_LIST)
121 | 


--------------------------------------------------------------------------------
/README.md:
--------------------------------------------------------------------------------
  1 | # Agents-eval
  2 | 
  3 | This project aims to implement an evaluation pipeline to assess the effectiveness of open-source agentic AI systems across various use cases, focusing on use case agnostic metrics that measure core capabilities such as task decomposition, tool integration, adaptability, and overall performance.
  4 | 
  5 | ![License](https://img.shields.io/badge/license-BSD3Clause-green.svg)
  6 | ![Version](https://img.shields.io/badge/version-1.0.0-58f4c2)
  7 | [![CodeQL](https://github.com/qte77/Agents-eval/actions/workflows/codeql.yaml/badge.svg)](https://github.com/qte77/Agents-eval/actions/workflows/codeql.yaml)
  8 | [![CodeFactor](https://www.codefactor.io/repository/github/qte77/Agents-eval/badge)](https://www.codefactor.io/repository/github/qte77/Agents-eval)
  9 | [![ruff](https://github.com/qte77/Agents-eval/actions/workflows/ruff.yaml/badge.svg)](https://github.com/qte77/Agents-eval/actions/workflows/ruff.yaml)
 10 | [![pytest](https://github.com/qte77/Agents-eval/actions/workflows/pytest.yaml/badge.svg)](https://github.com/qte77/Agents-eval/actions/workflows/pytest.yaml)
 11 | [![Link Checker](https://github.com/qte77/Agents-eval/actions/workflows/links-fail-fast.yaml/badge.svg)](https://github.com/qte77/Agents-eval/actions/workflows/links-fail-fast.yaml)
 12 | [![Deploy Docs](https://github.com/qte77/Agents-eval/actions/workflows/generate-deploy-mkdocs-ghpages.yaml/badge.svg)](https://github.com/qte77/Agents-eval/actions/workflows/generate-deploy-mkdocs-ghpages.yaml)
 13 | 
 14 | **DevEx** [![vscode.dev](https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=vscode.dev&labelColor=2c2c32&color=007acc&logoColor=007acc)](https://vscode.dev/github/qte77/Agents-eval)
 15 | [![Codespace Dev](https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Codespace%20Dev&labelColor=2c2c32&color=007acc&logoColor=007acc)](https://github.com/codespaces/new?repo=qte77/Agents-eval&devcontainer_path=.devcontainer/setup_dev/devcontainer.json)
 16 | [![Codespace Dev Claude Code](https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Codespace%20Dev%20Claude%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc)](https://github.com/codespaces/new?repo=qte77/Agents-eval&devcontainer_path=.devcontainer/setup_dev_claude/devcontainer.json)
 17 | [![Codespace Dev Ollama](https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Codespace%20Dev%20Ollama&labelColor=2c2c32&color=007acc&logoColor=007acc)](https://github.com/codespaces/new?repo=qte77/Agents-eval&devcontainer_path=.devcontainer/setup_dev_ollama/devcontainer.json)
 18 | [![TalkToGithub](https://img.shields.io/badge/TalkToGithub-7a83ff.svg)](https://talktogithub.com/qte77/Agents-eval)
 19 | [![llms.txt (UitHub)](https://img.shields.io/badge/llms.txt-uithub-800080.svg)](https://github.com/qte77/Agents-eval)
 20 | [![llms.txt (GitToDoc)](https://img.shields.io/badge/llms.txt-GitToDoc-fe4a60.svg)](https://gittodoc.com/qte77/Agents-eval)
 21 | 
 22 | ## Status
 23 | 
 24 | (DRAFT) (WIP) ----> Not fully implemented yet
 25 | 
 26 | For version history have a look at the [CHANGELOG](CHANGELOG.md).
 27 | 
 28 | ## Setup and Usage
 29 | 
 30 | - `make setup_prod`
 31 | - `make setup_dev` or `make setup_dev_claude` or `make setup_dev_ollama`
 32 | - `make run_cli` or `make run_cli ARGS="--help"`
 33 | - `make run_gui`
 34 | - `make test_all`
 35 | 
 36 | ### Configuration
 37 | 
 38 | - [config_app.py](src/app/config/config_app.py) contains configuration constants for the application.
 39 | - [config_chat.json](src/app/config/config_chat.json) contains inference provider configuration and prompts. inference endpoints used should adhere to [OpenAI Model Spec 2024-05-08](https://cdn.openai.com/spec/model-spec-2024-05-08.html) which is used by [pydantic-ai OpenAI-compatible Models](https://ai.pydantic.dev/models/#openai-compatible-models).
 40 | - [config_eval.json](src/app/config/config_eval.json) contains evaluation metrics and their weights.
 41 | - [data_models.py](src/app/config/data_models.py) contains the pydantic data models for agent system configuration and results.
 42 | 
 43 | ### Environment
 44 | 
 45 | [.env.example](.env.example) contains examples for usage of API keys and variables.
 46 | 
 47 | ```text
 48 | # inference EP
 49 | GEMINI_API_KEY="xyz"
 50 | 
 51 | # tools
 52 | TAVILY_API_KEY=""
 53 | 
 54 | # log/mon/trace
 55 | WANDB_API_KEY="xyz"
 56 | ```
 57 | 
 58 | ### Customer Journey
 59 | 
 60 | <details>
 61 |   <summary>Show Customer Journey</summary>
 62 |   <img src="assets/images/customer-journey-activity-light.png#gh-light-mode-only" alt="Customer Journey" title="Customer Journey" width="60%" />
 63 |   <img src="assets/images/customer-journey-activity-dark.png#gh-dark-mode-only" alt="Customer Journey" title="Customer Journey" width="60%" />
 64 | </details>
 65 | 
 66 | ### Note
 67 | 
 68 | 1. The contained chat configuration uses free inference endpoints which are subject to change by the providers. See lists such as [free-llm-api-resources](https://github.com/cheahjs/free-llm-api-resources) to find other providers.
 69 | 2. The contained chat configuration uses models which are also subject to change by the providers and have to be updated from time to time.
 70 | 3. LLM-as-judge is also subject to the chat configuration.
 71 | 
 72 | ## Documentation
 73 | 
 74 | [Agents-eval](https://qte77.github.io/Agents-eval)
 75 | 
 76 | ### Project Outline
 77 | 
 78 | `# TODO`
 79 | 
 80 | ### Datasets used
 81 | 
 82 | `# TODO`
 83 | 
 84 | ### Evalutions metrics
 85 | 
 86 | #### # TODO
 87 | 
 88 | - Time to complete task (time_taken)
 89 | - Task success rate (task_success)
 90 | - Agent coordination (coordination_quality)
 91 | - Tool usage efficiency (tool_efficiency)
 92 | - Plan coherence (planning_rational)
 93 | - Text response quality (text_similarity)
 94 | - Autonomy vs. human intervention (HITL, user feedback)
 95 | - Reactivity (adapt to changes of tasks and environments)
 96 | - Memory consistency
 97 | 
 98 | ### Evaluations Metrics Baseline
 99 | 
100 | As configured in [config_eval.json](src/app/config/config_eval.json).
101 | 
102 | ```json
103 | {
104 |     "evaluators_and_weights": {
105 |         "planning_rational": "1/6",
106 |         "task_success": "1/6",
107 |         "tool_efficiency": "1/6",
108 |         "coordination_quality": "1/6",
109 |         "time_taken": "1/6",
110 |         "text_similarity": "1/6"
111 |     }
112 | }
113 | ```
114 | 
115 | ### Eval Metrics Sweep
116 | 
117 | <details>
118 |   <summary>Eval Metrics Sweep</summary>
119 |   <img src="assets/images/metrics-eval-sweep.png" alt="Eval Metrics Sweep" title="Eval Metrics Sweep" width="60%" />
120 | </details>
121 | 
122 | ### Tools available
123 | 
124 | Other pydantic-ai agents and [pydantic-ai DuckDuckGo Search Tool](https://ai.pydantic.dev/common-tools/#duckduckgo-search-tool).
125 | 
126 | ### Agentic System Architecture
127 | 
128 | <details>
129 |   <summary>Show Agentic System Architecture</summary>
130 |   <img src="assets/images/c4-multi-agent-system.png#gh-dark-mode-only" alt="Agentic System C4-Arch" title="Agentic System C4-Arch" width="60%" />
131 | </details>
132 | 
133 | ### Project Repo Structure
134 | 
135 | <details>
136 |   <summary>Show Repo Structure</summary>
137 | ```sh
138 | |- .devcontainer  # pre-configured dev env
139 | |- .github  # workflows
140 | |- .streamlit  # config.toml
141 | |- .vscode  # extensions, settings
142 | |- assets/images
143 | |- docs
144 | |- src  # source code
145 |    |- app
146 |       |- agents
147 |       |- config
148 |       |- evals
149 |       |- utils
150 |       \- main.py
151 |    |- examples
152 |    |- gui
153 |    \- run_gui.py
154 | |- tests
155 | |- .env.example  # example env vars
156 | |- CHANGEOG.md  # short project history
157 | |- Dockerfile  # create app image
158 | |- Makefile  # helper scripts
159 | |- mkdocs.yaml  # docu from docstrings
160 | |- pyproject.toml  # project settings
161 | |- README.md  # project description
162 | \- uv.lock  # resolved package versions
163 | ```
164 | </details>
165 | 
166 | ## Landscape overview
167 | 
168 | ### Agentic System Frameworks
169 | 
170 | - [PydanticAI](https://github.com/pydantic/pydantic-ai)
171 | - [restack](https://www.restack.io/)
172 | - [smolAgents](https://github.com/huggingface/smolagents)
173 | - [AutoGen](https://github.com/microsoft/autogen)
174 | - [Semantic Kernel](https://github.com/microsoft/semantic-kernel)
175 | - [CrewAI](https://github.com/crewAIInc/crewAI)
176 | - [Langchain](https://github.com/langchain-ai/langchain)
177 | - [Langflow](https://github.com/langflow-ai/langflow)
178 | 
179 | ### Agent-builder
180 | 
181 | - [Archon](https://github.com/coleam00/Archon)
182 | - [Agentstack](https://github.com/AgentOps-AI/AgentStack)
183 | 
184 | ### Evaluation
185 | 
186 | - Focusing on agentic systems
187 |   - [AgentNeo](https://github.com/raga-ai-hub/agentneo)
188 |   - [AutoGenBench](https://github.com/microsoft/autogen/blob/0.2/samples/tools/autogenbench)
189 |   - [Langchain AgentEvals](https://github.com/langchain-ai/agentevals)
190 |   - [Mosaic AI Agent Evaluation](https://docs.databricks.com/en/generative-ai/agent-evaluation/index.html)
191 |   - [RagaAI-Catalyst](https://github.com/raga-ai-hub/RagaAI-Catalyst)
192 |   - [AgentBench](https://github.com/THUDM/AgentBench)
193 | - RAG oriented
194 |   - [RAGAs](https://github.com/explodinggradients/ragas)
195 | - LLM apps
196 |   - [DeepEval](https://github.com/confident-ai/deepeval)
197 |   - [Langchain OpenEvals](https://github.com/langchain-ai/openevals)
198 |   - [MLFlow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)
199 |   - [DeepEval (DeepSeek)]( github.com/confident-ai/deepeval)
200 | 
201 | ### Observation, Monitoring, Tracing
202 | 
203 | - [AgentOps - Agency](https://www.agentops.ai/)
204 | - [arize](https://arize.com/)
205 | - [Langtrace](https://www.langtrace.ai/)
206 | - [LangSmith - Langchain](https://www.langchain.com/langsmith)
207 | - [Weave - Weights & Biases](https://wandb.ai/site/weave/)
208 | - [Pydantic- Logfire](https://pydantic.dev/logfire)
209 | 
210 | ### Datasets
211 | 
212 | - [awesome-reasoning - Collection of datasets](https://github.com/neurallambda/awesome-reasoning)
213 | 
214 | #### Scientific
215 | 
216 | - [SWIF2T](https://arxiv.org/abs/2405.20477), Automated Focused Feedback Generation for Scientific Writing Assistance, 2024, 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation
217 | - [PeerRead](https://github.com/allenai/PeerRead), A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications, 2018, 14K paper drafts and the corresponding accept/reject decisions, over 10K textual peer reviews written by experts for a subset of the papers, structured JSONL, clear labels
218 | - [BigSurvey](https://www.ijcai.org/proceedings/2022/0591.pdf), Generating a Structured Summary of Numerous Academic Papers: Dataset and Method, 2022, 7K survey papers and 430K referenced papers abstracts
219 | - [SciXGen](https://arxiv.org/abs/2110.10774), A Scientific Paper Dataset for Context-Aware Text Generation, 2021, 205k papers
220 | - [scientific_papers](https://huggingface.co/datasets/armanc/scientific_papers), 2018, two sets of long and structured documents, obtained from ArXiv and PubMed OpenAccess, 300k+ papers, total disk 7GB
221 | 
222 | #### Reasoning, Deduction, Commonsense, Logic
223 | 
224 | - [LIAR](https://www.cs.ucsb.edu/~william/data/liar_dataset.zip), fake news detection, only 12.8k records, single label
225 | - [X-Fact](https://github.com/utahnlp/x-fact/), Benchmark Dataset for Multilingual Fact Checking, 31.1k records, large, multilingual
226 | - [MultiFC](https://www.copenlu.com/publication/2019_emnlp_augenstein/), A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims, 34.9k records
227 | - [FEVER](https://fever.ai/dataset/fever.html), Fact Extraction and VERification, 185.4k records
228 | - TODO GSM8K, bAbI, CommonsenseQA, DROP, LogiQA, MNLI
229 | 
230 | #### Planning, Execution
231 | 
232 | - [Plancraft](https://arxiv.org/abs/2412.21033), an evaluation dataset for planning with LLM agents, both a text-only and multi-modal interface
233 | - [IDAT](https://arxiv.org/abs/2407.08898), A Multi-Modal Dataset and Toolkit for Building and Evaluating Interactive Task-Solving Agents
234 | - [PDEBench](https://github.com/pdebench/PDEBench), set of benchmarks for scientific machine learning
235 | - [MatSci-NLP](https://arxiv.org/abs/2305.08264), evaluating the performance of natural language processing (NLP) models on materials science text
236 | - TODO BigBench Hard, FSM Game
237 | 
238 | #### Tool Use, Function Invocation
239 | 
240 | - [Trelis Function Calling](https://huggingface.co/datasets/Trelis/function_calling_v3)
241 | - [KnowLM Tool](https://huggingface.co/datasets/zjunlp/KnowLM-Tool)
242 | - [StatLLM](https://arxiv.org/abs/2502.17657), statistical analysis tasks, LLM-generated SAS code, and human evaluation scores
243 | - TODO ToolComp
244 | 
245 | ### Benchmarks
246 | 
247 | - [SciArena: A New Platform for Evaluating Foundation Models in Scientific Literature Tasks](https://allenai.org/blog/sciarena)
248 | - [AgentEvals CORE-Bench Leaderboard](https://huggingface.co/spaces/agent-evals/core_leaderboard)
249 | - [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)
250 | - [Chatbot Arena LLM Leaderboard](https://lmsys.org/projects/)
251 | - [GAIA Leaderboard](https://gaia-benchmark-leaderboard.hf.space/)
252 | - [GalileoAI Agent Leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard)
253 | - [WebDev Arena Leaderboard](https://web.lmarena.ai/leaderboard)
254 | - [MiniWoB++: a web interaction benchmark for reinforcement learning](https://miniwob.farama.org/)
255 | 
256 | ## Further Reading
257 | 
258 | - [[2506.18096] Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096), [gh / ai-agents-2030 / awesome-deep-research-agent](https://github.com/ai-agents-2030/awesome-deep-research-agent)
259 | - [[2504.19678] From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review](https://arxiv.org/abs/2504.19678)
260 | - [[2503.21460] Large Language Model Agent: A Survey on Methodology, Applications and Challenges](https://arxiv.org/abs/2503.21460)
261 | - [[2503.16416] Survey on Evaluation of LLM-based Agents](https://arxiv.org/abs/2503.16416)
262 | - [[2503.13657] Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657)
263 | - [[2502.14776] SurveyX: Academic Survey Automation via Large Language Models](https://arxiv.org/abs/2502.14776)
264 | - [[2502.05957] AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents](https://arxiv.org/abs/2502.05957)
265 | - [[2502.02649] Fully Autonomous AI Agents Should Not be Developed](https://arxiv.org/abs/2502.02649)
266 | - [[2501.16150] AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants](https://arxiv.org/abs/2501.16150)
267 | - [[2501.06590] ChemAgent](https://arxiv.org/abs/2501.06590)
268 | - [[2501.06322] Multi-Agent Collaboration Mechanisms: A Survey of LLMs](https://arxiv.org/abs/2501.06322)
269 | - [[2501.04227] Agent Laboratory: Using LLM Agents as Research Assitants](https://arxiv.org/abs/2501.04227), [AgentRxiv:Towards Collaborative Autonomous Research](https://agentrxiv.github.io/)
270 | - [[2501.00881] Agentic Systems: A Guide to Transforming Industries with Vertical AI Agents](https://arxiv.org/abs/2501.00881)
271 | - [[2412.04093] Practical Considerations for Agentic LLM Systems](https://arxiv.org/abs/2412.04093)
272 | - [[2411.13768] Evaluation-driven Approach to LLM Agents](https://arxiv.org/abs/2411.13768)
273 | - [[2411.10478] Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey](https://arxiv.org/abs/2411.10478)
274 | - [[2411.05285] A taxonomy of agentops for enabling observability of foundation model based agents](https://arxiv.org/abs/2411.05285)
275 | - [[2410.22457] Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration and Evaluation using Novel Metrics and Dataset](https://arxiv.org/abs/2410.22457)
276 | - [[2408.06361] Large Language Model Agent in Financial Trading: A Survey](https://arxiv.org/abs/2408.06361)
277 | - [[2408.06292] The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery](https://arxiv.org/abs/2408.06292)
278 | - [[2404.13501] A Survey on the Memory Mechanism of Large Language Model based Agents](https://arxiv.org/pdf/2404.13501)
279 | - [[2402.06360] CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models](https://arxiv.org/abs/2402.06360)
280 | - [[2402.02716] Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)
281 | - [[2402.01030] Executable Code Actions Elicit Better LLM Agents](https://arxiv.org/abs/2402.01030)
282 | - [[2308.11432] A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432)
283 | 


--------------------------------------------------------------------------------
/assets/images/c4-multi-agent-system.png:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/assets/images/c4-multi-agent-system.png


--------------------------------------------------------------------------------
/assets/images/customer-journey-activity-dark.png:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/assets/images/customer-journey-activity-dark.png


--------------------------------------------------------------------------------
/assets/images/customer-journey-activity-light.png:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/assets/images/customer-journey-activity-light.png


--------------------------------------------------------------------------------
/assets/images/metrics-eval-sweep.png:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/assets/images/metrics-eval-sweep.png


--------------------------------------------------------------------------------
/docs/PRD.md:
--------------------------------------------------------------------------------
 1 | # Product Requirements Document (PRD) for Agents-eval
 2 | 
 3 | ## Overview
 4 | 
 5 | **Agents-eval** is a project aimed at evaluating the effectiveness of open-source agentic AI systems across various use cases. The focus is on use case agnostic metrics that measure core capabilities such as task decomposition, tool integration, adaptability, and overall performance.
 6 | 
 7 | ## Goals
 8 | 
 9 | - **Evaluate Agentic AI Systems:** Provide a comprehensive evaluation pipeline to assess the performance of agentic AI systems.
10 | - **Metric Development:** Develop and implement metrics that are agnostic to specific use cases but measure core agentic capabilities.
11 | - **Continuous Improvement:** Promote continuous improvement through automated testing, version control, and documentation.
12 | 
13 | ## Functional Requirements
14 | 
15 | ### CLI
16 | 
17 | - **Command Line Interface:**
18 |   - Commands to start, stop, and check the status of the Ollama server or remote inference endpoints.
19 |   - Commands to download or call models and run tests.
20 | 
21 | ### Frontend (Streamlit)
22 | 
23 | - **User Interface:**
24 |   - Display test results and system performance metrics.
25 | 
26 | ### (Optional) Backend (FastAPI)
27 | 
28 | - **Agentic System Integration:**
29 |   - Support for adding tools to agents using Pydantic-AI.
30 |   - Ensure agents can use tools effectively and return expected results.
31 | - **Model Management:**
32 |   - Ability to download, list, and manage models using the `ollama` Python package.
33 | - **API Endpoints:**
34 |   - Endpoint to start and check the status of the Ollama server.
35 |   - Endpoint to download and manage models.
36 |   - Endpoint to run tests and return results.
37 | 
38 | ## Non-Functional Requirements
39 | 
40 | - **Maintainability:**
41 |   - Use modular design patterns for easy updates and maintenance.
42 |   - Implement logging and error handling for debugging and monitoring.
43 | - **Documentation:**
44 |   - Comprehensive documentation for setup, usage, and testing.
45 | - **Scalability:**
46 |   - Design the system to handle multiple concurrent requests.
47 | - **Performance:**
48 |   - Ensure low latency in server responses and model downloads.
49 |   - Optimize for memory usage and CPU/GPU utilization.
50 | - **Security:**
51 |   - Implement secure communication between components.
52 |   - Use environment variables for sensitive information.
53 | 
54 | ## Assumptions
55 | 
56 | - **Remote Inference Endpoints:** The project can use remote inference endpoints provided within a `config.json` and using API keys from `.env`.
57 | - **Local Ollama Server:** The project can make use of a local Ollama server for model hosting and inference.
58 | - **Python Environment:** The project uses Python 3.12 and related tools like `uv` for dependency management.
59 | - **GitHub Actions:** CI/CD pipelines are set up using GitHub Actions for automated testing, version bumping, and documentation deployment.
60 | 
61 | ## Constraints
62 | 
63 | - **Hardware:** The project assumes access to appropriate hardwareif running the Ollama server and models, including sufficient RAM and GPU capabilities.
64 | - **Software:** Requires Python 3.12, `uv`, and other dependencies listed in `pyproject.toml`.
65 | 
66 | ## Main Dependencies
67 | 
68 | - **Pydantic-AI:** For agent and tool management.
69 | - **Pytest:** For testing.
70 | - **Ollama:** For local model hosting and inference.
71 | - **Streamlit:** For frontend dashboard.
72 | - **Ruff:** For code linting.
73 | - **MkDocs:** For documentation generation.
74 | 
75 | ## Future Enhancements
76 | 
77 | - **Additional Metrics:** Develop more metrics to evaluate agentic systems.
78 | - **Integration with More Frameworks:** Expand compatibility with other agentic system frameworks. Meaning other popular agentic system frameworks like LangChain, AutoGen, CrewAI, LangGraph, Semantic Kernel, and smolAgents.
79 | - **Performance Optimization:** Further optimize for latency and resource usage.
80 | - **User Feedback:** Implement a feedback loop for users to report issues or suggest improvements.
81 | 


--------------------------------------------------------------------------------
/docs/SprintPlan.md:
--------------------------------------------------------------------------------
 1 | # Project Plan Outline
 2 | 
 3 | ## Week 1 starting 2025-03-31: Metric Development and CLI Enhancements
 4 | 
 5 | ### Milestones
 6 | 
 7 | - Metric Development: Implement at least three new metrics for evaluating agentic AI systems.
 8 | - CLI Streaming: Enhance the CLI to stream Pydantic-AI output.
 9 | 
10 | ### Tasks and Sequence
11 | 
12 | - [ ] Research and Design New Metrics
13 |   - Task Definition: Conduct literature review and design three new metrics that are agnostic to specific use cases but measure core agentic capabilities.
14 |   - Sequence: Before implementing any code changes.
15 |   - Definition of Done: A detailed document outlining the metrics, their mathematical formulations, and how they will be integrated into the evaluation pipeline.
16 | - [ ] Implement New Metrics
17 |   - Task Definition: Write Python code to implement the new metrics, ensuring they are modular and easily integratable with existing evaluation logic.
18 |   - Sequence: After completing the design document.
19 |   - Definition of Done: Unit tests for each metric pass, and they are successfully integrated into the evaluation pipeline.
20 | - [ ] Enhance CLI for Streaming
21 |   - Task Definition: Modify the CLI to stream Pydantic-AI output using asynchronous functions.
22 |   - Sequence: Concurrently with metric implementation.
23 |   - Definition of Done: The CLI can stream output from Pydantic-AI models without blocking, and tests demonstrate successful streaming.
24 | - [ ] Update Documentation
25 |   - Task Definition: Update PRD.md and README.md to reflect new metrics and CLI enhancements.
26 |   - Sequence: After completing metric implementation and CLI enhancements.
27 |   - Definition of Done: PRD.md includes detailed descriptions of new metrics, and README.md provides instructions on how to use the enhanced CLI.
28 | 
29 | ## Week 2 starting 2025-03-07: Streamlit GUI Enhancements and Testing
30 | 
31 | ### Milestones
32 | 
33 | - Streamlit GUI Output: Enhance the Streamlit GUI to display streamed output from Pydantic-AI.
34 | - Comprehensive Testing: Perform thorough testing of the entire system with new metrics and GUI enhancements.
35 | 
36 | ### Tasks and Sequence
37 | 
38 | - [ ] Enhance Streamlit GUI
39 |   - Task Definition: Modify the Streamlit GUI to display the streamed output from Pydantic-AI models.
40 |   - Sequence: Start of Week 2.
41 |   - Definition of Done: The GUI can display streamed output without errors, and user interactions (e.g., selecting models, inputting queries) work as expected.
42 | - [ ] Integrate New Metrics into GUI
43 |   - Task Definition: Ensure the Streamlit GUI can display results from the new metrics.
44 |   - Sequence: After enhancing the GUI for streamed output.
45 |   - Definition of Done: The GUI displays metric results clearly, and users can easily interpret the output.
46 | - [ ] Comprehensive System Testing
47 |   - Task Definition: Perform end-to-end testing of the system, including new metrics and GUI enhancements.
48 |   - Sequence: After integrating new metrics into the GUI.
49 |   - Definition of Done: All tests pass without errors, and the system functions as expected in various scenarios.
50 | - [ ] Finalize Documentation and Deployment
51 |   - Task Definition: Update MkDocs documentation to reflect all changes and deploy it to GitHub Pages.
52 |   - Sequence: After completing system testing.
53 |   - Definition of Done: Documentation is updated, and the latest version is live on GitHub Pages.
54 | 
55 | ## Additional Considerations
56 | 
57 | - Code Reviews: Schedule regular code reviews to ensure quality and adherence to project standards.
58 | - Feedback Loop: Establish a feedback loop with stakeholders to gather input on the new metrics and GUI enhancements.
59 | 


--------------------------------------------------------------------------------
/docs/UserStory.md:
--------------------------------------------------------------------------------
 1 | # User Story for Agents-eval
 2 | 
 3 | ## Introduction
 4 | 
 5 | Agents-eval is designed to evaluate the effectiveness of open-source agentic AI systems across various use cases. This user story focuses on the perspective of Gez, an AI researcher who aims to assess and improve these systems using Agents-eval.
 6 | 
 7 | ## User Profile
 8 | 
 9 | - **Name:** Gez
10 | - **Role:** AI Researcher
11 | - **Goals:**
12 |   - Evaluate the performance of agentic AI systems.
13 |   - Identify areas for improvement in these systems.
14 |   - Develop and integrate new metrics for evaluation.
15 | 
16 | ## User Story
17 | 
18 | **As** an AI researcher,
19 | **I want** to use Agents-eval to evaluate the effectiveness of agentic AI systems,
20 | **so that** I can assess their performance across different use cases and improve their capabilities.
21 | 
22 | ### Acceptance Criteria
23 | 
24 | 1. **Evaluation Pipeline:**
25 |    - The system should provide a comprehensive evaluation pipeline that measures core agentic capabilities such as task decomposition, tool integration, adaptability, and overall performance.
26 |    - The pipeline should support multiple agentic AI frameworks (e.g., Pydantic-AI, LangChain).
27 | 
28 | 2. **Metric Development:**
29 |    - The system should allow for the development and integration of new metrics that are agnostic to specific use cases.
30 |    - These metrics should be modular and easily integratable with existing evaluation logic.
31 | 
32 | 3. **CLI and GUI Interactions:**
33 |    - The system should offer both a CLI and a Streamlit GUI for user interaction.
34 |    - The CLI should support streaming output from Pydantic-AI models.
35 |    - The Streamlit GUI should display streamed output and provide an intuitive interface for setting up and running evaluations.
36 | 
37 | 4. **Documentation and Feedback:**
38 |    - The system should include comprehensive documentation for setup, usage, and testing.
39 |    - There should be a feedback loop for users to report issues or suggest improvements.
40 | 
41 | ## Example Scenario
42 | 
43 | - **Scenario:** Gez wants to evaluate a research agent system using Agents-eval.
44 | - **Steps:**
45 |   1. She sets up the environment using the CLI or devcontainer.
46 |   2. She configures the agent system with the desired models and tools.
47 |   3. She runs the evaluation using the CLI or Streamlit GUI.
48 |   4. She views the results and metrics displayed by the system.
49 |   5. She provides feedback on the system's performance and suggests improvements.
50 | 
51 | ## Benefits
52 | 
53 | - **Improved Evaluation Capabilities:** Agents-eval provides a structured approach to evaluating agentic AI systems, allowing researchers to focus on improving these systems.
54 | - **Flexibility and Customization:** The system supports multiple frameworks and allows for the development of new metrics, making it adaptable to various research needs.
55 | - **Enhanced User Experience:** The combination of CLI and GUI interfaces offers flexibility in how users interact with the system, catering to different preferences and workflows.
56 | 


--------------------------------------------------------------------------------
/docs/architecture/c4-multi-agent-system.plantuml:
--------------------------------------------------------------------------------
 1 | @startuml "Multi-Agent Research System - C4 System Context"
 2 | !theme plain
 3 | 
 4 | !include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Context.puml
 5 | !include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Container.puml
 6 | !include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Component.puml
 7 | 
 8 | LAYOUT_WITH_LEGEND()
 9 | 
10 | title "Multi-Agent Research System"
11 | Person(user, "User", "Submits research queries")
12 | 
13 | System_Boundary(research_system, "Supporting System") {
14 |     Container(main_module, "Main Module", "Python", "Entry point that configures and runs the agent system")
15 |     Container(utils, "Utilities", "Python", "Helper functions and data models")
16 |     Container(config, "Configuration", "JSON", "Provider and model settings")
17 | }
18 |     
19 | System_Boundary(agent_system, "Agent System") {
20 |     Container(manager_agent, "Manager Agent", "pydantic-ai", "Coordinates research and analysis tasks")
21 |     Container(research_agent, "Research Agent", "pydantic-ai", "Gathers information on topics")
22 |     Container(analysis_agent, "Analysis Agent", "pydantic-ai", "Analyzes research")
23 |     Container(synthesiser_agent, "Synthesiser Agent", "pydantic-ai", "Produces scientific reports")
24 | }
25 | 
26 | System_Ext(llm_provider, "LLM Provider", "External inference service for AI models")
27 | System_Ext(search_api, "DuckDuckGo Search", "External search API")
28 | 
29 | Rel(user, main_module, "Submits query", "CLI Input or GUI")
30 | Rel(main_module, config, "Loads", "Reads JSON config")
31 | Rel(main_module, agent_system, "Initializes and runs")
32 | 
33 | Rel(manager_agent, research_agent, "Delegates research tasks to", "Optional Tool call")
34 | Rel(manager_agent, analysis_agent, "Delegates analysis tasks to", "Optional Tool call")
35 | Rel(manager_agent, synthesiser_agent, "Delegates synthesis tasks to", "Optional Tool call")
36 | 
37 | Rel(research_agent, search_api, "Searches for information", "API call")
38 | 
39 | Rel(manager_agent, llm_provider, "Generates responses", "API call")
40 | Rel(research_agent, llm_provider, "Generates responses", "API call")
41 | Rel(analysis_agent, llm_provider, "Generates responses", "API call")
42 | Rel(synthesiser_agent, llm_provider, "Generates responses", "API call")
43 | 
44 | Rel(agent_system, utils, "Uses", "Import")
45 | Rel(main_module, utils, "Uses", "Import")
46 | 
47 | @enduml
48 | 


--------------------------------------------------------------------------------
/docs/architecture/customer-journey-activity-dark:
--------------------------------------------------------------------------------
 1 | @startuml
 2 | !theme amiga
 3 | skinparam monochrome true
 4 | 
 5 | title Customer Journey Activity Diagram for CLI and Streamlit
 6 | 
 7 | start
 8 | :Discover Agents-eval;
 9 | if (Choose CLI?) then (yes)
10 |   :Run CLI with `make run_cli`;
11 |   :Interact with CLI for agent setup and execution;
12 |   :View results and metrics in CLI output;
13 | else (no)
14 |   :Run Streamlit GUI with `make run_gui`;
15 |   :Interact with Streamlit for agent setup and execution;
16 |   :View results and metrics in Streamlit dashboard;
17 | endif
18 | :Continue using and provide feedback;
19 | :Improve based on feedback;
20 | 
21 | stop
22 | @enduml


--------------------------------------------------------------------------------
/docs/architecture/customer-journey-activity-light.plantuml:
--------------------------------------------------------------------------------
 1 | @startuml
 2 | !theme plain
 3 | 
 4 | title Customer Journey Activity Diagram for CLI and Streamlit
 5 | 
 6 | start
 7 | :Discover Agents-eval;
 8 | if (Choose CLI?) then (yes)
 9 |   :Run CLI with `make run_cli`;
10 |   :Interact with CLI for agent setup and execution;
11 |   :View results and metrics in CLI output;
12 | else (no)
13 |   :Run Streamlit GUI with `make run_gui`;
14 |   :Interact with Streamlit for agent setup and execution;
15 |   :View results and metrics in Streamlit dashboard;
16 | endif
17 | :Continue using and provide feedback;
18 | :Improve based on feedback;
19 | 
20 | stop
21 | @enduml


--------------------------------------------------------------------------------
/docs/architecture/metrics-eval-sweep.plantuml:
--------------------------------------------------------------------------------
 1 | @startuml
 2 | !theme plain
 3 | skinparam ConditionEndStyle diamond
 4 | skinparam ParticipantPadding 20
 5 | skinparam BoxPadding 20
 6 | 
 7 | participant "Sweep Engine" as SE
 8 | participant "Agentic System" as AS
 9 | participant "Evaluation Engine" as EE
10 | 
11 | SE -> EE: Set baseline parameters
12 | 
13 | group Sweep over parameter variations [Independent runs]
14 | 
15 |     group Vary number of runs [ numbers of runs ]
16 |         loop for each run_number
17 |             SE -> AS: Start runs
18 |             AS -> EE: Execute runs
19 |             EE--> SE: Send results
20 |         end
21 |     end
22 | 
23 |     group Sweep metrics weights [ metrics weights ]
24 |         loop for each weight_config
25 |             SE -> AS: Set weights and start runs
26 |             AS -> EE: Execute runs
27 |             EE--> SE: Send results
28 |         end
29 |     end
30 | 
31 | end
32 | @enduml
33 | 


--------------------------------------------------------------------------------
/mkdocs.yaml:
--------------------------------------------------------------------------------
 1 | ---
 2 | # https://github.com/james-willett/mkdocs-material-youtube-tutorial
 3 | # https://mkdocstrings.github.io/recipes/
 4 | # site info set in workflow
 5 | site_name: '<gha_sed_site_name_here>'
 6 | site_description: '<gha_sed_site_description_here>'
 7 | repo_url: '<gha_sed_repo_url_here>'
 8 | edit_uri: edit/main
 9 | theme:
10 |   name: material
11 |   language: en
12 |   features:
13 |     - content.code.annotation
14 |     - content.code.copy
15 |     - content.tabs.link
16 |     - navigation.footer
17 |     - navigation.sections
18 |     - navigation.tabs
19 |     - navigation.top
20 |     - toc.integrate
21 |     - search.suggest
22 |     - search.highlight
23 |   palette:
24 |     - media: "(prefers-color-scheme: light)"
25 |       scheme: default
26 |       toggle:
27 |         # icon: material/brightness-7
28 |         icon: material/toggle-switch-off-outline 
29 |         name: "Toggle Dark Mode"
30 |     - media: "(prefers-color-scheme: dark)"
31 |       scheme: slate
32 |       toggle:
33 |         # icon: material/brightness-4
34 |         icon: material/toggle-switch
35 |         name: "Toggle Light Mode"
36 | nav:
37 |   - Home: index.md
38 |   - PRD: PRD.md
39 |   - User Story: UserStory.md
40 |   - Sprint Plan: SprintPlan.md
41 |   - Code: docstrings.md
42 |   - Change Log: CHANGELOG.md
43 |   - License: LICENSE.md
44 |   - llms.txt: llms.txt
45 | plugins:
46 |   - search:
47 |       lang: en
48 |   - autorefs
49 |   - mkdocstrings:
50 |       handlers:
51 |         python:
52 |           paths: [src]
53 |           options:
54 |             show_root_heading: true
55 |             show_root_full_path: true
56 |             show_object_full_path: false
57 |             show_root_members_full_path: false
58 |             show_category_heading: true
59 |             show_submodules: true
60 | markdown_extensions:
61 |   - attr_list
62 |   - pymdownx.magiclink
63 |   - pymdownx.tabbed
64 |   - pymdownx.highlight:
65 |       anchor_linenums: true
66 |   - pymdownx.superfences
67 |   - pymdownx.snippets:
68 |       check_paths: true
69 |   - pymdownx.tasklist:
70 |       custom_checkbox: true
71 |   - sane_lists
72 |   - smarty
73 |   - toc:
74 |       permalink: true
75 | validation:
76 |   links:
77 |     not_found: warn
78 |     anchors: warn
79 | # builds only if validation succeeds while
80 | # threating warnings as errors
81 | # also checks for broken links
82 | # strict: true
83 | ...
84 | 


--------------------------------------------------------------------------------
/pyproject.toml:
--------------------------------------------------------------------------------
  1 | [build-system]
  2 | requires = ["hatchling"]
  3 | build-backend = "hatchling.build"
  4 | 
  5 | [project]
  6 | version = "1.0.0"
  7 | name = "Agents-eval"
  8 | description = "Assess the effectiveness of agentic AI systems across various use cases focusing on agnostic metrics that measure core agentic capabilities."
  9 | authors = [
 10 |     {name = "qte77", email = "qte@77.gh"}
 11 | ]
 12 | readme = "README.md"
 13 | requires-python = "==3.13.*"
 14 | license = "bsd-3-clause"
 15 | dependencies = [
 16 |     "agentops>=0.4.14",
 17 |     "logfire>=3.16.1",
 18 |     "loguru>=0.7.3",
 19 |     "pydantic>=2.10.6",
 20 |     # "pydantic-ai>=0.0.36",
 21 |     "pydantic-ai-slim[duckduckgo,openai,tavily]>=0.2.12",
 22 |     "pydantic-settings>=2.9.1",
 23 |     "scalene>=1.5.51",
 24 |     "weave>=0.51.49",
 25 | ]
 26 | 
 27 | # [project.urls]
 28 | # Documentation = ""
 29 | 
 30 | [dependency-groups]
 31 | dev = [
 32 |     # "commitizen>=4.4.1",
 33 |     "mypy>=1.16.0",
 34 |     "ruff>=0.11.12",
 35 | ]
 36 | gui = [
 37 |     "streamlit>=1.43.1",
 38 | ]
 39 | test = [
 40 |     "pytest>=8.3.4",
 41 |     "pytest-cov>=6.0.0",
 42 |     "pytest-asyncio>=0.25.3",
 43 |     "pytest-bdd>=8.1.0",
 44 |     "requests>=2.32.3",
 45 |     "ruff>=0.9.2",
 46 | ]
 47 | docs = [
 48 |     "griffe>=1.5.1",
 49 |     "mkdocs>=1.6.1",
 50 |     "mkdocs-awesome-pages-plugin>=2.9.3",
 51 |     "mkdocs-gen-files>=0.5.0",
 52 |     "mkdocs-literate-nav>=0.6.1",
 53 |     "mkdocs-material>=9.5.44",
 54 |     "mkdocs-section-index>=0.3.8",
 55 |     "mkdocstrings[python]>=0.27.0",
 56 | ]
 57 | 
 58 | [tool.uv]
 59 | package = true
 60 | exclude-newer = "2025-05-31T00:00:00Z"
 61 | 
 62 | [tool.hatch.build.targets.wheel]
 63 | only-include = ["/README.md"]
 64 | 
 65 | [tool.hatch.build.targets.sdist]
 66 | include = ["/README.md", "/Makefile", "/tests"]
 67 | 
 68 | [tool.logfire]
 69 | ignore_no_config=true
 70 | send_to_logfire="if-token-present"
 71 | 
 72 | [[tool.mypy.overrides]]
 73 | module = "agentops"
 74 | ignore_missing_imports = true
 75 | 
 76 | [tool.ruff]
 77 | target-version = "py313"
 78 | src = ["src", "tests"]
 79 | 
 80 | [tool.ruff.format]
 81 | docstring-code-format = true
 82 | 
 83 | [tool.ruff.lint]
 84 | # ignore = ["E203"]  # Whitespace before ':'
 85 | unfixable = ["B"]
 86 | select = [
 87 |     # pycodestyle
 88 |     "E",
 89 |     # Pyflakes
 90 |     "F",
 91 |     # pyupgrade
 92 |     "UP",
 93 |     # isort
 94 |     "I",
 95 | ]
 96 | 
 97 | [tool.ruff.lint.isort]
 98 | known-first-party = ["src", "tests"]
 99 | 
100 | [tool.ruff.lint.pydocstyle]
101 | convention = "google"
102 | 
103 | [tool.pytest.ini_options]
104 | addopts = "--strict-markers"
105 | # "function", "class", "module", "package", "session"
106 | asyncio_default_fixture_loop_scope = "function"
107 | pythonpath = ["src"]
108 | testpaths = ["tests/"]
109 | 
110 | [tool.coverage]
111 | [tool.coverage.run]
112 | include = [
113 |     "tests/**/*.py",
114 | ]
115 | # omit = []
116 | # branch = true
117 | 
118 | [tool.coverage.report]
119 | show_missing = true
120 | exclude_lines = [
121 |     # 'pragma: no cover',
122 |     'raise AssertionError',
123 |     'raise NotImplementedError',
124 | ]
125 | omit = [
126 |     'env/*',
127 |     'venv/*',
128 |     '.venv/*',
129 |     '*/virtualenv/*',
130 |     '*/virtualenvs/*',
131 |     '*/tests/*',
132 | ]
133 | 
134 | [tool.bumpversion]
135 | current_version = "1.0.0"
136 | parse = "(?P<major>\\d+)\\.(?P<minor>\\d+)\\.(?P<patch>\\d+)"
137 | serialize = ["{major}.{minor}.{patch}"]
138 | commit = true
139 | tag = true
140 | allow_dirty = false
141 | ignore_missing_version = false
142 | sign_tags = false
143 | tag_name = "v{new_version}"
144 | tag_message = "Bump version: {current_version} → {new_version}"
145 | message = "Bump version: {current_version} → {new_version}"
146 | commit_args = ""
147 | 
148 | [[tool.bumpversion.files]]
149 | filename = "pyproject.toml"
150 | search = 'version = "{current_version}"'
151 | replace = 'version = "{new_version}"'
152 | 
153 | [[tool.bumpversion.files]]
154 | filename = "src/app/__init__.py"
155 | search = '__version__ = "{current_version}"'
156 | replace = '__version__ = "{new_version}"'
157 | 
158 | [[tool.bumpversion.files]]
159 | filename = "README.md"
160 | search = "version-{current_version}-58f4c2"
161 | replace = "version-{new_version}-58f4c2"
162 | 
163 | [[tool.bumpversion.files]]
164 | filename = "CHANGELOG.md"
165 | search = """
166 | ## [Unreleased]
167 | """
168 | replace = """
169 | ## [Unreleased]
170 | 
171 | ## [{new_version}] - {now:%Y-%m-%d}
172 | """
173 | 


--------------------------------------------------------------------------------
/src/app/__init__.py:
--------------------------------------------------------------------------------
1 | """Defines the application version."""
2 | 
3 | __version__ = "1.0.0"
4 | 


--------------------------------------------------------------------------------
/src/app/agents/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/src/app/agents/__init__.py


--------------------------------------------------------------------------------
/src/app/agents/agent_system.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Agent system utilities for orchestrating multi-agent workflows.
  3 | 
  4 | This module provides functions and helpers to create, configure, and run agent
  5 | systems using Pydantic AI. It supports delegation of tasks to research, analysis, and
  6 | synthesis agents, and manages agent configuration, environment setup, and execution.
  7 | Args:
  8 |     provider (str): The name of the provider. provider_config (ProviderConfig):
  9 |         Configuration settings for the provider.
 10 |     api_key (str): API key for authentication with the provider.
 11 |     prompts (dict[str, str]): Configuration for prompts.
 12 |     include_researcher (bool): Flag to include the researcher agent.
 13 |     include_analyst (bool): Flag to include the analyst agent.
 14 |     include_synthesiser (bool): Flag to include the synthesiser agent.
 15 |     query (str | list[dict[str, str]]): The query or messages for the agent.
 16 |     chat_config (ChatConfig): The configuration object for agents and providers.
 17 |     usage_limits (UsageLimits): Usage limits for agent execution.
 18 |     pydantic_ai_stream (bool): Whether to use Pydantic AI streaming.
 19 | 
 20 | Functions:
 21 |     get_manager: Initializes and returns a manager agent with the specified
 22 |         configuration.
 23 |     run_manager: Asynchronously runs the manager agent with the given query and
 24 |         provider.
 25 |     setup_agent_env: Sets up the environment for an agent by configuring provider
 26 |         settings, prompts, API key, and usage limits.
 27 | """
 28 | 
 29 | from pydantic import BaseModel, ValidationError
 30 | from pydantic_ai import Agent, RunContext
 31 | from pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool
 32 | from pydantic_ai.messages import ModelRequest
 33 | from pydantic_ai.usage import UsageLimits
 34 | 
 35 | from app.agents.llm_model_funs import get_api_key, get_models, get_provider_config
 36 | from app.config.data_models import (
 37 |     AgentConfig,
 38 |     AnalysisResult,
 39 |     AppEnv,
 40 |     ChatConfig,
 41 |     EndpointConfig,
 42 |     ModelDict,
 43 |     ProviderConfig,
 44 |     ResearchResult,
 45 |     ResearchSummary,
 46 |     ResultBaseType,
 47 |     UserPromptType,
 48 | )
 49 | from app.utils.error_messages import generic_exception, invalid_data_model_format
 50 | from app.utils.log import logger
 51 | 
 52 | 
 53 | def _add_tools_to_manager_agent(
 54 |     manager_agent: Agent[None, BaseModel],
 55 |     research_agent: Agent[None, BaseModel] | None = None,
 56 |     analysis_agent: Agent[None, BaseModel] | None = None,
 57 |     synthesis_agent: Agent[None, BaseModel] | None = None,
 58 | ):
 59 |     """
 60 |     Adds tools to the manager agent for delegating tasks to research, analysis, and
 61 |         synthesis agents.
 62 |     Args:
 63 |         manager_agent (Agent): The manager agent to which tools will be added.
 64 |         research_agent (Agent): The agent responsible for handling research tasks.
 65 |         analysis_agent (Agent, optional): The agent responsible for handling
 66 |             analysis tasks. Defaults to None.
 67 |         synthesis_agent (Agent, optional): The agent responsible for handling
 68 |             synthesis tasks. Defaults to None.
 69 |     Returns:
 70 |         None
 71 |     """
 72 | 
 73 |     def _validate_model_return(
 74 |         result_output: str,
 75 |         result_model: type[ResultBaseType],
 76 |     ) -> ResultBaseType:
 77 |         """Validates the output against the expected model."""
 78 |         try:
 79 |             return result_model.model_validate(result_output)
 80 |         except ValidationError as e:
 81 |             msg = invalid_data_model_format(str(e))
 82 |             logger.error(msg)
 83 |             raise ValidationError(msg)
 84 |         except Exception as e:
 85 |             msg = generic_exception(str(e))
 86 |             logger.exception(msg)
 87 |             raise Exception(msg)
 88 | 
 89 |     if research_agent is not None:
 90 | 
 91 |         @manager_agent.tool
 92 |         # TODO remove redundant tool creation
 93 |         # ignore "delegate_research" is not accessed because of decorator
 94 |         async def delegate_research(  # type: ignore[reportUnusedFunction]
 95 |             ctx: RunContext[None], query: str
 96 |         ) -> ResearchResult:
 97 |             """Delegate research task to ResearchAgent."""
 98 |             result = await research_agent.run(query, usage=ctx.usage)
 99 |             return _validate_model_return(str(result.output), ResearchResult)
100 | 
101 |     if analysis_agent is not None:
102 | 
103 |         @manager_agent.tool
104 |         # ignore "delegate_research" is not accessed because of decorator
105 |         async def delegate_analysis(  # type: ignore[reportUnusedFunction]
106 |             ctx: RunContext[None], query: str
107 |         ) -> AnalysisResult:
108 |             """Delegate analysis task to AnalysisAgent."""
109 |             result = await analysis_agent.run(query, usage=ctx.usage)
110 |             return _validate_model_return(str(result.output), AnalysisResult)
111 | 
112 |     if synthesis_agent is not None:
113 | 
114 |         @manager_agent.tool
115 |         # ignore "delegate_research" is not accessed because of decorator
116 |         async def delegate_synthesis(  # type: ignore[reportUnusedFunction]
117 |             ctx: RunContext[None], query: str
118 |         ) -> ResearchSummary:
119 |             """Delegate synthesis task to AnalysisAgent."""
120 |             result = await synthesis_agent.run(query, usage=ctx.usage)
121 |             return _validate_model_return(str(result.output), ResearchSummary)
122 | 
123 | 
124 | def _create_agent(agent_config: AgentConfig) -> Agent[None, BaseModel]:
125 |     """Factory for creating configured agents"""
126 | 
127 |     return Agent(
128 |         model=agent_config.model,
129 |         output_type=agent_config.output_type,
130 |         system_prompt=agent_config.system_prompt,
131 |         tools=agent_config.tools,
132 |         retries=agent_config.retries,
133 |     )
134 | 
135 | 
136 | def _create_manager(
137 |     prompts: dict[str, str],
138 |     models: ModelDict,
139 | ) -> Agent[None, BaseModel]:
140 |     """
141 |     Creates and configures a manager Agent with associated researcher, analyst,
142 |     and optionally synthesiser agents.
143 |     Args:
144 |         prompts (Dict[str, str]): Dictionary containing system prompts for each agent.
145 |         model_manager (GeminiModel | OpenAIModel): Model to be used by the manager
146 |             agent.
147 |         model_researcher (GeminiModel | OpenAIModel | None, optional): Model to be used
148 |             by the researcher agent.
149 |         model_analyst (GeminiModel | OpenAIModel | None, optional): Model to be used by
150 |             the analyst agent. Defaults to None.
151 |         model_synthesiser (GeminiModel | OpenAIModel | None, optional): Model to be used
152 |             by the synthesiser agent. Defaults to None.
153 |     Returns:
154 |         Agent: Configured manager agent with associated tools and agents.
155 |     """
156 | 
157 |     status = f"Creating manager({models.model_manager.model_name})"
158 |     active_agents = [
159 |         agent
160 |         for agent in [
161 |             f"researcher({models.model_researcher.model_name})"
162 |             if models.model_researcher
163 |             else None,
164 |             f"analyst({models.model_analyst.model_name})"
165 |             if models.model_analyst
166 |             else None,
167 |             f"synthesiser({models.model_synthesiser.model_name})"
168 |             if models.model_synthesiser
169 |             else None,
170 |         ]
171 |         if agent
172 |     ]
173 |     status += f" with agents: {', '.join(active_agents)}" if active_agents else ""
174 |     logger.info(status)
175 | 
176 |     manager = _create_agent(
177 |         AgentConfig.model_validate(
178 |             {
179 |                 "model": models.model_manager,
180 |                 "output_type": ResearchResult,
181 |                 "system_prompt": prompts["system_prompt_manager"],
182 |             }
183 |         )
184 |     )
185 | 
186 |     if models.model_researcher is None:
187 |         researcher = None
188 |     else:
189 |         researcher = _create_agent(
190 |             AgentConfig.model_validate(
191 |                 {
192 |                     "model": models.model_researcher,
193 |                     "output_type": ResearchResult,
194 |                     "system_prompt": prompts["system_prompt_researcher"],
195 |                     "tools": [duckduckgo_search_tool()],
196 |                 }
197 |             )
198 |         )
199 | 
200 |     if models.model_analyst is None:
201 |         analyst = None
202 |     else:
203 |         analyst = _create_agent(
204 |             AgentConfig.model_validate(
205 |                 {
206 |                     "model": models.model_analyst,
207 |                     "output_type": AnalysisResult,
208 |                     "system_prompt": prompts["system_prompt_analyst"],
209 |                 }
210 |             )
211 |         )
212 | 
213 |     if models.model_synthesiser is None:
214 |         synthesiser = None
215 |     else:
216 |         synthesiser = _create_agent(
217 |             AgentConfig.model_validate(
218 |                 {
219 |                     "model": models.model_synthesiser,
220 |                     "output_type": AnalysisResult,
221 |                     "system_prompt": prompts["system_prompt_synthesiser"],
222 |                 }
223 |             )
224 |         )
225 | 
226 |     _add_tools_to_manager_agent(manager, researcher, analyst, synthesiser)
227 |     return manager
228 | 
229 | 
230 | def get_manager(
231 |     provider: str,
232 |     provider_config: ProviderConfig,
233 |     api_key: str | None,
234 |     prompts: dict[str, str],
235 |     include_researcher: bool = False,
236 |     include_analyst: bool = False,
237 |     include_synthesiser: bool = False,
238 | ) -> Agent[None, BaseModel]:
239 |     """
240 |     Initializes and returns a Agent manager with the specified configuration.
241 |     Args:
242 |         provider (str): The name of the provider.
243 |         provider_config (ProviderConfig): Configuration settings for the provider.
244 |         api_key (str): API key for authentication with the provider.
245 |         prompts (PromptsConfig): Configuration for prompts.
246 |         include_researcher (bool, optional): Flag to include analyst model.
247 |             Defaults to False.
248 |         include_analyst (bool, optional): Flag to include analyst model.
249 |             Defaults to False.
250 |         include_synthesiser (bool, optional): Flag to include synthesiser model.
251 |             Defaults to False.
252 |     Returns:
253 |         Agent: The initialized Agent manager.
254 |     """
255 | 
256 |     # FIXME context manager try-catch
257 |     # with error_handling_context("get_manager()"):
258 |     model_config = EndpointConfig.model_validate(
259 |         {
260 |             "provider": provider,
261 |             "prompts": prompts,
262 |             "api_key": api_key,
263 |             "provider_config": provider_config,
264 |         }
265 |     )
266 |     models = get_models(
267 |         model_config, include_researcher, include_analyst, include_synthesiser
268 |     )
269 |     return _create_manager(prompts, models)
270 | 
271 | 
272 | async def run_manager(
273 |     manager: Agent[None, BaseModel],
274 |     query: UserPromptType,
275 |     provider: str,
276 |     usage_limits: UsageLimits | None,
277 |     pydantic_ai_stream: bool = False,
278 | ) -> None:
279 |     """
280 |     Asynchronously runs the manager with the given query and provider, handling errors
281 |         and printing results.
282 |     Args:
283 |         manager (Agent): The system agent responsible for running the query.
284 |         query (str): The query to be processed by the manager.
285 |         provider (str): The provider to be used for the query.
286 |         usage_limits (UsageLimits): The usage limits to be applied during the query
287 |             execution.
288 |         pydantic_ai_stream (bool, optional): Flag to enable or disable Pydantic AI
289 |             stream. Defaults to False.
290 |     Returns:
291 |         None
292 |     """
293 | 
294 |     # FIXME context manager try-catch
295 |     # with out ? error_handling_context("run_manager()"):
296 |     model_name = getattr(manager, "model")._model_name
297 |     mgr_cfg = {"user_prompt": query, "usage_limits": usage_limits}
298 |     logger.info(f"Researching with {provider}({model_name}) and Topic: {query} ...")
299 | 
300 |     if pydantic_ai_stream:
301 |         raise NotImplementedError(
302 |             "Streaming currently only possible for Agents with "
303 |             "output_type str not pydantic model"
304 |         )
305 |         # logger.info("Streaming model response ...")
306 |         # result = await manager.run(**mgr_cfg)
307 |         # aync for chunk in result.stream_text():  # .run(**mgr_cfg) as result:
308 |         # async with manager.run_stream(user_prompt=query) as stream:
309 |         #    async for chunk in stream.stream_text():
310 |         #        logger.info(str(chunk))
311 |         # result = await stream.get_result()
312 |     else:
313 |         logger.info("Waiting for model response ...")
314 |         # FIXME deprecated warning manager.run(), query unknown type
315 |         # FIXME [call-overload] error: No overload variant of "run" of "Agent"
316 |         # matches argument type "dict[str, list[dict[str, str]] |
317 |         # Sequence[str | ImageUrl | AudioUrl | DocumentUrl | VideoUrl |
318 |         # BinaryContent] | UsageLimits | None]"
319 |         result = await manager.run(**mgr_cfg)  # type: ignore[reportDeprecated,reportUnknownArgumentType,reportCallOverload,call-overload]
320 | 
321 |     logger.info(f"Result: {result}")
322 |     logger.info(f"Usage statistics: {result.usage()}")
323 | 
324 | 
325 | def setup_agent_env(
326 |     provider: str,
327 |     query: UserPromptType,
328 |     chat_config: ChatConfig | BaseModel,
329 |     chat_env_config: AppEnv,
330 | ) -> EndpointConfig:
331 |     """
332 |     Sets up the environment for an agent by configuring provider settings, prompts,
333 |     API key, and usage limits.
334 | 
335 |     Args:
336 |         provider (str): The name of the provider.
337 |         query (UserPromptType): The messages or queries to be sent to the agent.
338 |         chat_config (ChatConfig | BaseModel): The configuration object containing
339 |             provider and prompt settings.
340 |         chat_env_config (AppEnv): The application environment configuration
341 |             containing API keys.
342 | 
343 |     Returns:
344 |         EndpointConfig: The configuration object for the agent.
345 |     """
346 | 
347 |     if not isinstance(chat_config, ChatConfig):
348 |         raise TypeError("'chat_config' of invalid type: ChatConfig expected")
349 |     msg: str | None
350 |     # FIXME context manager try-catch
351 |     # with error_handling_context("setup_agent_env()"):
352 |     provider_config = get_provider_config(provider, chat_config.providers)
353 | 
354 |     prompts = chat_config.prompts
355 |     api_key = get_api_key(provider, chat_env_config)
356 | 
357 |     if provider.lower() == "ollama":
358 |         # TODO move usage limits to config
359 |         usage_limits = UsageLimits(request_limit=10, total_tokens_limit=100000)
360 |     else:
361 |         if api_key is None:
362 |             msg = f"API key for provider '{provider}' is not set."
363 |             logger.error(msg)
364 |             raise ValueError(msg)
365 |         # TODO Separate Gemini request into function
366 |         if provider.lower() == "gemini":
367 |             if isinstance(query, str):
368 |                 query = ModelRequest.user_text_prompt(query)
369 |             elif isinstance(query, list):  # type: ignore[reportUnnecessaryIsInstance]
370 |                 # query = [
371 |                 #    ModelRequest.user_text_prompt(
372 |                 #        str(msg.get("content", ""))
373 |                 #    )  # type: ignore[reportUnknownArgumentType]
374 |                 #    if isinstance(msg, dict)
375 |                 #    else msg
376 |                 #    for msg in query
377 |                 # ]
378 |                 raise NotImplementedError("Currently conflicting with UserPromptType")
379 |             else:
380 |                 msg = f"Unsupported query type for Gemini: {type(query)}"
381 |                 logger.error(msg)
382 |                 raise TypeError(msg)
383 |         # TODO move usage limits to config
384 |         usage_limits = UsageLimits(request_limit=10, total_tokens_limit=10000)
385 | 
386 |     return EndpointConfig.model_validate(
387 |         {
388 |             "provider": provider,
389 |             "query": query,
390 |             "api_key": api_key,
391 |             "prompts": prompts,
392 |             "provider_config": provider_config,
393 |             "usage_limits": usage_limits,
394 |         }
395 |     )
396 | 


--------------------------------------------------------------------------------
/src/app/agents/llm_model_funs.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Utility functions and classes for managing and instantiating LLM models and providers.
  3 | 
  4 | This module provides functions to retrieve API keys, provider configurations, and
  5 | to create model instances for supported LLM providers such as Gemini and OpenAI.
  6 | It also includes logic for assembling model dictionaries for system agents.
  7 | """
  8 | 
  9 | from pydantic import HttpUrl
 10 | from pydantic_ai.models.gemini import GeminiModel
 11 | from pydantic_ai.models.openai import OpenAIModel
 12 | from pydantic_ai.providers.openai import OpenAIProvider
 13 | 
 14 | from app.config.config_app import API_SUFFIX
 15 | from app.config.data_models import AppEnv, EndpointConfig, ModelDict, ProviderConfig
 16 | from app.utils.error_messages import generic_exception, get_key_error
 17 | from app.utils.log import logger
 18 | 
 19 | 
 20 | def get_api_key(
 21 |     provider: str,
 22 |     chat_env_config: AppEnv,
 23 | ) -> str | None:
 24 |     """Retrieve API key from chat env config variable."""
 25 | 
 26 |     provider = provider.upper()
 27 |     if provider == "OLLAMA":
 28 |         return None
 29 |     else:
 30 |         key_name = f"{provider}{API_SUFFIX}"
 31 |         if hasattr(chat_env_config, key_name):
 32 |             logger.info(f"Found API key for provider '{provider}'")
 33 |             return getattr(chat_env_config, key_name)
 34 |         else:
 35 |             raise KeyError(
 36 |                 f"API key for provider '{provider}' not found in configuration."
 37 |             )
 38 | 
 39 | 
 40 | def get_provider_config(
 41 |     provider: str, providers: dict[str, ProviderConfig]
 42 | ) -> dict[str, str | HttpUrl]:
 43 |     """Retrieve configuration settings for the specified provider."""
 44 | 
 45 |     try:
 46 |         model_name = providers[provider].model_name
 47 |         base_url = providers[provider].base_url
 48 |     except KeyError as e:
 49 |         msg = get_key_error(str(e))
 50 |         logger.error(msg)
 51 |         raise KeyError(msg)
 52 |     except Exception as e:
 53 |         msg = generic_exception(str(e))
 54 |         logger.exception(msg)
 55 |         raise Exception(msg)
 56 |     else:
 57 |         return {
 58 |             "model_name": model_name,
 59 |             "base_url": base_url,
 60 |         }
 61 | 
 62 | 
 63 | def _create_model(endpoint_config: EndpointConfig) -> GeminiModel | OpenAIModel:
 64 |     """Create a model that uses model_name and base_url for inference API"""
 65 | 
 66 |     if endpoint_config.provider.lower() == "gemini":
 67 |         # FIXME EndpointConfig: TypeError: 'ModelRequest' object is not iterable.
 68 |         raise NotImplementedError(
 69 |             "Current typing raises TypeError: 'ModelRequest' object is not iterable."
 70 |         )
 71 |     elif endpoint_config.provider.lower() == "huggingface":
 72 |         # FIXME HF not working with pydantic-ai OpenAI model
 73 |         raise NotImplementedError(
 74 |             "Hugging Face provider is not implemented yet. Please use Gemini or OpenAI."
 75 |             " https://huggingface.co/docs/inference-providers/providers/hf-inference"
 76 |         )
 77 |         # headers = {
 78 |         #    "Authorization": f"Bearer {endpoint_config.api_key}",
 79 |         # }
 80 |         # def query(payload):
 81 |         #    response = requests.post(API_URL, headers=headers, json=payload)
 82 |         #    return response.json()
 83 |         # query({"inputs": "", "parameters": {},})
 84 |     else:
 85 |         base_url_str = str(endpoint_config.provider_config.base_url)
 86 |         return OpenAIModel(
 87 |             model_name=endpoint_config.provider_config.model_name,
 88 |             provider=OpenAIProvider(
 89 |                 base_url=base_url_str,
 90 |                 api_key=endpoint_config.api_key,
 91 |             ),
 92 |         )
 93 | 
 94 | 
 95 | def get_models(
 96 |     endpoint_config: EndpointConfig,
 97 |     include_researcher: bool = False,
 98 |     include_analyst: bool = False,
 99 |     include_synthesiser: bool = False,
100 | ) -> ModelDict:
101 |     """
102 |     Get the models for the system agents.
103 |     Args:
104 |         endpoint_config (EndpointConfig): Configuration for the model.
105 |         include_analyist (Optional[bool]): Whether to include the analyst model.
106 |             Defaults to False.
107 |         include_synthesiser (Optional[bool]): Whether to include the synthesiser model.
108 |             Defaults to False.
109 |     Returns:
110 |         Dict[str, GeminiModel | OpenAIModel]: A dictionary containing the models for the
111 |             system agents.
112 |     """
113 | 
114 |     model = _create_model(endpoint_config)
115 |     return ModelDict.model_validate(
116 |         {
117 |             "model_manager": model,
118 |             "model_researcher": model if include_researcher else None,
119 |             "model_analyst": model if include_analyst else None,
120 |             "model_synthesiser": model if include_synthesiser else None,
121 |         }
122 |     )
123 | 


--------------------------------------------------------------------------------
/src/app/config/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/src/app/config/__init__.py


--------------------------------------------------------------------------------
/src/app/config/config_app.py:
--------------------------------------------------------------------------------
 1 | """Configuration constants for the application."""
 2 | 
 3 | # MARK: chat env
 4 | API_SUFFIX = "_API_KEY"
 5 | CHAT_DEFAULT_PROVIDER = "github"
 6 | 
 7 | 
 8 | # MARK: project
 9 | PROJECT_NAME = "rd-mas-example"
10 | 
11 | 
12 | # MARK: paths
13 | CHAT_CONFIG_FILE = "config/config_chat.json"
14 | LOGS_PATH = "logs"
15 | EVAL_CONFIG_FILE = "config/config_eval.json"
16 | 


--------------------------------------------------------------------------------
/src/app/config/config_chat.json:
--------------------------------------------------------------------------------
 1 | {
 2 |     "providers": {
 3 |         "huggingface": {
 4 |             "model_name": "facebook/bart-large-mnli",
 5 |             "base_url": "https://router.huggingface.co/hf-inference/models"
 6 |         },
 7 |         "gemini": {
 8 |             "model_name": "gemini-1.5-flash-8b",
 9 |             "base_url": "https://generativelanguage.googleapis.com/v1beta"
10 |         },
11 |         "github": {
12 |             "model_name": "GPT-4o",
13 |             "base_url": "https://models.inference.ai.azure.com"
14 |         },
15 |         "grok": {
16 |             "model_name": "grok-2-1212",
17 |             "base_url": "https://api.x.ai/v1"
18 |         },
19 |         "ollama": {
20 |             "model_name": "granite3-dense",
21 |             "base_url": "http://localhost:11434/v1"
22 |         },
23 |         "openrouter": {
24 |             "model_name": "google/gemini-2.0-flash-exp:free",
25 |             "base_url": "https://openrouter.ai/api/v1"
26 |         },
27 |         "perplexity": {
28 |             "model_name": "sonar",
29 |             "base_url": "https://api.perplexity.ai"
30 |         },
31 |         "restack": {
32 |             "model_name": "deepseek-chat",
33 |             "base_url": "https://ai.restack.io"
34 |         },
35 |         "together": {
36 |             "model_name": "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
37 |             "base_url": "https://api.together.xyz/v1"
38 |         }
39 |     },
40 |     "inference": {
41 |         "usage_limits": 10000,
42 |         "usage_limits_ollama": 10000,
43 |         "result_retries": 3,
44 |         "result_retries_ollama": 3
45 |     },
46 |     "prompts": {
47 |         "system_prompt_manager": "You are a manager overseeing research and analysis tasks. Your role is to coordinate the efforts of the research, analysis and synthesiser agents to provide comprehensive answers to user queries. The researcher should gather and analyze data relevant to the topic. The whole result must be handed to the analyst, who will check it for accuracy of the assumptions, facts, and conclusions. If an analyst is present the researchers output has to be approved by the analyst. If the analyst does not approve of the researcher's result, all of the analyst's response and the topic must be handed back to the researcher to be refined. Repeat this loop until the analyst approves. If a sysnthesiser is present and once the analyst approves, the synthesiser should output a well formatted scientific report using the data given.",
48 |         "system_prompt_researcher": "You are a researcher. Gather and analyze data relevant to the topic. Use the search tool to gather data. Always check accuracy of assumptions, facts, and conclusions.",
49 |         "system_prompt_analyst": "You are a research analyst. Use your analytical skills to check the accuracy of assumptions, facts, and conclusions in the data provided. Provide relevant feedback if you do not approve. Only approve if you do not have any feedback to give.",
50 |         "system_prompt_synthesiser": "You are a scientific writing assistant. Your task is to output a well formatted scientific report using the data given. Leave the privided facts, conclusions and sources unchanged."
51 |     }
52 | }


--------------------------------------------------------------------------------
/src/app/config/config_eval.json:
--------------------------------------------------------------------------------
 1 | {
 2 |     "metrics_and_weights": {
 3 |         "time_taken": 0.167,
 4 |         "task_success": 0.167,
 5 |         "coordination_quality": 0.167,
 6 |         "tool_efficiency": 0.167,
 7 |         "planning_rational": 0.167,
 8 |         "output_similarity": 0.167
 9 |     }
10 | }


--------------------------------------------------------------------------------
/src/app/config/data_models.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Data models for agent system configuration and results.
  3 | 
  4 | This module defines Pydantic models for representing research and analysis results,
  5 | summaries, provider and agent configurations, and model dictionaries used throughout
  6 | the application. These models ensure type safety and validation for data exchanged
  7 | between agents and system components.
  8 | """
  9 | 
 10 | from typing import Any, TypeVar
 11 | 
 12 | from pydantic import BaseModel, ConfigDict, HttpUrl, field_validator
 13 | from pydantic_ai.messages import ModelRequest
 14 | from pydantic_ai.models import Model
 15 | from pydantic_ai.tools import Tool
 16 | from pydantic_ai.usage import UsageLimits
 17 | from pydantic_settings import BaseSettings, SettingsConfigDict
 18 | 
 19 | type UserPromptType = (
 20 |     str | list[dict[str, str]] | ModelRequest | None
 21 | )  #  (1) Input validation
 22 | ResultBaseType = TypeVar(
 23 |     "ResultBaseType", bound=BaseModel
 24 | )  # (2) Generic type for model results
 25 | 
 26 | 
 27 | class ResearchResult(BaseModel):
 28 |     """Research results from the research agent."""
 29 | 
 30 |     topic: str | dict[str, str]
 31 |     findings: list[str] | dict[str, str | list[str]]
 32 |     sources: list[str] | dict[str, str | list[str]]
 33 | 
 34 | 
 35 | class AnalysisResult(BaseModel):
 36 |     """Analysis results from the analysis agent."""
 37 | 
 38 |     insights: list[str]
 39 |     recommendations: list[str]
 40 |     approval: bool
 41 | 
 42 | 
 43 | class ResearchSummary(BaseModel):
 44 |     """Expected model response of research on a topic"""
 45 | 
 46 |     topic: str
 47 |     key_points: list[str]
 48 |     key_points_explanation: list[str]
 49 |     conclusion: str
 50 |     sources: list[str]
 51 | 
 52 | 
 53 | class ProviderConfig(BaseModel):
 54 |     """Configuration for a model provider"""
 55 | 
 56 |     model_name: str
 57 |     base_url: HttpUrl
 58 | 
 59 | 
 60 | class ChatConfig(BaseModel):
 61 |     """Configuration settings for agents and model providers"""
 62 | 
 63 |     providers: dict[str, ProviderConfig]
 64 |     inference: dict[str, str | int]
 65 |     prompts: dict[str, str]
 66 | 
 67 | 
 68 | class EndpointConfig(BaseModel):
 69 |     """Configuration for an agent"""
 70 | 
 71 |     provider: str
 72 |     query: UserPromptType = None
 73 |     api_key: str | None
 74 |     prompts: dict[str, str]
 75 |     provider_config: ProviderConfig
 76 |     usage_limits: UsageLimits | None = None
 77 | 
 78 | 
 79 | class AgentConfig(BaseModel):
 80 |     """Configuration for an agent"""
 81 | 
 82 |     model: Model  # (1) Instance expected
 83 |     output_type: type[BaseModel]  # (2) Class expected
 84 |     system_prompt: str
 85 |     # FIXME tools: list[Callable[..., Awaitable[Any]]]
 86 |     tools: list[Any] = []  # (3) List of tools will be validated at creation
 87 |     retries: int = 3
 88 | 
 89 |     # Avoid pydantic.errors.PydanticSchemaGenerationError:
 90 |     # Unable to generate pydantic-core schema for <class 'openai.AsyncOpenAI'>.
 91 |     # Avoid Pydantic errors related to non-Pydantic types
 92 |     model_config = ConfigDict(
 93 |         arbitrary_types_allowed=True
 94 |     )  # (4) Suppress Error non-Pydantic types caused by <class 'openai.AsyncOpenAI'>
 95 | 
 96 |     @field_validator("tools", mode="before")
 97 |     def validate_tools(cls, v: list[Any]) -> list[Tool | None]:
 98 |         """Validate that all tools are instances of Tool."""
 99 |         if not v:
100 |             return []
101 |         if not all(isinstance(t, Tool) for t in v):
102 |             raise ValueError("All tools must be Tool instances")
103 |         return v
104 | 
105 | 
106 | class ModelDict(BaseModel):
107 |     """Dictionary of models used to create agent systems"""
108 | 
109 |     model_manager: Model
110 |     model_researcher: Model | None
111 |     model_analyst: Model | None
112 |     model_synthesiser: Model | None
113 |     model_config = ConfigDict(arbitrary_types_allowed=True)
114 | 
115 | 
116 | class EvalConfig(BaseModel):
117 |     metrics_and_weights: dict[str, float]
118 | 
119 | 
120 | class AppEnv(BaseSettings):
121 |     """
122 |     Application environment settings loaded from environment variables or .env file.
123 | 
124 |     This class uses Pydantic's BaseSettings to manage API keys and configuration
125 |     for various inference endpoints, tools, and logging/monitoring services.
126 |     Environment variables are loaded from a .env file by default.
127 |     """
128 | 
129 |     # Inference endpoints
130 |     GEMINI_API_KEY: str = ""
131 |     GITHUB_API_KEY: str = ""
132 |     GROK_API_KEY: str = ""
133 |     HUGGINGFACE_API_KEY: str = ""
134 |     OPENROUTER_API_KEY: str = ""
135 |     PERPLEXITY_API_KEY: str = ""
136 |     RESTACK_API_KEY: str = ""
137 |     TOGETHER_API_KEY: str = ""
138 | 
139 |     # Tools
140 |     TAVILY_API_KEY: str = ""
141 | 
142 |     # Logging/Monitoring/Tracing
143 |     AGENTOPS_API_KEY: str = ""
144 |     LOGFIRE_API_KEY: str = ""
145 |     WANDB_API_KEY: str = ""
146 | 
147 |     model_config = SettingsConfigDict(
148 |         env_file=".env", env_file_encoding="utf-8", extra="ignore"
149 |     )
150 | 


--------------------------------------------------------------------------------
/src/app/evals/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/src/app/evals/__init__.py


--------------------------------------------------------------------------------
/src/app/evals/metrics.py:
--------------------------------------------------------------------------------
 1 | def time_taken(start_time: float, end_time: float) -> float:
 2 |     """Calculate duration between start and end timestamps
 3 | 
 4 |     Args:
 5 |         start_time: Timestamp when execution started
 6 |         end_time: Timestamp when execution completed
 7 | 
 8 |     Returns:
 9 |         Duration in seconds with microsecond precision
10 |     """
11 | 
12 |     # TODO implement
13 |     return end_time - start_time
14 | 
15 | 
16 | def output_similarity(agent_output: str, expected_answer: str) -> bool:
17 |     """
18 |     Determine to what degree the agent's output matches the expected answer.
19 | 
20 |     Args:
21 |         agent_output (str): The output produced by the agent.
22 |         expected_answer (str): The correct or expected answer.
23 | 
24 |     Returns:
25 |         bool: True if the output matches the expected answer, False otherwise.
26 |     """
27 | 
28 |     # TODO score instead of bool
29 |     return agent_output.strip() == expected_answer.strip()
30 | 


--------------------------------------------------------------------------------
/src/app/main.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Main entry point for the Agents-eval application.
  3 | 
  4 | This module initializes the agentic system, loads configuration files,
  5 | handles user input, and orchestrates the multi-agent workflow using
  6 | asynchronous execution. It integrates logging, tracing, and authentication,
  7 | and supports both CLI and programmatic execution.
  8 | """
  9 | 
 10 | from asyncio import run
 11 | from pathlib import Path
 12 | from sys import argv
 13 | 
 14 | from logfire import span
 15 | from weave import op
 16 | 
 17 | from app.__init__ import __version__
 18 | from app.agents.agent_system import get_manager, run_manager, setup_agent_env
 19 | from app.config.config_app import (
 20 |     CHAT_CONFIG_FILE,
 21 |     CHAT_DEFAULT_PROVIDER,
 22 |     EVAL_CONFIG_FILE,
 23 |     PROJECT_NAME,
 24 | )
 25 | from app.config.data_models import AppEnv, ChatConfig, EvalConfig
 26 | from app.utils.error_messages import generic_exception
 27 | from app.utils.load_configs import load_config
 28 | from app.utils.log import logger
 29 | from app.utils.login import login
 30 | from app.utils.utils import parse_args
 31 | 
 32 | 
 33 | @op()
 34 | async def main(
 35 |     chat_provider: str = CHAT_DEFAULT_PROVIDER,
 36 |     query: str = "",
 37 |     include_researcher: bool = False,
 38 |     include_analyst: bool = False,
 39 |     include_synthesiser: bool = False,
 40 |     pydantic_ai_stream: bool = False,
 41 |     chat_config_file: str = CHAT_CONFIG_FILE,
 42 | ) -> None:
 43 |     """
 44 |     Main entry point for the application.
 45 | 
 46 |     Args:
 47 |         chat_provider (str): The inference chat_provider to be used.
 48 |         query (str): The query to be processed by the agent.
 49 |         include_researcher (bool): Whether to include the researcher in the process.
 50 |         include_analyst (bool): Whether to include the analyst in the process.
 51 |         include_synthesiser (bool): Whether to include the synthesiser in the process.
 52 |         pydantic_ai_stream (bool): Whether to use Pydantic AI streaming.
 53 |         chat_config_file (str): Full path to the configuration file.
 54 | 
 55 |     Returns:
 56 |         None
 57 |     """
 58 | 
 59 |     logger.info(f"Starting app '{PROJECT_NAME}' v{__version__}")
 60 |     try:
 61 |         with span("main()"):
 62 |             if not chat_provider:
 63 |                 chat_provider = input("Which inference chat_provider to use? ")
 64 |             if not query:
 65 |                 query = input("What would you like to research? ")
 66 | 
 67 |             chat_config_path = Path(__file__).parent / CHAT_CONFIG_FILE
 68 |             eval_config_path = Path(__file__).parent / EVAL_CONFIG_FILE
 69 |             chat_config = load_config(chat_config_path, ChatConfig)
 70 |             eval_config = load_config(eval_config_path, EvalConfig)
 71 |             chat_env_config = AppEnv()
 72 |             agent_env = setup_agent_env(
 73 |                 chat_provider, query, chat_config, chat_env_config
 74 |             )
 75 |             # TODO remove noqa and type ignore for unused variable
 76 |             metrics_and_weights = eval_config.metrics_and_weights  # noqa: F841  # type: ignore[reportUnusedVariable]
 77 | 
 78 |             # FIXME enhance login, not every run?
 79 |             login(PROJECT_NAME, chat_env_config)
 80 | 
 81 |             manager = get_manager(
 82 |                 agent_env.provider,
 83 |                 agent_env.provider_config,
 84 |                 agent_env.api_key,
 85 |                 agent_env.prompts,
 86 |                 include_researcher,
 87 |                 include_analyst,
 88 |                 include_synthesiser,
 89 |             )
 90 |             await run_manager(
 91 |                 manager,
 92 |                 agent_env.query,
 93 |                 agent_env.provider,
 94 |                 agent_env.usage_limits,
 95 |                 pydantic_ai_stream,
 96 |             )
 97 |             logger.info(f"Exiting app '{PROJECT_NAME}'")
 98 | 
 99 |     except Exception as e:
100 |         msg = generic_exception(f"Aborting app '{PROJECT_NAME}' with: {e}")
101 |         logger.exception(msg)
102 |         raise Exception(msg) from e
103 | 
104 | 
105 | if __name__ == "__main__":
106 |     args = parse_args(argv[1:])
107 |     run(main(**args))
108 | 


--------------------------------------------------------------------------------
/src/app/py.typed:
--------------------------------------------------------------------------------
1 | # PEP 561 – Distributing and Packaging Type Information
2 | # https://peps.python.org/pep-0561/


--------------------------------------------------------------------------------
/src/app/utils/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/qte77/Agents-eval/0d031ae2708eba24c87e0364d932ea9fc9b0a78e/src/app/utils/__init__.py


--------------------------------------------------------------------------------
/src/app/utils/error_messages.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Error message utilities for the Agents-eval application.
 3 | 
 4 | This module provides concise helper functions for generating standardized
 5 | error messages related to configuration loading and validation.
 6 | """
 7 | 
 8 | from pathlib import Path
 9 | 
10 | 
11 | def api_connection_error(error: str) -> str:
12 |     """
13 |     Generate a error message for API connection error.
14 |     """
15 |     return f"API connection error: {error}"
16 | 
17 | 
18 | def failed_to_load_config(error: str) -> str:
19 |     """
20 |     Generate a error message for configuration loading failure.
21 |     """
22 |     return f"Failed to load config: {error}"
23 | 
24 | 
25 | def file_not_found(file_path: str | Path) -> str:
26 |     """
27 |     Generate an error message for a missing configuration file.
28 |     """
29 |     return f"File not found: {file_path}"
30 | 
31 | 
32 | def generic_exception(error: str) -> str:
33 |     """
34 |     Generate a generic error message.
35 |     """
36 |     return f"Exception: {error}"
37 | 
38 | 
39 | def invalid_data_model_format(error: str) -> str:
40 |     """
41 |     Generate an error message for invalid pydantic data model format.
42 |     """
43 |     return f"Invalid pydantic data model format: {error}"
44 | 
45 | 
46 | def invalid_json(error: str) -> str:
47 |     """
48 |     Generate an error message for invalid JSON in a configuration file.
49 |     """
50 |     return f"Invalid JSON: {error}"
51 | 
52 | 
53 | def invalid_type(expected_type: str, actual_type: str) -> str:
54 |     """
55 |     Generate an error message for invalid Type.
56 |     """
57 |     return f"Type Error: Expected {expected_type}, got {actual_type} instead."
58 | 
59 | 
60 | def get_key_error(error: str) -> str:
61 |     """
62 |     Generate a generic error message.
63 |     """
64 |     return f"Key Error: {error}"
65 | 


--------------------------------------------------------------------------------
/src/app/utils/load_configs.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Configuration loading utilities.
 3 | 
 4 | Provides a generic function for loading and validating JSON configuration
 5 | files against Pydantic models, with error handling and logging support.
 6 | """
 7 | 
 8 | import json
 9 | from pathlib import Path
10 | 
11 | from pydantic import BaseModel, ValidationError
12 | 
13 | from app.utils.error_messages import (
14 |     failed_to_load_config,
15 |     file_not_found,
16 |     invalid_data_model_format,
17 |     invalid_json,
18 | )
19 | from app.utils.log import logger
20 | 
21 | 
22 | def load_config(config_path: str | Path, data_model: type[BaseModel]) -> BaseModel:
23 |     """
24 |     Generic configuration loader that validates against any Pydantic model.
25 | 
26 |     Args:
27 |         config_path: Path to the JSON configuration file
28 |         model: Pydantic model class for validation
29 | 
30 |     Returns:
31 |         Validated configuration instance
32 |     """
33 | 
34 |     try:
35 |         with open(config_path, encoding="utf-8") as f:
36 |             data = json.load(f)
37 |         return data_model.model_validate(data)
38 |     except FileNotFoundError as e:
39 |         msg = file_not_found(config_path)
40 |         logger.error(msg)
41 |         raise FileNotFoundError(msg) from e
42 |     except json.JSONDecodeError as e:
43 |         msg = invalid_json(str(e))
44 |         logger.error(msg)
45 |         raise ValueError(msg) from e
46 |     except ValidationError as e:
47 |         msg = invalid_data_model_format(str(e))
48 |         logger.error(msg)
49 |         raise ValidationError(msg) from e
50 |     except Exception as e:
51 |         msg = failed_to_load_config(str(e))
52 |         logger.exception(msg)
53 |         raise Exception(msg) from e
54 | 


--------------------------------------------------------------------------------
/src/app/utils/load_settings.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Utility functions and classes for loading application settings and configuration.
 3 | 
 4 | This module defines the AppEnv class for managing environment variables using Pydantic,
 5 | and provides a function to load and validate application configuration from a JSON file.
 6 | """
 7 | 
 8 | import json
 9 | from pathlib import Path
10 | 
11 | from pydantic_settings import BaseSettings, SettingsConfigDict
12 | 
13 | from app.config.data_models import ChatConfig
14 | from app.utils.error_messages import (
15 |     failed_to_load_config,
16 |     file_not_found,
17 |     invalid_json,
18 | )
19 | from app.utils.log import logger
20 | 
21 | 
22 | class AppEnv(BaseSettings):
23 |     """
24 |     Application environment settings loaded from environment variables or .env file.
25 | 
26 |     This class uses Pydantic's BaseSettings to manage API keys and configuration
27 |     for various inference endpoints, tools, and logging/monitoring services.
28 |     Environment variables are loaded from a .env file by default.
29 |     """
30 | 
31 |     # Inference endpoints
32 |     GEMINI_API_KEY: str = ""
33 |     GITHUB_API_KEY: str = ""
34 |     GROK_API_KEY: str = ""
35 |     HUGGINGFACE_API_KEY: str = ""
36 |     OPENROUTER_API_KEY: str = ""
37 |     PERPLEXITY_API_KEY: str = ""
38 |     RESTACK_API_KEY: str = ""
39 |     TOGETHER_API_KEY: str = ""
40 | 
41 |     # Tools
42 |     TAVILY_API_KEY: str = ""
43 | 
44 |     # Logging/Monitoring/Tracing
45 |     AGENTOPS_API_KEY: str = ""
46 |     LOGFIRE_TOKEN: str = ""
47 |     WANDB_API_KEY: str = ""
48 | 
49 |     model_config = SettingsConfigDict(
50 |         env_file=".env", env_file_encoding="utf-8", extra="ignore"
51 |     )
52 | 
53 | 
54 | chat_config = AppEnv()
55 | 
56 | 
57 | def load_config(config_path: str | Path) -> ChatConfig:
58 |     """
59 |     Load and validate application configuration from a JSON file.
60 | 
61 |     Args:
62 |         config_path (str): Path to the JSON configuration file.
63 | 
64 |     Returns:
65 |         ChatConfig: An instance of ChatConfig with validated configuration data.
66 | 
67 |     Raises:
68 |         FileNotFoundError: If the configuration file does not exist.
69 |         json.JSONDecodeError: If the file contains invalid JSON.
70 |         Exception: For any other unexpected errors during loading or validation.
71 |     """
72 | 
73 |     try:
74 |         with open(config_path) as f:
75 |             config_data = json.load(f)
76 |     except FileNotFoundError as e:
77 |         msg = file_not_found(config_path)
78 |         logger.error(msg)
79 |         raise FileNotFoundError(msg) from e
80 |     except json.JSONDecodeError as e:
81 |         msg = invalid_json(str(e))
82 |         logger.error(msg)
83 |         raise json.JSONDecodeError(msg, str(config_path), 0) from e
84 |     except Exception as e:
85 |         msg = failed_to_load_config(str(e))
86 |         logger.exception(msg)
87 |         raise Exception(msg) from e
88 | 
89 |     return ChatConfig.model_validate(config_data)
90 | 


--------------------------------------------------------------------------------
/src/app/utils/log.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Set up the logger with custom settings.
 3 | Logs are written to a file with automatic rotation.
 4 | """
 5 | 
 6 | from loguru import logger
 7 | 
 8 | from app.config.config_app import LOGS_PATH
 9 | 
10 | logger.add(
11 |     f"{LOGS_PATH}/{{time}}.log",
12 |     rotation="1 MB",
13 |     # level="DEBUG",
14 |     retention="7 days",
15 |     compression="zip",
16 | )
17 | 


--------------------------------------------------------------------------------
/src/app/utils/login.py:
--------------------------------------------------------------------------------
 1 | """
 2 | This module provides utility functions for managing login state and initializing
 3 | the environment for a given project. It includes functionality to load and save
 4 | login state, perform a one-time login, and check if the user is logged in.
 5 | """
 6 | 
 7 | from os import environ
 8 | 
 9 | from agentops import init as agentops_init
10 | from logfire import configure as logfire_conf
11 | from wandb import login as wandb_login
12 | from weave import init as weave_init
13 | 
14 | from app.agents.llm_model_funs import get_api_key
15 | from app.config.data_models import AppEnv
16 | from app.utils.error_messages import generic_exception
17 | from app.utils.log import logger
18 | 
19 | 
20 | def login(project_name: str, chat_env_config: AppEnv):
21 |     """
22 |     Logs in to the workspace and initializes the environment for the given project.
23 |     Args:
24 |         project_name (str): The name of the project to initialize.
25 |         chat_env_config (AppEnv): The application environment configuration
26 |             containing the API keys.
27 |     Returns:
28 |         None
29 |     """
30 | 
31 |     try:
32 |         logger.info(f"Logging in to the workspaces for project: {project_name}")
33 |         environ["AGENTOPS_LOGGING_TO_FILE"] = "FALSE"
34 |         agentops_init(
35 |             default_tags=[project_name],
36 |             api_key=get_api_key("AGENTOPS", chat_env_config),
37 |         )
38 |         logfire_conf(token=get_api_key("LOGFIRE", chat_env_config))
39 |         wandb_login(key=get_api_key("WANDB", chat_env_config))
40 |         weave_init(project_name)
41 |     except Exception as e:
42 |         msg = generic_exception(str(e))
43 |         logger.exception(e)
44 |         raise Exception(msg) from e
45 | 


--------------------------------------------------------------------------------
/src/app/utils/utils.py:
--------------------------------------------------------------------------------
  1 | """
  2 | This module provides utility functions and context managers for handling configurations,
  3 | error handling, and setting up agent environments.
  4 | 
  5 | Functions:
  6 |     load_config(config_path: str) -> Config:
  7 |         Load and validate configuration from a JSON file.
  8 | 
  9 |     print_research_Result(summary: Dict, usage: Usage) -> None:
 10 |         Output structured summary of the research topic.
 11 | 
 12 |     error_handling_context(operation_name: str, console: Console = None):
 13 |         Context manager for handling errors during operations.
 14 | 
 15 |     setup_agent_env(config: Config, console: Console = None) -> AgentConfig:
 16 |         Set up the agent environment based on the provided configuration.
 17 | """
 18 | 
 19 | from pydantic_ai.usage import Usage
 20 | 
 21 | from app.config.data_models import ResearchSummary
 22 | from app.utils.log import logger
 23 | 
 24 | 
 25 | def log_research_result(summary: ResearchSummary, usage: Usage) -> None:
 26 |     """
 27 |     Prints the research summary and usage details in a formatted manner.
 28 | 
 29 |     Args:
 30 |         summary (Dict): A dictionary containing the research summary with keys 'topic',
 31 |             'key_points', 'key_points_explanation', and 'conclusion'.
 32 |         usage (Usage): An object containing usage details to be printed.
 33 |     """
 34 | 
 35 |     logger.info(f"\n=== Research Summary: {summary.topic} ===")
 36 |     logger.info("\nKey Points:")
 37 |     for i, point in enumerate(summary.key_points, 1):
 38 |         logger.info(f"{i}. {point}")
 39 |     logger.info("\nKey Points Explanation:")
 40 |     for i, point in enumerate(summary.key_points_explanation, 1):
 41 |         logger.info(f"{i}. {point}")
 42 |     logger.info(f"\nConclusion: {summary.conclusion}")
 43 |     logger.info(f"\nResponse structure: {list(dict(summary).keys())}")
 44 |     logger.info(usage)
 45 | 
 46 | 
 47 | def parse_args(argv: list[str]) -> dict[str, str | bool]:
 48 |     """
 49 |     Parse command line arguments into a dictionary.
 50 | 
 51 |     This function processes a list of command-line arguments,
 52 |     extracting recognized options and their values.
 53 |     Supported arguments include flags (e.g., --help, --include-researcher
 54 |     and key-value pairs (e.g., `--chat-provider=ollama`).
 55 |     If the `--help` flag is present, a list of available commands and their
 56 |     descriptions is printed, and an empty dictionary is returned.
 57 | 
 58 |     Recognized arguments as list[str]
 59 |     ```
 60 |         --help                   Display help information and exit.
 61 |         --version                Display version information.
 62 |         --chat-provider=<str>    Specify the chat provider to use.
 63 |         --query=<str>            Specify the query to process.
 64 |         --include-researcher     Include the researcher agent.
 65 |         --include-analyst        Include the analyst agent.
 66 |         --include-synthesiser    Include the synthesiser agent.
 67 |         --no-stream              Disable streaming output.
 68 |         --chat-config-file=<str> Specify the path to the chat configuration file.
 69 |     ```
 70 | 
 71 |     Returns:
 72 |         `dict[str, str | bool]`: A dictionary mapping argument names
 73 |         (with leading '--' removed and hyphens replaced by underscores)
 74 |         to their values (`str` for key-value pairs, `bool` for flags).
 75 |         Returns an empty dict if `--help` is specified.
 76 | 
 77 |     Example:
 78 |         >>> `parse_args(['--chat-provider=ollama', '--include-researcher'])`
 79 |         returns `{'chat_provider': 'ollama', 'include_researcher': True}`
 80 |     """
 81 | 
 82 |     commands = {
 83 |         "--help": "Display help information",
 84 |         "--version": "Display version information",
 85 |         "--chat-provider": "Specify the chat provider to use",
 86 |         "--query": "Specify the query to process",
 87 |         "--include-researcher": "Include the researcher agent",
 88 |         "--include-analyst": "Include the analyst agent",
 89 |         "--include-synthesiser": "Include the synthesiser agent",
 90 |         "--no-stream": "Disable streaming output",
 91 |         "--chat-config-file": "Specify the path to the chat configuration file",
 92 |     }
 93 |     parsed_args: dict[str, str | bool] = {}
 94 | 
 95 |     if "--help" in argv:
 96 |         print("Available commands:")
 97 |         for cmd, desc in commands.items():
 98 |             print(f"{cmd}: {desc}")
 99 |         return parsed_args
100 | 
101 |     for arg in argv:
102 |         if arg.split("=", 1)[0] in commands.keys():
103 |             key, value = arg.split("=", 1) if "=" in arg else (arg, True)
104 |             key = key.lstrip("--").replace("-", "_")
105 |             parsed_args[key] = value
106 | 
107 |     if parsed_args:
108 |         logger.info(f"Used arguments: {parsed_args}")
109 | 
110 |     return parsed_args
111 | 


--------------------------------------------------------------------------------
/src/examples/config.json:
--------------------------------------------------------------------------------
 1 | {
 2 |     "providers": {
 3 |         "gemini": {
 4 |             "model_name": "gemini-1.5-flash-8b",
 5 |             "base_url": "https://generativelanguage.googleapis.com/v1beta"
 6 |         },
 7 |         "github": {
 8 |             "model_name": "GPT-4o",
 9 |             "base_url": "https://models.inference.ai.azure.com"
10 |         },
11 |         "huggingface": {
12 |             "model_name": "Qwen/QwQ-32B-Preview",
13 |             "base_url": "https://api-inference.huggingface.co/v1"
14 |         },
15 |         "ollama": {
16 |             "model_name": "granite3-dense",
17 |             "base_url": "http://localhost:11434/v1"
18 |         },
19 |         "openrouter": {
20 |             "model_name": "google/gemini-2.0-flash-lite-preview-02-05:free",
21 |             "base_url": "https://openrouter.ai/api/v1"
22 |         },
23 |         "restack": {
24 |             "model_name": "deepseek-chat",
25 |             "base_url": "https://ai.restack.io"
26 |         }
27 |     },
28 |     "prompts": {
29 |         "system_prompt": "You are a helpful research assistant. Extract key information about the topic and provide a structured summary.",
30 |         "user_prompt": "Provide a research summary about",
31 |         "system_prompt_researcher": "You are a manager overseeing research and analysis tasks. Your role is to coordinate the efforts of the research and analysis agents to provide comprehensive answers to user queries.",
32 |         "system_prompt_manager": "You are a research assistant. Your task is to find relevant information about the topic provided. Use the search tool to gather data and synthesize it into a concise summary.",
33 |         "system_prompt_analyst": "You are a data scientist. Your task is to analyze the data provided and extract meaningful insights. Use your analytical skills to identify trends, patterns, and correlations."
34 |     }
35 | }


--------------------------------------------------------------------------------
/src/examples/run_simple_agent_no_tools.py:
--------------------------------------------------------------------------------
 1 | """
 2 | A simple example of using a Pydantic AI agent to generate a structured summary of a
 3 | research topic.
 4 | """
 5 | 
 6 | from os import path
 7 | 
 8 | from .utils.agent_simple_no_tools import get_research
 9 | from .utils.utils import (
10 |     get_api_key,
11 |     get_provider_config,
12 |     load_config,
13 |     print_research_Result,
14 | )
15 | 
16 | CONFIG_FILE = "config.json"
17 | 
18 | 
19 | def main():
20 |     """Main function to run the research agent."""
21 | 
22 |     config_path = path.join(path.dirname(__file__), CONFIG_FILE)
23 |     config = load_config(config_path)
24 | 
25 |     provider = input("Which inference provider to use? ")
26 |     topic = input("What topic would you like to research? ")
27 | 
28 |     api_key = get_api_key(provider)
29 |     provider_config = get_provider_config(provider, config)
30 | 
31 |     result = get_research(topic, config.prompts, provider, provider_config, api_key)
32 |     print_research_Result(result.data, result.usage())
33 | 
34 | 
35 | if __name__ == "__main__":
36 |     main()
37 | 


--------------------------------------------------------------------------------
/src/examples/run_simple_agent_system.py:
--------------------------------------------------------------------------------
 1 | """
 2 | This example demonstrates how to run a simple agent system that consists of a manager
 3 | agent, a research agent, and an analysis agent. The manager agent delegates research
 4 | and analysis tasks to the corresponding agents and combines the results to provide a
 5 | comprehensive answer to the user query.
 6 | https://ai.pydantic.dev/multi-agent-applications/#agent-delegation
 7 | """
 8 | 
 9 | from asyncio import run
10 | from os import path
11 | 
12 | from openai import UnprocessableEntityError
13 | from pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool
14 | from pydantic_ai.exceptions import UnexpectedModelBehavior, UsageLimitExceeded
15 | from pydantic_ai.models.openai import OpenAIModel
16 | from pydantic_ai.usage import UsageLimits
17 | 
18 | from .utils.agent_simple_system import SystemAgent, add_tools_to_manager_agent
19 | from .utils.data_models import AnalysisResult, ResearchResult
20 | from .utils.utils import create_model, get_api_key, get_provider_config, load_config
21 | 
22 | CONFIG_FILE = "config.json"
23 | 
24 | 
25 | def get_models(model_config: dict) -> tuple[OpenAIModel]:
26 |     """Get the models for the system agents."""
27 |     model_researcher = create_model(**model_config)
28 |     model_analyst = create_model(**model_config)
29 |     model_manager = create_model(**model_config)
30 |     return model_researcher, model_analyst, model_manager
31 | 
32 | 
33 | def get_manager(
34 |     model_manager: OpenAIModel,
35 |     model_researcher: OpenAIModel,
36 |     model_analyst: OpenAIModel,
37 |     prompts: dict[str, str],
38 | ) -> SystemAgent:
39 |     """Get the agents for the system."""
40 |     researcher = SystemAgent(
41 |         model_researcher,
42 |         ResearchResult,
43 |         prompts["system_prompt_researcher"],
44 |         [duckduckgo_search_tool()],
45 |     )
46 |     analyst = SystemAgent(
47 |         model_analyst, AnalysisResult, prompts["system_prompt_analyst"]
48 |     )
49 |     manager = SystemAgent(
50 |         model_manager, ResearchResult, prompts["system_prompt_manager"]
51 |     )
52 |     add_tools_to_manager_agent(manager, researcher, analyst)
53 |     return manager
54 | 
55 | 
56 | async def main():
57 |     """Main function to run the research system."""
58 | 
59 |     provider = input("Which inference provider to use? ")
60 |     query = input("What would you like to research? ")
61 | 
62 |     config_path = path.join(path.dirname(__file__), CONFIG_FILE)
63 |     config = load_config(config_path)
64 | 
65 |     api_key = get_api_key(provider)
66 |     provider_config = get_provider_config(provider, config)
67 |     usage_limits = UsageLimits(request_limit=10, total_tokens_limit=4000)
68 | 
69 |     model_config = {
70 |         "base_url": provider_config["base_url"],
71 |         "model_name": provider_config["model_name"],
72 |         "api_key": api_key,
73 |         "provider": provider,
74 |     }
75 |     manager = get_manager(*get_models(model_config), config.prompts)
76 | 
77 |     print(f"\nResearching: {query}...")
78 | 
79 |     try:
80 |         result = await manager.run(query, usage_limits=usage_limits)
81 |     except (UnexpectedModelBehavior, UnprocessableEntityError) as e:
82 |         print(f"Error: Model returned unexpected result: {e}")
83 |     except UsageLimitExceeded as e:
84 |         print(f"Usage limit exceeded: {e}")
85 |     else:
86 |         print("\nFindings:", {result.data.findings})
87 |         print(f"Sources: {result.data.sources}")
88 |         print("\nUsage statistics:")
89 |         print(result.usage())
90 | 
91 | 
92 | if __name__ == "__main__":
93 |     run(main())
94 | 


--------------------------------------------------------------------------------
/src/examples/run_simple_agent_tools.py:
--------------------------------------------------------------------------------
 1 | """Run the dice game agent using simple tools."""
 2 | 
 3 | from os import path
 4 | 
 5 | from .utils.agent_simple_tools import get_dice
 6 | from .utils.utils import (
 7 |     get_api_key,
 8 |     get_provider_config,
 9 |     load_config,
10 | )
11 | 
12 | CONFIG_FILE = "config.json"
13 | system_prompt = (
14 |     "You're a dice game, you should roll the die and see if the number "
15 |     "you get back matches the user's guess. If so, tell them they're a winner. "
16 |     "Use the player's name in the response."
17 | )
18 | 
19 | 
20 | def main():
21 |     """Run the dice game agent."""
22 | 
23 |     provider = input("Which inference provider to use? ")
24 |     player_name = input("Enter your name: ")
25 |     guess = input("Guess a number between 1 and 6: ")
26 | 
27 |     config_path = path.join(path.dirname(__file__), CONFIG_FILE)
28 |     config = load_config(config_path)
29 | 
30 |     api_key = get_api_key(provider)
31 |     provider_config = get_provider_config(provider, config)
32 | 
33 |     result = get_dice(
34 |         player_name, guess, system_prompt, provider, api_key, provider_config
35 |     )
36 |     print(result.data)
37 |     print(f"{result._result_tool_name=}")
38 |     print(result.usage())
39 | 
40 | 
41 | if __name__ == "__main__":
42 |     main()
43 | 


--------------------------------------------------------------------------------
/src/examples/utils/agent_simple_no_tools.py:
--------------------------------------------------------------------------------
 1 | """
 2 | This module contains a function to create a research agent with the specified model,
 3 | result type, and system prompt.
 4 | """
 5 | 
 6 | from sys import exit
 7 | 
 8 | from openai import APIConnectionError
 9 | from pydantic_ai import Agent
10 | from pydantic_ai.agent import AgentRunResult
11 | from pydantic_ai.models.openai import OpenAIModel
12 | 
13 | from .data_models import Config, ResearchSummary
14 | from .utils import create_model
15 | 
16 | 
17 | def _create_research_agent(
18 |     model: OpenAIModel, result_type: ResearchSummary, system_prompt: str
19 | ) -> Agent:
20 |     """
21 |     Create a research agent with the specified model, result type, and system prompt.
22 |     """
23 | 
24 |     return Agent(model=model, result_type=result_type, system_prompt=system_prompt)
25 | 
26 | 
27 | def get_research(
28 |     topic: str,
29 |     prompts: dict[str, str],
30 |     provider: str,
31 |     provider_config: Config,
32 |     api_key: str,
33 | ) -> AgentRunResult:
34 |     """Run the research agent to generate a structured summary of a research topic."""
35 | 
36 |     model = create_model(
37 |         provider_config["base_url"], provider_config["model_name"], api_key, provider
38 |     )
39 |     agent = _create_research_agent(model, ResearchSummary, prompts["system_prompt"])
40 | 
41 |     print(f"\nResearching {topic}...")
42 |     try:
43 |         result = agent.run_sync(f"{prompts['user_prompt']} {topic}")
44 |     except APIConnectionError as e:
45 |         print(f"Error connecting to API: {e}")
46 |         exit()
47 |     except Exception as e:
48 |         print(f"Error connecting to API: {e}")
49 |         exit()
50 |     else:
51 |         return result
52 | 


--------------------------------------------------------------------------------
/src/examples/utils/agent_simple_system.py:
--------------------------------------------------------------------------------
 1 | """
 2 | This module contains a simple system of agents that can be used to research and analyze
 3 | data.
 4 | """
 5 | 
 6 | from pydantic_ai import Agent, RunContext
 7 | from pydantic_ai.models.openai import OpenAIModel
 8 | 
 9 | from .data_models import AnalysisResult, ResearchResult
10 | 
11 | 
12 | class SystemAgent(Agent):
13 |     """A generic system agent that can be used to research and analyze data."""
14 | 
15 |     def __init__(
16 |         self,
17 |         model: OpenAIModel,
18 |         result_type: ResearchResult | AnalysisResult,
19 |         system_prompt: str,
20 |         result_retries: int = 3,
21 |         tools: list | None = [],
22 |     ):
23 |         super().__init__(
24 |             model,
25 |             result_type=result_type,
26 |             system_prompt=system_prompt,
27 |             result_retries=result_retries,
28 |             tools=tools,
29 |         )
30 | 
31 | 
32 | def add_tools_to_manager_agent(
33 |     manager_agent: SystemAgent, research_agent: SystemAgent, analysis_agent: SystemAgent
34 | ) -> None:
35 |     """Create and configure the joke generation agent."""
36 | 
37 |     @manager_agent.tool
38 |     async def delegate_research(ctx: RunContext[None], query: str) -> ResearchResult:
39 |         """Delegate research task to ResearchAgent."""
40 |         result = await research_agent.run(query, usage=ctx.usage)
41 |         return result.data
42 | 
43 |     @manager_agent.tool
44 |     async def delegate_analysis(ctx: RunContext[None], data: str) -> AnalysisResult:
45 |         """Delegate analysis task to AnalysisAgent."""
46 |         result = await analysis_agent.run(data, usage=ctx.usage)
47 |         return result.data
48 | 


--------------------------------------------------------------------------------
/src/examples/utils/agent_simple_tools.py:
--------------------------------------------------------------------------------
 1 | """Simple agent for the dice game example."""
 2 | 
 3 | from openai import APIConnectionError
 4 | from pydantic_ai import Agent, Tool
 5 | from pydantic_ai.agent import AgentRunResult
 6 | from pydantic_ai.models.openai import OpenAIModel
 7 | 
 8 | from .tools import get_player_name, roll_die
 9 | from .utils import create_model
10 | 
11 | 
12 | class _DiceGameAgent(Agent):
13 |     """Dice game agent."""
14 | 
15 |     def __init__(self, model: OpenAIModel, system_prompt: str):
16 |         super().__init__(
17 |             model=model,
18 |             deps_type=str,
19 |             system_prompt=system_prompt,
20 |             tools=[  # (1)!
21 |                 Tool(roll_die, takes_ctx=False),
22 |                 Tool(get_player_name, takes_ctx=True),
23 |             ],
24 |         )
25 | 
26 | 
27 | def get_dice(
28 |     player_name: str,
29 |     guess: str,
30 |     system_prompt: str,
31 |     provider: str,
32 |     api_key: str,
33 |     config: dict,
34 | ) -> AgentRunResult:
35 |     """Run the dice game agent."""
36 | 
37 |     model = create_model(config["base_url"], config["model_name"], api_key, provider)
38 |     agent = _DiceGameAgent(model, system_prompt)
39 | 
40 |     try:
41 |         # usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),
42 |         result = agent.run_sync(f"Player is guessing {guess}...", deps=player_name)
43 |     except APIConnectionError as e:
44 |         print(f"Error connecting to API: {e}")
45 |         exit()
46 |     except Exception as e:
47 |         print(f"Error connecting to API: {e}")
48 |         exit()
49 |     else:
50 |         return result
51 | 


--------------------------------------------------------------------------------
/src/examples/utils/data_models.py:
--------------------------------------------------------------------------------
 1 | """Example of a module with data models"""
 2 | 
 3 | from pydantic import BaseModel
 4 | 
 5 | 
 6 | class ResearchResult(BaseModel):
 7 |     """Research results from the research agent."""
 8 | 
 9 |     topic: str
10 |     findings: list[str]
11 |     sources: list[str]
12 | 
13 | 
14 | class AnalysisResult(BaseModel):
15 |     """Analysis results from the analysis agent."""
16 | 
17 |     insights: list[str]
18 |     recommendations: list[str]
19 | 
20 | 
21 | class ResearchSummary(BaseModel):
22 |     """Expected model response of research on a topic"""
23 | 
24 |     topic: str
25 |     key_points: list[str]
26 |     key_points_explanation: list[str]
27 |     conclusion: str
28 | 
29 | 
30 | class ProviderConfig(BaseModel):
31 |     """Configuration for a model provider"""
32 | 
33 |     model_name: str
34 |     base_url: str
35 | 
36 | 
37 | class Config(BaseModel):
38 |     """Configuration settings for the research agent and model providers"""
39 | 
40 |     providers: dict[str, ProviderConfig]
41 |     prompts: dict[str, str]
42 | 


--------------------------------------------------------------------------------
/src/examples/utils/tools.py:
--------------------------------------------------------------------------------
 1 | """Example tools for the utils example."""
 2 | 
 3 | from random import randint
 4 | 
 5 | from pydantic_ai import RunContext
 6 | 
 7 | 
 8 | def roll_die() -> str:
 9 |     """Tool to roll a die."""
10 | 
11 |     async def _execute(self) -> str:
12 |         """Roll the die and return the result."""
13 |         return str(randint(1, 6))
14 | 
15 | 
16 | def get_player_name(ctx: RunContext[str]) -> str:
17 |     """Get the player's name from the context."""
18 |     return ctx.deps
19 | 


--------------------------------------------------------------------------------
/src/examples/utils/utils.py:
--------------------------------------------------------------------------------
  1 | """Utility functions for running the research agent example."""
  2 | 
  3 | from json import load
  4 | from os import getenv
  5 | from sys import exit
  6 | 
  7 | from dotenv import load_dotenv
  8 | from pydantic import ValidationError
  9 | from pydantic_ai.models.openai import OpenAIModel
 10 | from pydantic_ai.providers.openai import OpenAIProvider
 11 | from pydantic_ai.usage import Usage
 12 | 
 13 | from .data_models import Config
 14 | 
 15 | API_SUFFIX = "_API_KEY"
 16 | 
 17 | 
 18 | def load_config(config_path: str) -> Config:
 19 |     """Load and validate configuration from a JSON file."""
 20 | 
 21 |     try:
 22 |         with open(config_path) as file:
 23 |             config_data = load(file)
 24 |         config = Config.model_validate(config_data)
 25 |     except FileNotFoundError:
 26 |         raise FileNotFoundError(f"Configuration file not found: {config_path}")
 27 |         exit()
 28 |     except ValidationError as e:
 29 |         raise ValueError(f"Invalid configuration format: {e}")
 30 |         exit()
 31 |     except Exception as e:
 32 |         raise Exception(f"Error loading configuration: {e}")
 33 |         exit()
 34 |     else:
 35 |         return config
 36 | 
 37 | 
 38 | def get_api_key(provider: str) -> str | None:
 39 |     """Retrieve API key from environment variable."""
 40 | 
 41 |     # TODO replace with pydantic-settings ?
 42 |     load_dotenv()
 43 | 
 44 |     if provider.lower() == "ollama":
 45 |         return None
 46 |     else:
 47 |         return getenv(f"{provider.upper()}{API_SUFFIX}")
 48 | 
 49 | 
 50 | def get_provider_config(provider: str, config: Config) -> dict[str, str]:
 51 |     """Retrieve configuration settings for the specified provider."""
 52 | 
 53 |     try:
 54 |         model_name = config.providers[provider].model_name
 55 |         base_url = config.providers[provider].base_url
 56 |     except KeyError as e:
 57 |         raise ValueError(f"Missing configuration for {provider}: {e}.")
 58 |         exit()
 59 |     except Exception as e:
 60 |         raise Exception(f"Error loading provider configuration: {e}")
 61 |         exit()
 62 |     else:
 63 |         return {
 64 |             "model_name": model_name,
 65 |             "base_url": base_url,
 66 |         }
 67 | 
 68 | 
 69 | def create_model(
 70 |     base_url: str,
 71 |     model_name: str,
 72 |     api_key: str | None = None,
 73 |     provider: str | None = None,
 74 | ) -> OpenAIModel:
 75 |     """Create a model that uses base_url as inference API"""
 76 | 
 77 |     if api_key is None and not provider.lower() == "ollama":
 78 |         raise ValueError("API key is required for model.")
 79 |         exit()
 80 |     else:
 81 |         return OpenAIModel(
 82 |             model_name, provider=OpenAIProvider(base_url=base_url, api_key=api_key)
 83 |         )
 84 | 
 85 | 
 86 | def print_research_Result(summary: dict, usage: Usage) -> None:
 87 |     """Output structured summary of the research topic."""
 88 | 
 89 |     print(f"\n=== Research Summary: {summary.topic} ===")
 90 |     print("\nKey Points:")
 91 |     for i, point in enumerate(summary.key_points, 1):
 92 |         print(f"{i}. {point}")
 93 |     print("\nKey Points Explanation:")
 94 |     for i, point in enumerate(summary.key_points_explanation, 1):
 95 |         print(f"{i}. {point}")
 96 |     print(f"\nConclusion: {summary.conclusion}")
 97 | 
 98 |     print(f"\nResponse structure: {list(dict(summary).keys())}")
 99 |     print(usage)
100 | 


--------------------------------------------------------------------------------
/src/gui/components/footer.py:
--------------------------------------------------------------------------------
1 | from streamlit import caption, divider
2 | 
3 | 
4 | def render_footer(footer_caption: str):
5 |     """Render the page footer."""
6 |     divider()
7 |     caption(footer_caption)
8 | 


--------------------------------------------------------------------------------
/src/gui/components/header.py:
--------------------------------------------------------------------------------
1 | from streamlit import divider, title
2 | 
3 | 
4 | def render_header(header_title: str):
5 |     """Render the page header with title."""
6 |     title(header_title)
7 |     divider()
8 | 


--------------------------------------------------------------------------------
/src/gui/components/output.py:
--------------------------------------------------------------------------------
 1 | from typing import Any
 2 | 
 3 | from streamlit import empty, info
 4 | 
 5 | 
 6 | def render_output(
 7 |     result: Any = None, info_str: str | None = None, type: str | None = None
 8 | ):
 9 |     """
10 |     Renders the output in a Streamlit app based on the provided type.
11 | 
12 |     Args:
13 |         result (Any, optional): The content to be displayed. Can be JSON, code
14 |             markdown, or plain text.
15 |         info (str, optional): The information message to be displayed if result is None.
16 |         type (str, optional): The type of the result content. Can be 'json', 'code',
17 |             'md', or other for plain text.
18 | 
19 |     Returns:
20 |         Out: None
21 |     """
22 | 
23 |     if result:
24 |         output_container = empty()
25 |         output_container.write(result)
26 |         # match type:
27 |         #     case "json":
28 |         #         json(result)
29 |         #     case "code":
30 |         #         code(result)
31 |         #     case "md":
32 |         #         markdown(result)
33 |         #     case _:
34 |         #         text(result)
35 |         #         # st.write(result)
36 |     else:
37 |         info(info_str)
38 | 


--------------------------------------------------------------------------------
/src/gui/components/prompts.py:
--------------------------------------------------------------------------------
 1 | from streamlit import text_area
 2 | 
 3 | 
 4 | def render_prompt_editor(
 5 |     prompt_name: str, prompt_value: str, height: int = 150
 6 | ) -> str | None:
 7 |     return text_area(
 8 |         f"{prompt_name.replace('_', ' ').title()}", value=prompt_value, height=height
 9 |     )
10 | 


--------------------------------------------------------------------------------
/src/gui/components/sidebar.py:
--------------------------------------------------------------------------------
 1 | from streamlit import sidebar
 2 | 
 3 | from gui.config.config import PAGES
 4 | 
 5 | 
 6 | def render_sidebar(sidebar_title: str):
 7 |     sidebar.title(sidebar_title)
 8 |     selected_page = sidebar.radio(" ", PAGES)
 9 | 
10 |     # st.sidebar.divider()
11 |     # st.sidebar.info(" ")
12 |     return selected_page
13 | 


--------------------------------------------------------------------------------
/src/gui/config/config.py:
--------------------------------------------------------------------------------
 1 | APP_PATH = "app"
 2 | PAGES = ["Home", "Settings", "Prompts", "App"]
 3 | PROMPTS_DEFAULT = {
 4 |     "system_prompt_manager": (
 5 |         "You are a manager overseeing research and analysis tasks..."
 6 |     ),
 7 |     "system_prompt_researcher": ("You are a researcher. Gather and analyze data..."),
 8 |     "system_prompt_analyst": (
 9 |         "You are a research analyst. Use your analytical skills..."
10 |     ),
11 |     "system_prompt_synthesiser": (
12 |         "You are a research synthesiser. Use your analytical skills..."
13 |     ),
14 | }
15 | 


--------------------------------------------------------------------------------
/src/gui/config/styling.py:
--------------------------------------------------------------------------------
 1 | from streamlit import markdown, set_page_config
 2 | 
 3 | 
 4 | def add_custom_styling(page_title: str):
 5 |     set_page_config(
 6 |         page_title=f"{page_title}",
 7 |         page_icon="🤖",
 8 |         layout="wide",
 9 |         initial_sidebar_state="expanded",
10 |     )
11 | 
12 |     custom_css = """
13 |     <style>    
14 |     /* Hide the default radio button circles */
15 |     div[role="radiogroup"] label > div:first-child {
16 |         display: none !important;
17 |     }
18 |     </style>
19 |     """
20 |     markdown(custom_css, unsafe_allow_html=True)
21 | 


--------------------------------------------------------------------------------
/src/gui/config/text.py:
--------------------------------------------------------------------------------
 1 | HOME_INFO = "Select 'App' to start using the system"
 2 | HOME_HEADER = "Welcome to the Multi-Agent Research System"
 3 | HOME_DESCRIPTION = """
 4 | This system allows you to:
 5 | 
 6 | - Run research queries using multiple specialized agents
 7 | - Configure agent settings and prompts
 8 | - View detailed results from your research
 9 | 
10 | Use the sidebar to navigate between different sections of the application.
11 | """
12 | PAGE_TITLE = "MAS Eval 👾⚗️🧠💡"
13 | PROMPTS_WARNING = "No prompts found. Using default prompts."
14 | PROMPTS_HEADER = "Agent Prompts"
15 | RUN_APP_HEADER = "Run Research App"
16 | RUN_APP_QUERY_PLACEHOLDER = "What would you like to research?"
17 | RUN_APP_PROVIDER_PLACEHOLDER = "Provider?"
18 | RUN_APP_BUTTON = "Run Query"
19 | RUN_APP_OUTPUT_PLACEHOLDER = "Run the agent to see results here"
20 | RUN_APP_QUERY_WARNING = "Please enter a query"
21 | RUN_APP_QUERY_RUN_INFO = "Running query: "
22 | SETTINGS_HEADER = "Settings"
23 | SETTINGS_PROVIDER_LABEL = "Select Provider"
24 | SETTINGS_PROVIDER_PLACEHOLDER = "Select Provider"
25 | SETTINGS_ADD_PROVIDER = "Add New Provider"
26 | SETTINGS_API_KEY_LABEL = "API Key"
27 | OUTPUT_SUBHEADER = "Output"
28 | 


--------------------------------------------------------------------------------
/src/gui/pages/home.py:
--------------------------------------------------------------------------------
 1 | from streamlit import header, info, markdown
 2 | 
 3 | from gui.config.text import HOME_DESCRIPTION, HOME_HEADER, HOME_INFO
 4 | 
 5 | 
 6 | def render_home():
 7 |     header(HOME_HEADER)
 8 |     markdown(HOME_DESCRIPTION)
 9 |     info(HOME_INFO)
10 | 


--------------------------------------------------------------------------------
/src/gui/pages/prompts.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Streamlit component for editing agent system prompts.
 3 | 
 4 | This module provides a function to render and edit prompt configurations
 5 | for agent roles using a Streamlit-based UI. It validates the input configuration,
 6 | displays warnings if prompts are missing, and allows interactive editing of each prompt.
 7 | """
 8 | 
 9 | from pydantic import BaseModel
10 | from streamlit import error, header, warning
11 | 
12 | from app.config.data_models import ChatConfig
13 | from app.utils.error_messages import invalid_type
14 | from app.utils.log import logger
15 | from gui.components.prompts import render_prompt_editor
16 | from gui.config.config import PROMPTS_DEFAULT
17 | from gui.config.text import PROMPTS_HEADER, PROMPTS_WARNING
18 | 
19 | 
20 | def render_prompts(chat_config: ChatConfig | BaseModel):  # -> dict[str, str]:
21 |     """
22 |     Render and edit the prompt configuration for agent roles in the Streamlit UI.
23 |     """
24 | 
25 |     header(PROMPTS_HEADER)
26 | 
27 |     if not isinstance(chat_config, ChatConfig):
28 |         msg = invalid_type("ChatConfig", type(chat_config).__name__)
29 |         logger.error(msg)
30 |         error(msg)
31 |         return None
32 | 
33 |     # updated = False
34 |     prompts = chat_config.prompts
35 | 
36 |     if not prompts:
37 |         warning(PROMPTS_WARNING)
38 |         prompts = PROMPTS_DEFAULT
39 | 
40 |     updated_prompts = prompts.copy()
41 | 
42 |     # Edit prompts
43 |     for prompt_key, prompt_value in prompts.items():
44 |         new_value = render_prompt_editor(prompt_key, prompt_value, height=200)
45 |         if new_value != prompt_value and new_value is not None:
46 |             updated_prompts[prompt_key] = new_value
47 |             # updated = True
48 | 
49 |     # return updated_prompts if updated else prompts
50 | 


--------------------------------------------------------------------------------
/src/gui/pages/run_app.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Streamlit interface for running the agentic system interactively.
 3 | 
 4 | This module defines the render_app function, which provides a Streamlit-based UI
 5 | for users to select a provider, enter a query, and execute the main agent workflow.
 6 | Results and errors are displayed in real time, supporting asynchronous execution.
 7 | """
 8 | 
 9 | from streamlit import button, exception, header, info, subheader, text_input, warning
10 | 
11 | from app.main import main
12 | from app.utils.log import logger
13 | from gui.components.output import render_output
14 | from gui.config.text import (
15 |     OUTPUT_SUBHEADER,
16 |     RUN_APP_BUTTON,
17 |     RUN_APP_HEADER,
18 |     RUN_APP_OUTPUT_PLACEHOLDER,
19 |     RUN_APP_PROVIDER_PLACEHOLDER,
20 |     RUN_APP_QUERY_PLACEHOLDER,
21 |     RUN_APP_QUERY_RUN_INFO,
22 |     RUN_APP_QUERY_WARNING,
23 | )
24 | 
25 | 
26 | async def render_app(provider: str | None = None):
27 |     """
28 |     Render the main app interface for running agentic queries via Streamlit.
29 | 
30 |     Displays input fields for provider and query, a button to trigger execution,
31 |     and an area for output or error messages. Handles async invocation of the
32 |     main agent workflow and logs any exceptions.
33 |     """
34 | 
35 |     header(RUN_APP_HEADER)
36 |     if provider is None:
37 |         provider = text_input(RUN_APP_PROVIDER_PLACEHOLDER)
38 |     query = text_input(RUN_APP_QUERY_PLACEHOLDER)
39 | 
40 |     subheader(OUTPUT_SUBHEADER)
41 |     if button(RUN_APP_BUTTON):
42 |         if query:
43 |             info(f"{RUN_APP_QUERY_RUN_INFO} {query}")
44 |             try:
45 |                 result = await main(chat_provider=provider, query=query)
46 |                 render_output(result)
47 |             except Exception as e:
48 |                 render_output(None)
49 |                 exception(e)
50 |                 logger.exception(e)
51 |         else:
52 |             warning(RUN_APP_QUERY_WARNING)
53 |     else:
54 |         render_output(RUN_APP_OUTPUT_PLACEHOLDER)
55 | 


--------------------------------------------------------------------------------
/src/gui/pages/settings.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Streamlit settings UI for provider and agent configuration.
 3 | 
 4 | This module provides a function to render and edit agent system settings,
 5 | including provider selection and related options, within the Streamlit GUI.
 6 | It validates the input configuration and ensures correct typing before rendering.
 7 | """
 8 | 
 9 | from streamlit import error, header, selectbox
10 | 
11 | from app.config.data_models import BaseModel, ChatConfig
12 | from app.utils.error_messages import invalid_type
13 | from app.utils.log import logger
14 | from gui.config.text import SETTINGS_HEADER, SETTINGS_PROVIDER_LABEL
15 | 
16 | 
17 | def render_settings(chat_config: ChatConfig | BaseModel) -> str:
18 |     """
19 |     Render and edit agent system settings in the Streamlit UI.
20 | 
21 |     Displays a header and a selectbox for choosing the inference provider.
22 |     Validates that the input is a ChatConfig instance and displays an error if not.
23 |     """
24 |     header(SETTINGS_HEADER)
25 | 
26 |     # updated = False
27 |     # updated_config = config.copy()
28 | 
29 |     if not isinstance(chat_config, ChatConfig):
30 |         msg = invalid_type("ChatConfig", type(chat_config).__name__)
31 |         logger.error(msg)
32 |         error(msg)
33 |         return msg
34 | 
35 |     provider = selectbox(
36 |         label=SETTINGS_PROVIDER_LABEL,
37 |         options=chat_config.providers.keys(),
38 |     )
39 | 
40 |     # Run options
41 |     # col1, col2 = st.columns(2)
42 |     # with col1:
43 |     #     streamed_output = st.checkbox(
44 |     #         "Stream Output", value=config.get("streamed_output", False)
45 |     #     )
46 |     # with col2:
47 |     #     st.checkbox("Include Sources", value=True)  # include_sources
48 | 
49 |     # Allow adding new providers
50 |     # new_provider = st.text_input("Add New Provider")
51 |     # api_key = st.text_input(f"{provider} API Key", type="password")
52 |     # if st.button("Add Provider") and new_provider and new_provider not in providers:
53 |     #     providers.append(new_provider)
54 |     #     updated_config["providers"] = providers
55 |     #     updated_config["api_key"] = api_key
56 |     #     updated = True
57 |     #     st.success(f"Added provider: {new_provider}")
58 | 
59 |     # # Update config if changed
60 |     # if (
61 |     #     include_a != config.get("include_a", False)
62 |     #     or include_b != config.get("include_b", False)
63 |     #     or streamed_output != config.get("streamed_output", False)
64 |     # ):
65 |     #     updated_config["include_a"] = include_a
66 |     #     updated_config["include_b"] = include_b
67 |     #     updated_config["streamed_output"] = streamed_output
68 |     #     updated = True
69 | 
70 |     return provider
71 | 


--------------------------------------------------------------------------------
/src/run_gui.py:
--------------------------------------------------------------------------------
 1 | """
 2 | This module sets up and runs a Streamlit application for a Multi-Agent System.
 3 | 
 4 | The application includes the following components:
 5 | - Header
 6 | - Sidebar for configuration options
 7 | - Main content area for prompts
 8 | - Footer
 9 | 
10 | The main function loads the configuration, renders the UI components, and handles the
11 | execution of the Multi-Agent System based on user input.
12 | 
13 | Functions:
14 | - run_app(): Placeholder function to run the main application logic.
15 | - main(): Main function to set up and run the Streamlit application.
16 | """
17 | 
18 | from asyncio import run
19 | from pathlib import Path
20 | 
21 | from app.config.config_app import CHAT_CONFIG_FILE, CHAT_DEFAULT_PROVIDER
22 | from app.config.data_models import ChatConfig
23 | from app.utils.load_configs import load_config
24 | from app.utils.log import logger
25 | from gui.components.sidebar import render_sidebar
26 | from gui.config.config import APP_PATH
27 | from gui.config.styling import add_custom_styling
28 | from gui.config.text import PAGE_TITLE
29 | from gui.pages.home import render_home
30 | from gui.pages.prompts import render_prompts
31 | from gui.pages.run_app import render_app
32 | from gui.pages.settings import render_settings
33 | 
34 | # TODO create sidebar tabs, move settings to page,
35 | # set readme.md as home, separate prompts into page
36 | 
37 | chat_config_pfile = Path(__file__).parent / APP_PATH / CHAT_CONFIG_FILE
38 | chat_config = load_config(chat_config_pfile, ChatConfig)
39 | provider = CHAT_DEFAULT_PROVIDER
40 | logger.info(f"Default provider: {CHAT_DEFAULT_PROVIDER}")
41 | 
42 | 
43 | async def main():
44 |     add_custom_styling(PAGE_TITLE)
45 |     selected_page = render_sidebar(PAGE_TITLE)
46 | 
47 |     if selected_page == "Home":
48 |         render_home()
49 |     elif selected_page == "Settings":
50 |         # TODO temp save settings to be used in gui
51 |         provider = render_settings(chat_config)
52 |         logger.info(f"Page 'Settings' provider: {provider}")
53 |     elif selected_page == "Prompts":
54 |         render_prompts(chat_config)
55 |     elif selected_page == "App":
56 |         logger.info(f"Page 'App' provider: {CHAT_DEFAULT_PROVIDER}")
57 |         await render_app(CHAT_DEFAULT_PROVIDER)
58 | 
59 | 
60 | if __name__ == "__main__":
61 |     run(main())
62 | 


--------------------------------------------------------------------------------
/tests/test_agent_system.py:
--------------------------------------------------------------------------------
 1 | from app.agents.agent_system import get_manager
 2 | from app.config.data_models import ProviderConfig
 3 | 
 4 | 
 5 | def test_get_manager_minimal():
 6 |     provider = "github"
 7 |     provider_config = ProviderConfig.model_validate(
 8 |         {"model_name": "test-model", "base_url": "http://test.com"}
 9 |     )
10 |     api_key = "test"
11 |     prompts = {"system_prompt_manager": "test"}
12 |     agent = get_manager(provider, provider_config, api_key, prompts)
13 |     assert hasattr(agent, "run")
14 | 


--------------------------------------------------------------------------------
/tests/test_env.py:
--------------------------------------------------------------------------------
 1 | from pytest import MonkeyPatch
 2 | 
 3 | from app.config.data_models import AppEnv
 4 | 
 5 | 
 6 | def test_app_env_loads_env_vars(monkeypatch: MonkeyPatch):
 7 |     monkeypatch.setenv("GEMINI_API_KEY", "test-gemini")
 8 |     env = AppEnv()
 9 |     assert env.GEMINI_API_KEY == "test-gemini"
10 | 


--------------------------------------------------------------------------------
/tests/test_metrics_output_similarity.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Tests for the output_similarity metric.
 3 | 
 4 | This module verifies that the output_similarity metric correctly identifies when
 5 | an agent's output matches the expected answer.
 6 | """
 7 | 
 8 | from app.evals.metrics import output_similarity
 9 | 
10 | 
11 | def test_output_similarity_exact_match():
12 |     assert output_similarity("42", "42") is True
13 | 
14 | 
15 | def test_output_similarity_whitespace():
16 |     assert output_similarity("  answer  ", "answer") is True
17 | 
18 | 
19 | def test_output_similarity_incorrect():
20 |     assert output_similarity("foo", "bar") is False
21 | 


--------------------------------------------------------------------------------
/tests/test_metrics_time_taken.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Tests for the time_taken metric.
 3 | 
 4 | This module verifies that the time_taken metric correctly computes the elapsed
 5 | time between two timestamps, ensuring accurate measurement of agent execution
 6 | duration for evaluation purposes.
 7 | """
 8 | 
 9 | import asyncio
10 | import time
11 | 
12 | import pytest
13 | 
14 | from app.evals.metrics import time_taken
15 | 
16 | 
17 | @pytest.mark.asyncio
18 | async def test_time_taken_metric():
19 |     """Scenario: Calculate time taken for agent execution"""
20 | 
21 |     # Given: Start and end timestamps
22 |     start_time = time.perf_counter()
23 |     await asyncio.sleep(0.1)
24 |     end_time = time.perf_counter()
25 | 
26 |     # When: Calculating time taken
27 |     result = time_taken(start_time, end_time)
28 | 
29 |     # Then: Verify correct duration calculation
30 |     assert result == pytest.approx(0.1, abs=0.05)
31 | 


--------------------------------------------------------------------------------
/tests/test_provider_config.py:
--------------------------------------------------------------------------------
 1 | from pytest import MonkeyPatch
 2 | 
 3 | from app.config.data_models import ProviderConfig
 4 | 
 5 | 
 6 | def test_provider_config_parsing(monkeypatch: MonkeyPatch):
 7 |     pcfg = ProviderConfig.model_validate(
 8 |         {"model_name": "foo", "base_url": "https://foo.bar"}
 9 |     )
10 |     assert pcfg.model_name == "foo"
11 |     assert pcfg.base_url == "bar"
12 | 


--------------------------------------------------------------------------------
