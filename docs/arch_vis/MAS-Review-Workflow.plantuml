@startuml MAS-Review-Workflow
title PeerRead Evaluation Workflow

!log Current 'STYLE' dvar: STYLE
!log About to include: styles/github-STYLE.puml
!include styles/github-STYLE.puml

actor User
participant "Manager Agent" as Manager
participant "Researcher Agent" as Researcher
database "PeerRead Dataset" as DB
participant "Large Context LLM" as LLM
entity "Evaluation System" as EvalSystem
entity "ReviewPersistence" as Persistence

User -> Manager: Request to evaluate paper "X"
activate Manager

Manager -> DB: Get paper content for "X"
activate DB
DB --> Manager: Return full paper content
deactivate DB

note right of Manager
  Large context models (Claude 4, Gemini 2.5, ...)
  process full papers without chunking.
  Execution is traced for evaluation.
end note

Manager -> LLM: Generate review using large context
activate LLM
LLM --> Manager: Return comprehensive review + traces
deactivate LLM

Manager -> Persistence: Save review + execution traces
activate Persistence
Persistence -> Persistence: Create timestamped JSON file with metadata
Persistence --> Manager: Confirm save
deactivate Persistence

group Optional Delegation
    Manager -> Researcher: Delegate research query
    activate Researcher
    Researcher -> Researcher: Use DuckDuckGo, Tavily, Exa, ...
    Researcher --> Manager: Return research results
    deactivate Researcher
end group

Manager -> EvalSystem: Initiate three-tier evaluation
activate EvalSystem

EvalSystem -> EvalSystem: Traditional Metrics (Text similarity, execution time, ...)
EvalSystem -> EvalSystem: LLM-as-a-Judge (quality + execution assessment)
EvalSystem -> EvalSystem: Graph Analysis (tool calls + complexity)
EvalSystem -> EvalSystem: Composite Score = (Results / Time / Complexity)

EvalSystem --> Manager: Complete evaluation results
deactivate EvalSystem

Manager --> User: Final agent performance score
deactivate Manager
@enduml
