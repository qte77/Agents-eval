@startuml metrics-eval-sweep
title Benchmarking Sweep Workflow

!log Current 'STYLE' dvar: STYLE
!log About to include: styles/github-STYLE.puml
!include styles/github-STYLE.puml

participant "SweepConfig\n(Configuration)" as SC
participant "SweepRunner\n(Orchestrator)" as SR
participant "Agentic System\n(app.main)" as AS
participant "EvaluationPipeline\n(3-tier eval)" as EP
participant "SweepAnalysis\n(Statistics)" as SA
participant "Result Store\n(results.json)" as RS

SC -> SR: Initialize sweep\n(compositions, paper_ids, repetitions)
SR -> RS: Create empty results.json

group Composition Sweep [compositions × papers × repetitions]

    loop for each AgentComposition
        loop for each paper_id
            loop for each repetition

                SR -> AS: run(composition, paper_id, engine)

                alt engine = "mas" (default)

                    group Rate-Limit Retry [max 3 attempts, exponential backoff]
                        AS -> EP: Execute evaluation\n(Tier 1/2/3)
                        alt Success
                            EP --> AS: CompositeResult\n(tier1/2/3 scores, composite)
                            AS --> SR: CompositeResult
                            SR -> RS: Append result\n(incremental save)
                        else HTTP 429 / Rate Limit
                            AS -> AS: Wait (retry_delay * 2^attempt)
                            AS -> EP: Retry
                        else Max retries exceeded
                            AS --> SR: None (skip, log error)
                        end
                    end

                else engine = "cc" (Claude Code headless)

                    SR -> SR: Verify: shutil.which("claude")
                    SR -> AS: claude -p "prompt" (subprocess)
                    AS -> AS: Collect artifacts\n(~/.claude/teams/, raw_stream.jsonl)
                    AS -> EP: CCTraceAdapter.parse(artifacts)\n→ GraphTraceData → evaluation
                    EP --> AS: CompositeResult
                    AS --> SR: CompositeResult
                    SR -> RS: Append result\n(incremental save)

                end

            end
        end
    end

end

SR -> SA: Aggregate results\n(mean, stddev per composition)
SA -> RS: Write final results.json
SA -> RS: Write summary.md\n(Markdown table per composition)

@enduml
