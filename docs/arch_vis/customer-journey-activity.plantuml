@startuml customer-journey-activity
title PeerRead Agent Evaluation Journey

!log Current 'STYLE' dvar: STYLE
!log About to include: styles/github-STYLE.puml
!include styles/github-STYLE.puml

start

:User discovers the agent evaluation project;

:Clones repository and sets up development environment;

if (First time use or dataset update?) then (yes)
  :Download PeerRead dataset with `make run_cli --download-peerread-full-only`;
  :Dataset is cached locally in `datasets/peerread/`;
else (no)
endif

:User selects evaluation interface;

if (Interface choice) then (CLI)
  :Execute `make run_cli` with evaluation parameters;
  group CLI Agent Evaluation
    :Select paper from PeerRead dataset;
    :Configure agent system (Manager/Researcher/Analyst/Synthesizer);
    :Initialize large context window models (Claude-3.5, GPT-4, Gemini-1.5);
    :Agent system generates comprehensive review;
    :Three-tier evaluation runs automatically;
  end group
else (Streamlit GUI)
  :Launch `make run_gui` for interactive evaluation;
  group GUI Agent Evaluation
    :Browse PeerRead papers in web interface;
    :Configure evaluation parameters and agent settings;
    :Trigger agent review generation;
    :View real-time evaluation metrics and scores;
  end group
endif

:System executes three-tier evaluation;

group Three-Tier Evaluation Process
  :Traditional Metrics: text similarity, execution time, task success;
  :LLM-as-a-Judge: planning rational, coordination quality, tool efficiency;
  :Graph Analysis: agent interaction complexity and patterns;
  :Composite Scoring: weighted final score with config metrics;
end group

:User analyzes evaluation results;

if (Evaluation results satisfactory?) then (yes)
  :Export evaluation data and performance metrics;
  :Document agent performance insights;
else (no)
  :Adjust agent configuration or model selection;
  :Re-run evaluation with different parameters;
endif

:User iterates to improve agent performance;

stop
@enduml
