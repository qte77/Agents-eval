@startuml customer-journey-activity
title PeerRead Agent Evaluation Journey

!log Current 'STYLE' dvar: STYLE
!log About to include: styles/github-STYLE.puml
!include styles/github-STYLE.puml

start

:User discovers the agent evaluation project;

:Clones repository and sets up development environment;

if (First time use or dataset update?) then (yes)
  :Download PeerRead dataset with `make run_cli --download-peerread-full-only`;
  :Dataset cached locally in `datasets/peerread/`;
else (no)
endif

:User selects evaluation interface;

if (Interface choice) then (CLI)
  :Execute `make run_cli` with evaluation parameters;
  group CLI Agent Evaluation
    :Select paper via `--paper-id=ID`;
    :Choose engine: `--engine=mas` (default) or `--engine=cc`;
    :Configure agent composition and provider via `--chat-provider`;
    :Agent system generates comprehensive review;
    :Three-tier evaluation runs automatically;
  end group
else (Streamlit GUI)
  :Launch `make run_gui` for interactive evaluation;
  group GUI Agent Evaluation
    :Browse and select PeerRead paper (dropdown with title/abstract);
    :Choose engine (MAS or Claude Code);
    :Configure provider, judge, and agent settings;
    :Monitor agent progress via real-time debug log;
    :View evaluation metrics and agent interaction graph;
  end group
endif

:System executes three-tier evaluation;

group Three-Tier Evaluation Process
  :Tier 1 — Traditional Metrics: BLEU, ROUGE, BERTScore, execution time;
  :Tier 2 — LLM-as-a-Judge: planning rationale, coordination quality, tool efficiency;
  :Tier 3 — Graph Analysis: agent interaction complexity and delegation patterns;
  :Composite Scoring: weighted final score;
end group

:User analyzes evaluation results;

if (Evaluation results satisfactory?) then (yes)
  :Export evaluation data and composite scores;
  :Document agent performance insights;
else (no)
  :Adjust composition, provider, or paper selection;
  :Re-run evaluation with different parameters;
endif

if (Sweep comparison needed?) then (yes)
  :Run `make run_sweep` across compositions × papers × repetitions;
  :Review statistical summary in `results/sweeps/<timestamp>/summary.md`;
else (no)
endif

:User iterates to improve agent performance;

stop
@enduml
